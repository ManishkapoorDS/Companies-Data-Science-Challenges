{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beat the machine challenge (SMAPE loss function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x22</th>\n",
       "      <th>x23</th>\n",
       "      <th>x24</th>\n",
       "      <th>x25</th>\n",
       "      <th>x26</th>\n",
       "      <th>x27</th>\n",
       "      <th>x28</th>\n",
       "      <th>x29</th>\n",
       "      <th>x30</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-06-15T00:00:00.000000Z</td>\n",
       "      <td>-27.155995</td>\n",
       "      <td>0.332523</td>\n",
       "      <td>735.701951</td>\n",
       "      <td>107.513212</td>\n",
       "      <td>1.459349</td>\n",
       "      <td>0.099577</td>\n",
       "      <td>-62.472969</td>\n",
       "      <td>122.606756</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.195166</td>\n",
       "      <td>0.966355</td>\n",
       "      <td>102.051843</td>\n",
       "      <td>30.555759</td>\n",
       "      <td>24.130086</td>\n",
       "      <td>680.589152</td>\n",
       "      <td>-927.424059</td>\n",
       "      <td>-87.123208</td>\n",
       "      <td>62.666205</td>\n",
       "      <td>1156.681711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-06-14T00:00:00.000000Z</td>\n",
       "      <td>-17.012365</td>\n",
       "      <td>0.269606</td>\n",
       "      <td>723.576863</td>\n",
       "      <td>107.761407</td>\n",
       "      <td>11.138383</td>\n",
       "      <td>-0.254799</td>\n",
       "      <td>-60.965083</td>\n",
       "      <td>122.834661</td>\n",
       "      <td>0.008695</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080149</td>\n",
       "      <td>0.965490</td>\n",
       "      <td>102.031867</td>\n",
       "      <td>-12.797338</td>\n",
       "      <td>13.641165</td>\n",
       "      <td>699.645742</td>\n",
       "      <td>-924.946099</td>\n",
       "      <td>-84.658409</td>\n",
       "      <td>63.023348</td>\n",
       "      <td>972.159870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-06-13T00:00:00.000000Z</td>\n",
       "      <td>-3.306774</td>\n",
       "      <td>-1.054852</td>\n",
       "      <td>738.375336</td>\n",
       "      <td>107.556928</td>\n",
       "      <td>7.168404</td>\n",
       "      <td>-0.158063</td>\n",
       "      <td>-42.867335</td>\n",
       "      <td>122.621138</td>\n",
       "      <td>-0.003350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036628</td>\n",
       "      <td>0.965188</td>\n",
       "      <td>101.939175</td>\n",
       "      <td>20.709537</td>\n",
       "      <td>27.210881</td>\n",
       "      <td>705.710819</td>\n",
       "      <td>-939.902095</td>\n",
       "      <td>-85.299329</td>\n",
       "      <td>60.244882</td>\n",
       "      <td>1000.999754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-06-12T00:00:00.000000Z</td>\n",
       "      <td>-20.244923</td>\n",
       "      <td>0.199990</td>\n",
       "      <td>729.565275</td>\n",
       "      <td>107.706857</td>\n",
       "      <td>-8.592282</td>\n",
       "      <td>0.089837</td>\n",
       "      <td>-25.569892</td>\n",
       "      <td>122.127499</td>\n",
       "      <td>-0.000076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>0.963559</td>\n",
       "      <td>101.843815</td>\n",
       "      <td>-9.351715</td>\n",
       "      <td>-1.445244</td>\n",
       "      <td>724.573891</td>\n",
       "      <td>-931.245191</td>\n",
       "      <td>-87.296879</td>\n",
       "      <td>57.449428</td>\n",
       "      <td>970.889792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-06-11T00:00:00.000000Z</td>\n",
       "      <td>11.278201</td>\n",
       "      <td>0.284900</td>\n",
       "      <td>710.838013</td>\n",
       "      <td>107.527512</td>\n",
       "      <td>-12.255875</td>\n",
       "      <td>0.447218</td>\n",
       "      <td>-29.382555</td>\n",
       "      <td>122.137525</td>\n",
       "      <td>-0.002483</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.818158</td>\n",
       "      <td>0.963392</td>\n",
       "      <td>101.896694</td>\n",
       "      <td>1.270608</td>\n",
       "      <td>-11.391039</td>\n",
       "      <td>733.148718</td>\n",
       "      <td>-922.523210</td>\n",
       "      <td>-86.495199</td>\n",
       "      <td>55.497440</td>\n",
       "      <td>1021.174747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Unnamed: 0         x1        x2          x3          x4  \\\n",
       "0  2018-06-15T00:00:00.000000Z -27.155995  0.332523  735.701951  107.513212   \n",
       "1  2018-06-14T00:00:00.000000Z -17.012365  0.269606  723.576863  107.761407   \n",
       "2  2018-06-13T00:00:00.000000Z  -3.306774 -1.054852  738.375336  107.556928   \n",
       "3  2018-06-12T00:00:00.000000Z -20.244923  0.199990  729.565275  107.706857   \n",
       "4  2018-06-11T00:00:00.000000Z  11.278201  0.284900  710.838013  107.527512   \n",
       "\n",
       "          x5        x6         x7          x8        x9     ...       \\\n",
       "0   1.459349  0.099577 -62.472969  122.606756  0.000297     ...        \n",
       "1  11.138383 -0.254799 -60.965083  122.834661  0.008695     ...        \n",
       "2   7.168404 -0.158063 -42.867335  122.621138 -0.003350     ...        \n",
       "3  -8.592282  0.089837 -25.569892  122.127499 -0.000076     ...        \n",
       "4 -12.255875  0.447218 -29.382555  122.137525 -0.002483     ...        \n",
       "\n",
       "        x22       x23         x24        x25        x26         x27  \\\n",
       "0 -0.195166  0.966355  102.051843  30.555759  24.130086  680.589152   \n",
       "1 -0.080149  0.965490  102.031867 -12.797338  13.641165  699.645742   \n",
       "2  0.036628  0.965188  101.939175  20.709537  27.210881  705.710819   \n",
       "3  0.001390  0.963559  101.843815  -9.351715  -1.445244  724.573891   \n",
       "4 -0.818158  0.963392  101.896694   1.270608 -11.391039  733.148718   \n",
       "\n",
       "          x28        x29        x30            y  \n",
       "0 -927.424059 -87.123208  62.666205  1156.681711  \n",
       "1 -924.946099 -84.658409  63.023348   972.159870  \n",
       "2 -939.902095 -85.299329  60.244882  1000.999754  \n",
       "3 -931.245191 -87.296879  57.449428   970.889792  \n",
       "4 -922.523210 -86.495199  55.497440  1021.174747  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('Beat the machine challenge dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symmetric Mean Absolute Percent Error (SMAPE) \n",
    "def SMAPE(A, F):\n",
    "    return 100/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df.drop(['y','Unnamed: 0'], axis = 1).values\n",
    "Y = df['y'].fillna(0)\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size = 0.05, random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[475]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "# SMAPE(X_Train,Y_Train)\n",
    "from numba import jit\n",
    "import math\n",
    "\n",
    "@jit\n",
    "def smape_cpcm(y_true, y_pred):\n",
    "    out = 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        a = y_true[i]\n",
    "        b = y_pred[i]\n",
    "        c = math.fabs(a)+math.fabs(b)\n",
    "        if c == 0:\n",
    "            continue\n",
    "        out += math.fabs(a - b) / c\n",
    "    out *= (200.0 / y_true.shape[0])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smape_fast(X_Train,Y_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def smape_kun(y_true, y_pred):\n",
    "    return np.mean((np.abs(y_pred - y_true) * 200/ (np.abs(y_pred) + np.abs(y_true))).fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199.2119149506594"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smape_kun(X_Train[:,1],Y_Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2047,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.644971944366066\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/nickycan/much-faster-smape-metric-function\n",
    "# https://stackoverflow.com/questions/51444630/how-to-use-smape-evaluation-metric-on-train-dataset\n",
    "# https://stackoverflow.com/questions/41925157/logisticregression-unknown-label-type-continuous-using-sklearn-in-python\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_jobs=-1000)\n",
    "\n",
    "model.fit(X_Train,Y_Train)\n",
    "\n",
    "y_pred = model.predict(X_Test)\n",
    "\n",
    "print(smape_kun(Y_Test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.063916740529667\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n",
    "from sklearn import linear_model\n",
    "clf = linear_model.Lasso(alpha=0.1)\n",
    "\n",
    "clf.fit(X_Train,Y_Train)\n",
    "\n",
    "y_pred = clf.predict(X_Test)\n",
    "\n",
    "print(smape_kun(Y_Test,y_pred))\n",
    "# print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.004396613559596\n",
      "[878.94415482 271.43547028 597.30961577 967.43822198 439.99987519\n",
      " 368.86978256 888.68338747 247.13701981 758.57591368 222.06131969\n",
      " -31.0845677   54.86896653 677.90026147 361.20533806 754.39439162\n",
      " 822.70540431 259.26541475 145.6921482  266.32647931 128.17883079\n",
      " 950.93466172 221.66391014 191.10224366 161.51247384 148.57011966]\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(X_Train,Y_Train)\n",
    "\n",
    "y_pred = reg.predict(X_Test)\n",
    "\n",
    "print(smape_kun(Y_Test,y_pred))\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.2885507824887\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr_regressor = SVR(kernel='rbf', gamma='auto')\n",
    "svr_regressor.fit(X_Train,Y_Train)\n",
    "\n",
    "y_pred = svr_regressor.predict(X_Test)\n",
    "\n",
    "print(smape_kun(Y_Test,y_pred))\n",
    "#print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2057,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.31179557795265\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_regressor = DecisionTreeRegressor(random_state = 0)\n",
    "tree_regressor.fit(X_Train,Y_Train)\n",
    "\n",
    "y_pred = tree_regressor.predict(X_Test)\n",
    "\n",
    "print(smape_kun(Y_Test,y_pred))\n",
    "#print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2053,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22 22 26  0 -2 29  0 24 -2 -2 -2 19 23  8  6 27 -2 14 -2 -2 29 -2 -2 11\n",
      " -2 -2 12 -2 -2 27 -2 -2 26 24 27  4 24 -2 -2  5 -2 -2 18 -2 29  3 -2 -2\n",
      " -2 22  4 -2  0 -2 -2  7 26  6 -2 -2 -2 12 21 -2  4 -2 -2 12 -2 -2 22  7\n",
      " -2 22 -2 -2  1 -2 -2 27 13  9  5 27  8 -2 -2 -2 14 -2 13  0 -2 -2  2 -2\n",
      " -2 26 -2 -2  6 19 12  3 29 10 -2 -2 -2 15 27 -2 -2 12 -2 -2  0 26 -2  2\n",
      " -2 -2 29 21 -2 -2  0 -2 -2  1 29 29 -2 14 -2 -2 12 -2 -2 15 -2 -2  9  6\n",
      "  0 -2 -2 -2 10  1 -2 -2 12  4 16 -2 -2 27 -2 -2 24  6 -2 -2 -2  6 11 26\n",
      "  5 29  5 -2 15 -2 -2 10  8 18 -2  3 -2 -2 -2 -2  7 17  8  9 -2 17 -2 -2\n",
      " 13 -2 -2 21  8 21 -2 -2  4 -2 -2 -2  8 -2 11 27  6  5 -2 -2 -2  4 28 15\n",
      "  4 -2 -2 -2 -2 12 16 29 -2 -2 13 -2 -2 -2 21 -2 16 -2 -2 29  8  9 24 17\n",
      "  4 -2 -2  7 -2  6 -2 -2  1  1 -2 -2 -2 26 -2 -2 12 -2 12 29 -2 -2  3 -2\n",
      " -2  4 14 18  6 -2 -2 -2  6 15 -2 -2 -2  7 25 10 -2 -2 -2 -2 24  9 22 -2\n",
      " 18 12 -2 27 27 -2 -2 -2 -2 21 18 23  9  1  1 -2 -2  9 -2 -2 -2 17 -2  2\n",
      " -2 -2 23  6 -2 -2 -2 23 -2  7 15 -2 -2  6 -2 -2 27 28  1  5 23 -2 -2 -2\n",
      " 25 29 19 -2 -2 29  0  5 15 -2 -2 13 21 -2 -2  1 -2 -2  9 26 -2 -2 12  4\n",
      " -2 -2 -2 13 18  0 -2 -2 -2 24  2 13 -2 -2 -2 -2 19  5 -2 -2 -2 10 -2  7\n",
      " 28 -2 26 -2 -2 11 -2 -2 16  5 27 17 -2  3 19 -2 -2 -2 -2 28 -2 -2 27 19\n",
      " -2 -2  2 -2 -2 29 24 26 25 10  8 -2 -2 -2 10 12 -2 15 -2 -2  2 -2 -2 22\n",
      " 21 -2 -2 22 -2 -2 10  8 -2 -2 13 -2  7 26 -2 -2 17 -2 -2 17 21 27  8 11\n",
      " 12 19 27 -2 -2 -2 16 10 -2 23 11 -2 -2 -2  2 -2  5 27 -2 -2 -2 28  0 21\n",
      " -2 29 -2 10 -2 -2 28  2  5 -2 -2 -2 29 17 -2 -2 13 -2 -2  6 15 11  0 -2\n",
      " 17  3 -2 -2 29  4 -2 -2 -2 -2  7 -2 -2 17 29  5 -2 -2 -2  3 -2 17 13 11\n",
      " -2 13 10 -2 -2 -2 26 22 -2 -2 24  0 17 22 -2 -2 20 -2 25 29 -2 -2 -2 -2\n",
      " 29 21 -2 -2 29 -2 -2 29  8 -2 12 -2 -2 20 12  8 -2 -2 -2  2 -2  4 -2  5\n",
      " -2 -2  6 15 -2 -2 -2 15  0  3 21 18 11 -2 -2 26 -2 20 -2 -2 28  0 -2 -2\n",
      " -2 10  4  0 -2 -2  7 -2 -2 17 20 -2 -2 -2  7 -2 -2  2 -2 15  8 14 -2 -2\n",
      " -2 -2 10 26 -2  4 -2 -2 -2 -2 18 18 15  2 22 11 -2 22 -2 -2 22 -2 -2 -2\n",
      "  7 29 17  0 21 10 26 -2 -2 -2 15 -2 -2 -2  4 18 -2 -2 -2 25 16 13  1 -2\n",
      " -2 -2 26 13 -2 -2 -2 20 -2 -2 26 -2 14 -2 -2 26 15  7 12 -2 -2 -2 29 -2\n",
      "  5  9 28 -2 -2  5 -2 18 -2 12 12 10 -2 -2 -2 -2 26 29 -2 14  2 17 15 18\n",
      " -2 -2 -2 -2  9 23 -2 -2 25 -2 -2 -2 -2  7 13 -2  2 -2 -2 16 14 21 -2 -2\n",
      " 18 -2 -2 -2 18 18  9  5 16 25 -2  9 14 11 25 26 19 -2 -2  4 29 19 -2 -2\n",
      " -2 -2 24 -2 -2  5 14  1  0 16 -2 -2 -2 27 -2 -2 19 -2 24 15 -2  9 -2 -2\n",
      " -2  0 -2 13 -2 -2 28 21 -2 13 -2 -2 14 -2 -2  2 -2 -2 25 -2 -2 13 25 -2\n",
      " 18 -2 -2  4 -2 -2 -2 27 28 27 -2  3 -2 -2  2 10  0 -2 -2 19 10 -2 -2 26\n",
      " -2 -2 14 23 -2 12 -2 -2 -2 17 10 -2 -2 -2 18 11 15 -2 -2 12  1  9 21 -2\n",
      " -2 13 -2 -2 16 -2 -2  4 -2 -2 12 18  4  5 -2 -2 -2 -2 11  1  1 11  8 -2\n",
      " -2 18 -2 -2 13 -2 -2 22 -2 17 18 -2 -2 -2 -2]\n"
     ]
    }
   ],
   "source": [
    "n_nodes = tree_regressor.tree_.node_count\n",
    "children_left = tree_regressor.tree_.children_left\n",
    "children_right = tree_regressor.tree_.children_right\n",
    "feature = tree_regressor.tree_.feature\n",
    "threshold = tree_regressor.tree_.threshold\n",
    "print(feature)\n",
    "\n",
    "#clf = clf.fit(iris.data, iris.target)\n",
    "tree.export_graphviz(tree_regressor,out_file='tree.dot') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# poly_reg = PolynomialFeatures(degree = 1)\n",
    "# X_poly = poly_reg.fit_transform(X_Train)\n",
    "# poly_reg_model = LinearRegression()\n",
    "# poly_reg_model.fit(X_poly, Y_Train)\n",
    "\n",
    "# y_pred = poly_reg_model.predict(X_Test)\n",
    "\n",
    "# print(smape_kun(Y_Test,y_pred))\n",
    "# #print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiclass\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "\n",
    "lab_enc = preprocessing.LabelEncoder()\n",
    "Y_Train = lab_enc.fit_transform(Y_Train)\n",
    "print(utils.multiclass.type_of_target(Y_Train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.34879909224844\n"
     ]
    }
   ],
   "source": [
    "trainedmodel = LogisticRegression().fit(X_Train,Y_Train)\n",
    "predictions =trainedmodel.predict(X_Test)\n",
    "print(smape_kun(Y_Test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.17716928709699\n"
     ]
    }
   ],
   "source": [
    "trainedforest = RandomForestClassifier(n_estimators=700).fit(X_Train,Y_Train)\n",
    "predictionforest = trainedforest.predict(X_Test)\n",
    "print(smape_kun(Y_Test,predictionforest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.102591933409116\n"
     ]
    }
   ],
   "source": [
    "trainedsvm = svm.LinearSVC().fit(X_Train, Y_Train)\n",
    "predictionsvm = trainedsvm.predict(X_Test)\n",
    "print(smape_kun(Y_Test,predictionsvm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.102591933409116\n"
     ]
    }
   ],
   "source": [
    "trainedtree = tree.DecisionTreeClassifier().fit(X_Train, Y_Train)\n",
    "predictionstree = trainedtree.predict(X_Test)\n",
    "print(smape_kun(Y_Test,predictionsvm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.918068761306586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "trainedlda = LinearDiscriminantAnalysis().fit(X_Train, Y_Train)\n",
    "predictionlda = trainedlda.predict(X_Test)\n",
    "print(smape_kun(Y_Test,predictionlda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174.86033285782491\n"
     ]
    }
   ],
   "source": [
    "trainednb = GaussianNB().fit(X_Train, Y_Train)\n",
    "predictionnb = trainednb.predict(X_Test)\n",
    "print(smape_kun(Y_Test,predictionnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.43621080149953\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial').fit(X_Train,Y_Train)\n",
    "\n",
    "y_pred = clf.predict(X_Test)\n",
    "\n",
    "print(smape_kun(Y_Test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def smape_kun2(y_true, y_pred):\n",
    "    return K.mean((np.abs(y_pred - y_true) * 200/ (np.abs(y_pred) + np.abs(y_true))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 427 samples, validate on 48 samples\n",
      "Epoch 1/10\n",
      " - 7s - loss: 198.3986 - val_loss: 198.3949\n",
      "Epoch 2/10\n",
      " - 0s - loss: 198.3986 - val_loss: 198.3949\n",
      "Epoch 3/10\n",
      " - 0s - loss: 198.3986 - val_loss: 198.3949\n",
      "Epoch 4/10\n",
      " - 0s - loss: 198.3986 - val_loss: 198.3949\n",
      "Epoch 5/10\n",
      " - 0s - loss: 198.3986 - val_loss: 198.3949\n",
      "Epoch 6/10\n",
      " - 0s - loss: 198.3986 - val_loss: 198.3949\n",
      "Epoch 7/10\n",
      " - 0s - loss: 198.3986 - val_loss: 198.3949\n",
      "Epoch 8/10\n",
      " - 0s - loss: 198.3986 - val_loss: 198.3949\n",
      "Epoch 9/10\n",
      " - 0s - loss: 198.3986 - val_loss: 198.3949\n",
      "Epoch 10/10\n",
      " - 0s - loss: 198.3986 - val_loss: 198.3949\n",
      "Generating test predictions...\n",
      "198.1458501453552\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X_train = X_Train\n",
    "y_train = Y_Train\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "#nb_classes = Y_Train.shape[1]\n",
    "\n",
    "# Here's a Deep Dumb MLP (DDMLP)\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=input_dim))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# we'll use categorical xent for the loss, and RMSprop as the optimizer\n",
    "model.compile(loss=smape_kun2, optimizer='rmsprop')\n",
    "\n",
    "print(\"Training...\")\n",
    "model.fit(X_train, y_train, nb_epoch=10, validation_split=0.1,verbose=2)\n",
    "\n",
    "print(\"Generating test predictions...\")\n",
    "preds = model.predict_classes(X_Test, verbose=0)\n",
    "\n",
    "preds = model.predict(X_Test)\n",
    "def twoone(list1):\n",
    "    return [val for lst in list1 for val in lst]\n",
    "\n",
    "preds = twoone(preds)\n",
    "\n",
    "print(smape_kun(Y_Test,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=30, units=256)`\n",
      "  import sys\n",
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=128)`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=64)`\n",
      "  \n",
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 427 samples, validate on 48 samples\n",
      "Epoch 1/1000\n",
      " - 7s - loss: 199.7936 - mean_squared_error: 200855.6350 - mean_absolute_error: 359.9769 - val_loss: 199.8989 - val_mean_squared_error: 239648.8906 - val_mean_absolute_error: 406.2511\n",
      "Epoch 2/1000\n",
      " - 0s - loss: 199.0881 - mean_squared_error: 200929.9507 - mean_absolute_error: 360.1365 - val_loss: 199.8147 - val_mean_squared_error: 239731.1406 - val_mean_absolute_error: 406.4583\n",
      "Epoch 3/1000\n",
      " - 0s - loss: 198.5350 - mean_squared_error: 201024.6970 - mean_absolute_error: 360.3620 - val_loss: 199.7036 - val_mean_squared_error: 239838.6406 - val_mean_absolute_error: 406.7338\n",
      "Epoch 4/1000\n",
      " - 0s - loss: 197.9387 - mean_squared_error: 201140.2840 - mean_absolute_error: 360.6442 - val_loss: 199.5602 - val_mean_squared_error: 239931.8281 - val_mean_absolute_error: 407.0365\n",
      "Epoch 5/1000\n",
      " - 0s - loss: 197.3886 - mean_squared_error: 201230.7729 - mean_absolute_error: 360.9322 - val_loss: 199.3915 - val_mean_squared_error: 239984.4219 - val_mean_absolute_error: 407.3373\n",
      "Epoch 6/1000\n",
      " - 0s - loss: 196.9860 - mean_squared_error: 201295.8859 - mean_absolute_error: 361.2365 - val_loss: 199.1882 - val_mean_squared_error: 239991.3281 - val_mean_absolute_error: 407.6406\n",
      "Epoch 7/1000\n",
      " - 0s - loss: 196.3649 - mean_squared_error: 201324.6765 - mean_absolute_error: 361.5354 - val_loss: 198.9260 - val_mean_squared_error: 239942.3125 - val_mean_absolute_error: 407.9310\n",
      "Epoch 8/1000\n",
      " - 0s - loss: 195.8065 - mean_squared_error: 201294.7554 - mean_absolute_error: 361.8042 - val_loss: 198.6177 - val_mean_squared_error: 239841.4844 - val_mean_absolute_error: 408.1968\n",
      "Epoch 9/1000\n",
      " - 0s - loss: 195.6130 - mean_squared_error: 201239.4566 - mean_absolute_error: 362.1095 - val_loss: 198.2756 - val_mean_squared_error: 239696.6719 - val_mean_absolute_error: 408.4410\n",
      "Epoch 10/1000\n",
      " - 0s - loss: 195.2014 - mean_squared_error: 201131.7550 - mean_absolute_error: 362.3306 - val_loss: 197.9238 - val_mean_squared_error: 239468.8750 - val_mean_absolute_error: 408.5780\n",
      "Epoch 11/1000\n",
      " - 0s - loss: 194.9746 - mean_squared_error: 200944.2067 - mean_absolute_error: 362.4806 - val_loss: 197.5352 - val_mean_squared_error: 239150.2031 - val_mean_absolute_error: 408.6252\n",
      "Epoch 12/1000\n",
      " - 0s - loss: 194.6209 - mean_squared_error: 200664.2834 - mean_absolute_error: 362.5263 - val_loss: 197.1137 - val_mean_squared_error: 238699.3906 - val_mean_absolute_error: 408.5088\n",
      "Epoch 13/1000\n",
      " - 0s - loss: 194.2438 - mean_squared_error: 200271.7279 - mean_absolute_error: 362.4146 - val_loss: 196.5860 - val_mean_squared_error: 238123.5469 - val_mean_absolute_error: 408.2695\n",
      "Epoch 14/1000\n",
      " - 0s - loss: 193.7788 - mean_squared_error: 199748.4606 - mean_absolute_error: 362.1736 - val_loss: 195.9039 - val_mean_squared_error: 237337.9219 - val_mean_absolute_error: 407.7581\n",
      "Epoch 15/1000\n",
      " - 0s - loss: 193.0639 - mean_squared_error: 199014.4934 - mean_absolute_error: 361.5807 - val_loss: 195.0753 - val_mean_squared_error: 236288.7500 - val_mean_absolute_error: 406.8945\n",
      "Epoch 16/1000\n",
      " - 0s - loss: 192.1902 - mean_squared_error: 198026.4283 - mean_absolute_error: 360.6759 - val_loss: 194.1592 - val_mean_squared_error: 234973.5000 - val_mean_absolute_error: 405.6862\n",
      "Epoch 17/1000\n",
      " - 0s - loss: 191.0707 - mean_squared_error: 196822.2979 - mean_absolute_error: 359.4416 - val_loss: 193.1150 - val_mean_squared_error: 233357.1250 - val_mean_absolute_error: 404.1064\n",
      "Epoch 18/1000\n",
      " - 0s - loss: 189.5629 - mean_squared_error: 195367.5951 - mean_absolute_error: 357.7880 - val_loss: 191.9906 - val_mean_squared_error: 231381.2969 - val_mean_absolute_error: 402.0500\n",
      "Epoch 19/1000\n",
      " - 0s - loss: 187.7289 - mean_squared_error: 193531.0574 - mean_absolute_error: 355.6153 - val_loss: 190.7347 - val_mean_squared_error: 229005.0000 - val_mean_absolute_error: 399.4474\n",
      "Epoch 20/1000\n",
      " - 0s - loss: 185.5313 - mean_squared_error: 191250.7518 - mean_absolute_error: 352.8703 - val_loss: 189.1724 - val_mean_squared_error: 226230.2500 - val_mean_absolute_error: 396.3619\n",
      "Epoch 21/1000\n",
      " - 0s - loss: 183.0788 - mean_squared_error: 188643.3418 - mean_absolute_error: 349.7030 - val_loss: 186.8817 - val_mean_squared_error: 223053.9531 - val_mean_absolute_error: 392.8406\n",
      "Epoch 22/1000\n",
      " - 0s - loss: 180.5497 - mean_squared_error: 185809.5442 - mean_absolute_error: 346.2990 - val_loss: 184.2065 - val_mean_squared_error: 219615.9219 - val_mean_absolute_error: 389.3223\n",
      "Epoch 23/1000\n",
      " - 0s - loss: 177.4982 - mean_squared_error: 182648.8593 - mean_absolute_error: 342.7226 - val_loss: 181.1381 - val_mean_squared_error: 215824.7969 - val_mean_absolute_error: 385.6814\n",
      "Epoch 24/1000\n",
      " - 0s - loss: 173.8071 - mean_squared_error: 179143.5420 - mean_absolute_error: 339.0955 - val_loss: 177.8712 - val_mean_squared_error: 211721.9844 - val_mean_absolute_error: 382.0980\n",
      "Epoch 25/1000\n",
      " - 0s - loss: 169.5263 - mean_squared_error: 175485.5055 - mean_absolute_error: 335.3892 - val_loss: 174.2162 - val_mean_squared_error: 207065.0625 - val_mean_absolute_error: 378.0534\n",
      "Epoch 26/1000\n",
      " - 0s - loss: 164.8934 - mean_squared_error: 171187.6234 - mean_absolute_error: 331.0415 - val_loss: 169.8956 - val_mean_squared_error: 201536.3906 - val_mean_absolute_error: 372.7046\n",
      "Epoch 27/1000\n",
      " - 0s - loss: 159.7134 - mean_squared_error: 166008.7172 - mean_absolute_error: 325.3204 - val_loss: 164.7443 - val_mean_squared_error: 195105.0000 - val_mean_absolute_error: 366.0282\n",
      "Epoch 28/1000\n",
      " - 0s - loss: 153.7066 - mean_squared_error: 160268.9663 - mean_absolute_error: 318.3114 - val_loss: 158.7761 - val_mean_squared_error: 187769.1406 - val_mean_absolute_error: 358.0862\n",
      "Epoch 29/1000\n",
      " - 0s - loss: 147.4468 - mean_squared_error: 153418.3399 - mean_absolute_error: 310.0409 - val_loss: 152.2810 - val_mean_squared_error: 179686.4219 - val_mean_absolute_error: 349.3166\n",
      "Epoch 30/1000\n",
      " - 0s - loss: 140.9638 - mean_squared_error: 146004.0095 - mean_absolute_error: 300.9801 - val_loss: 145.5425 - val_mean_squared_error: 170931.6562 - val_mean_absolute_error: 339.8389\n",
      "Epoch 31/1000\n",
      " - 0s - loss: 133.7811 - mean_squared_error: 138435.0036 - mean_absolute_error: 291.6638 - val_loss: 138.7622 - val_mean_squared_error: 161620.8906 - val_mean_absolute_error: 329.8132\n",
      "Epoch 32/1000\n",
      " - 0s - loss: 126.2055 - mean_squared_error: 130027.3879 - mean_absolute_error: 281.3327 - val_loss: 131.4042 - val_mean_squared_error: 151392.9219 - val_mean_absolute_error: 318.3444\n",
      "Epoch 33/1000\n",
      " - 0s - loss: 118.6127 - mean_squared_error: 120887.3352 - mean_absolute_error: 269.6285 - val_loss: 123.0832 - val_mean_squared_error: 139934.8125 - val_mean_absolute_error: 304.3276\n",
      "Epoch 34/1000\n",
      " - 0s - loss: 110.1613 - mean_squared_error: 111035.5733 - mean_absolute_error: 255.6055 - val_loss: 114.6073 - val_mean_squared_error: 127581.3984 - val_mean_absolute_error: 289.0918\n",
      "Epoch 35/1000\n",
      " - 0s - loss: 103.4974 - mean_squared_error: 100400.0825 - mean_absolute_error: 241.1628 - val_loss: 105.4424 - val_mean_squared_error: 114731.3750 - val_mean_absolute_error: 272.7495\n",
      "Epoch 36/1000\n",
      " - 0s - loss: 95.8249 - mean_squared_error: 89681.6858 - mean_absolute_error: 226.4817 - val_loss: 91.7807 - val_mean_squared_error: 101470.2109 - val_mean_absolute_error: 255.3828\n",
      "Epoch 37/1000\n",
      " - 0s - loss: 84.7997 - mean_squared_error: 78198.2572 - mean_absolute_error: 210.5892 - val_loss: 83.9288 - val_mean_squared_error: 88239.9922 - val_mean_absolute_error: 237.6844\n",
      "Epoch 38/1000\n",
      " - 0s - loss: 75.5700 - mean_squared_error: 67246.2437 - mean_absolute_error: 195.1176 - val_loss: 76.0402 - val_mean_squared_error: 75261.2656 - val_mean_absolute_error: 220.2806\n",
      "Epoch 39/1000\n",
      " - 0s - loss: 68.1686 - mean_squared_error: 57374.3656 - mean_absolute_error: 181.0236 - val_loss: 68.2255 - val_mean_squared_error: 62926.1367 - val_mean_absolute_error: 201.9798\n",
      "Epoch 40/1000\n",
      " - 0s - loss: 61.9810 - mean_squared_error: 47582.7104 - mean_absolute_error: 166.9858 - val_loss: 61.2653 - val_mean_squared_error: 51595.3008 - val_mean_absolute_error: 183.9570\n",
      "Epoch 41/1000\n",
      " - 0s - loss: 57.3199 - mean_squared_error: 39868.5303 - mean_absolute_error: 155.6591 - val_loss: 55.1489 - val_mean_squared_error: 41679.0078 - val_mean_absolute_error: 166.4073\n",
      "Epoch 42/1000\n",
      " - 0s - loss: 53.2822 - mean_squared_error: 32798.6648 - mean_absolute_error: 144.2846 - val_loss: 50.1028 - val_mean_squared_error: 33618.6445 - val_mean_absolute_error: 150.0597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/1000\n",
      " - 0s - loss: 50.3356 - mean_squared_error: 28138.9954 - mean_absolute_error: 135.4953 - val_loss: 46.1601 - val_mean_squared_error: 27448.7012 - val_mean_absolute_error: 135.3173\n",
      "Epoch 44/1000\n",
      " - 0s - loss: 48.3893 - mean_squared_error: 24785.5562 - mean_absolute_error: 128.1077 - val_loss: 42.8395 - val_mean_squared_error: 23157.7500 - val_mean_absolute_error: 121.1001\n",
      "Epoch 45/1000\n",
      " - 0s - loss: 47.1990 - mean_squared_error: 22885.9213 - mean_absolute_error: 122.4502 - val_loss: 40.4085 - val_mean_squared_error: 20537.1816 - val_mean_absolute_error: 108.4279\n",
      "Epoch 46/1000\n",
      " - 0s - loss: 46.5108 - mean_squared_error: 21915.3667 - mean_absolute_error: 118.1790 - val_loss: 39.4949 - val_mean_squared_error: 19289.6055 - val_mean_absolute_error: 102.8799\n",
      "Epoch 47/1000\n",
      " - 0s - loss: 46.2652 - mean_squared_error: 21928.9038 - mean_absolute_error: 116.4363 - val_loss: 39.3178 - val_mean_squared_error: 18822.1348 - val_mean_absolute_error: 101.8852\n",
      "Epoch 48/1000\n",
      " - 0s - loss: 46.1553 - mean_squared_error: 22075.5300 - mean_absolute_error: 115.9476 - val_loss: 39.1984 - val_mean_squared_error: 18643.8223 - val_mean_absolute_error: 101.8194\n",
      "Epoch 49/1000\n",
      " - 0s - loss: 45.8844 - mean_squared_error: 21800.5282 - mean_absolute_error: 115.0282 - val_loss: 38.7542 - val_mean_squared_error: 18240.4238 - val_mean_absolute_error: 100.4979\n",
      "Epoch 50/1000\n",
      " - 0s - loss: 45.2875 - mean_squared_error: 20958.6531 - mean_absolute_error: 112.6465 - val_loss: 37.9161 - val_mean_squared_error: 17447.9629 - val_mean_absolute_error: 97.2322\n",
      "Epoch 51/1000\n",
      " - 0s - loss: 44.2411 - mean_squared_error: 19572.9581 - mean_absolute_error: 108.1858 - val_loss: 36.8865 - val_mean_squared_error: 16432.4160 - val_mean_absolute_error: 92.9738\n",
      "Epoch 52/1000\n",
      " - 0s - loss: 43.0657 - mean_squared_error: 17966.2940 - mean_absolute_error: 102.7680 - val_loss: 35.8200 - val_mean_squared_error: 15373.9678 - val_mean_absolute_error: 88.3510\n",
      "Epoch 53/1000\n",
      " - 0s - loss: 41.8055 - mean_squared_error: 16314.4994 - mean_absolute_error: 96.9770 - val_loss: 35.3727 - val_mean_squared_error: 14507.1719 - val_mean_absolute_error: 85.2915\n",
      "Epoch 54/1000\n",
      " - 0s - loss: 40.8799 - mean_squared_error: 14948.5294 - mean_absolute_error: 92.7080 - val_loss: 35.2740 - val_mean_squared_error: 13936.0586 - val_mean_absolute_error: 83.2738\n",
      "Epoch 55/1000\n",
      " - 0s - loss: 40.1501 - mean_squared_error: 13935.3639 - mean_absolute_error: 89.4039 - val_loss: 35.4089 - val_mean_squared_error: 13636.2773 - val_mean_absolute_error: 82.5687\n",
      "Epoch 56/1000\n",
      " - 0s - loss: 39.7308 - mean_squared_error: 13217.6640 - mean_absolute_error: 87.3618 - val_loss: 35.6155 - val_mean_squared_error: 13465.2607 - val_mean_absolute_error: 82.6139\n",
      "Epoch 57/1000\n",
      " - 0s - loss: 39.4683 - mean_squared_error: 12752.8942 - mean_absolute_error: 86.1877 - val_loss: 35.8267 - val_mean_squared_error: 13358.6826 - val_mean_absolute_error: 82.7575\n",
      "Epoch 58/1000\n",
      " - 0s - loss: 39.1342 - mean_squared_error: 12407.3886 - mean_absolute_error: 84.9253 - val_loss: 35.9809 - val_mean_squared_error: 13247.4688 - val_mean_absolute_error: 82.6977\n",
      "Epoch 59/1000\n",
      " - 0s - loss: 38.8438 - mean_squared_error: 12174.5604 - mean_absolute_error: 83.8099 - val_loss: 36.0289 - val_mean_squared_error: 13117.6064 - val_mean_absolute_error: 82.4125\n",
      "Epoch 60/1000\n",
      " - 0s - loss: 38.4380 - mean_squared_error: 11950.5562 - mean_absolute_error: 82.3987 - val_loss: 35.9386 - val_mean_squared_error: 12978.3291 - val_mean_absolute_error: 81.8576\n",
      "Epoch 61/1000\n",
      " - 0s - loss: 38.0242 - mean_squared_error: 11768.0323 - mean_absolute_error: 80.9604 - val_loss: 35.8101 - val_mean_squared_error: 12884.8594 - val_mean_absolute_error: 81.3952\n",
      "Epoch 62/1000\n",
      " - 0s - loss: 37.5985 - mean_squared_error: 11641.9516 - mean_absolute_error: 79.5664 - val_loss: 35.7827 - val_mean_squared_error: 12853.5625 - val_mean_absolute_error: 81.3853\n",
      "Epoch 63/1000\n",
      " - 0s - loss: 37.3098 - mean_squared_error: 11626.5180 - mean_absolute_error: 78.7755 - val_loss: 35.7838 - val_mean_squared_error: 12900.5469 - val_mean_absolute_error: 81.5827\n",
      "Epoch 64/1000\n",
      " - 0s - loss: 37.0728 - mean_squared_error: 11614.5622 - mean_absolute_error: 78.1895 - val_loss: 35.8494 - val_mean_squared_error: 12938.9766 - val_mean_absolute_error: 81.9480\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.objectives import MSE, MAE\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=X_Train.shape[1], output_dim=256))\n",
    "# model.add(Activation(\"tanh\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "#model.add(Dropout(0.50))\n",
    "model.add(Dense(output_dim=128))\n",
    "model.add(Activation(\"relu\"))\n",
    "#model.add(Dropout(0.50))\n",
    "model.add(Dense(output_dim=64))\n",
    "model.add(Activation(\"relu\"))\n",
    "#model.add(Dropout(0.50))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(loss=smape_kun2, optimizer='adam', metrics=['mse','mae'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "train_log = model.fit(X_Train, Y_Train.values, batch_size=256, nb_epoch=1000, validation_split=0.1, verbose=2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[911.6407, 187.3954, 571.9978, 957.6853, 495.19437, 229.01299, 921.4407, 247.2915, 712.6823, 254.09769, 276.24014, 280.7891, 627.2105, 261.51898, 694.2395, 815.4807, 273.53857, 164.92758, 250.02097, 232.25717, 940.82605, 244.24734, 228.15686, 175.56685, 142.95177]\n",
      "35.24893362544566\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(X_Test)\n",
    "def twoone(list1):\n",
    "    return [val for lst in list1 for val in lst]\n",
    "\n",
    "pred = twoone(preds)\n",
    "print(pred)\n",
    "print(smape_kun(Y_Test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wider: -39.94 (9.28) MSE\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def wider_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(20, input_dim=30, kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss=smape_kun2, optimizer='adam')\n",
    "\treturn model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=wider_model, epochs=100, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(pipeline, X_Train, Y_Train, cv=kfold, n_jobs=1)\n",
    "print(\"Wider: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[911.6407, 187.3954, 571.9978, 957.6853, 495.19437, 229.01299, 921.4407, 247.2915, 712.6823, 254.09769, 276.24014, 280.7891, 627.2105, 261.51898, 694.2395, 815.4807, 273.53857, 164.92758, 250.02097, 232.25717, 940.82605, 244.24734, 228.15686, 175.56685, 142.95177]\n",
      "35.24893362544566\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(X_Test)\n",
    "\n",
    "preds = twoone(preds)\n",
    "print(preds)\n",
    "print(smape_kun(Y_Test,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "475/475 [==============================] - 27s 57ms/step - loss: 202821.8991 - mean_absolute_error: 362.6674 - acc: 0.0232\n",
      "Epoch 2/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 196451.7145 - mean_absolute_error: 355.0742 - acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 194513.5022 - mean_absolute_error: 352.6288 - acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 193337.2537 - mean_absolute_error: 351.2096 - acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      "475/475 [==============================] - 6s 13ms/step - loss: 192200.6888 - mean_absolute_error: 349.8334 - acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      "475/475 [==============================] - 6s 12ms/step - loss: 191086.2868 - mean_absolute_error: 348.5499 - acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      "475/475 [==============================] - 6s 12ms/step - loss: 190031.5357 - mean_absolute_error: 347.2829 - acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 188985.2757 - mean_absolute_error: 346.0319 - acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 187968.7251 - mean_absolute_error: 344.7933 - acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 186963.3622 - mean_absolute_error: 343.5812 - acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 185971.4799 - mean_absolute_error: 342.3519 - acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      "475/475 [==============================] - 6s 12ms/step - loss: 184977.0743 - mean_absolute_error: 341.1631 - acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      "475/475 [==============================] - 6s 13ms/step - loss: 184011.3050 - mean_absolute_error: 339.9652 - acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      "475/475 [==============================] - 6s 12ms/step - loss: 183050.8482 - mean_absolute_error: 338.7816 - acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 182093.1026 - mean_absolute_error: 337.6314 - acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 181157.1225 - mean_absolute_error: 336.4378 - acc: 0.0000e+00\n",
      "Epoch 17/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 180212.7833 - mean_absolute_error: 335.2804 - acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 179293.7012 - mean_absolute_error: 334.1387 - acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      "475/475 [==============================] - 6s 12ms/step - loss: 178364.1012 - mean_absolute_error: 332.9602 - acc: 0.0000e+00\n",
      "Epoch 20/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 177458.4034 - mean_absolute_error: 331.8382 - acc: 0.0000e+00\n",
      "Epoch 21/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 176550.2337 - mean_absolute_error: 330.7085 - acc: 0.0000e+00\n",
      "Epoch 22/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 175650.2513 - mean_absolute_error: 329.5539 - acc: 0.0000e+00\n",
      "Epoch 23/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 174759.2440 - mean_absolute_error: 328.4249 - acc: 0.0000e+00\n",
      "Epoch 24/100\n",
      "475/475 [==============================] - 6s 13ms/step - loss: 173870.4009 - mean_absolute_error: 327.2968 - acc: 0.0000e+00\n",
      "Epoch 25/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 173004.0581 - mean_absolute_error: 326.1725 - acc: 0.0000e+00\n",
      "Epoch 26/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 172118.8774 - mean_absolute_error: 325.0427 - acc: 0.0000e+00\n",
      "Epoch 27/100\n",
      "475/475 [==============================] - 6s 12ms/step - loss: 171253.8317 - mean_absolute_error: 323.9431 - acc: 0.0000e+00\n",
      "Epoch 28/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 170402.9777 - mean_absolute_error: 322.8299 - acc: 0.0000e+00\n",
      "Epoch 29/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 169539.5638 - mean_absolute_error: 321.7165 - acc: 0.0000e+00\n",
      "Epoch 30/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 168689.4277 - mean_absolute_error: 320.6176 - acc: 0.0000e+00\n",
      "Epoch 31/100\n",
      "475/475 [==============================] - 5s 10ms/step - loss: 167858.1762 - mean_absolute_error: 319.5221 - acc: 0.0000e+00\n",
      "Epoch 32/100\n",
      "475/475 [==============================] - 5s 10ms/step - loss: 167008.9288 - mean_absolute_error: 318.4267 - acc: 0.0000e+00\n",
      "Epoch 33/100\n",
      "475/475 [==============================] - 5s 10ms/step - loss: 166188.2261 - mean_absolute_error: 317.3114 - acc: 0.0000e+00\n",
      "Epoch 34/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 165361.4506 - mean_absolute_error: 316.2446 - acc: 0.0000e+00\n",
      "Epoch 35/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 164545.5386 - mean_absolute_error: 315.1523 - acc: 0.0000e+00\n",
      "Epoch 36/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 163729.3075 - mean_absolute_error: 314.0737 - acc: 0.0000e+00\n",
      "Epoch 37/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 162919.1214 - mean_absolute_error: 313.0038 - acc: 0.0000e+00\n",
      "Epoch 38/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 162120.1062 - mean_absolute_error: 311.9211 - acc: 0.0000e+00\n",
      "Epoch 39/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 161334.5730 - mean_absolute_error: 310.8598 - acc: 0.0000e+00\n",
      "Epoch 40/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 160532.8241 - mean_absolute_error: 309.7986 - acc: 0.0000e+00\n",
      "Epoch 41/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 159751.1451 - mean_absolute_error: 308.7143 - acc: 0.0000e+00\n",
      "Epoch 42/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 158973.3361 - mean_absolute_error: 307.6895 - acc: 0.0000e+00\n",
      "Epoch 43/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 158195.3899 - mean_absolute_error: 306.6490 - acc: 0.0000e+00\n",
      "Epoch 44/100\n",
      "475/475 [==============================] - 5s 10ms/step - loss: 157428.2433 - mean_absolute_error: 305.5939 - acc: 0.0000e+00\n",
      "Epoch 45/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 156676.3204 - mean_absolute_error: 304.5663 - acc: 0.0000e+00\n",
      "Epoch 46/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 155905.9262 - mean_absolute_error: 303.5189 - acc: 0.0000e+00\n",
      "Epoch 47/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 155156.6999 - mean_absolute_error: 302.5084 - acc: 0.0000e+00\n",
      "Epoch 48/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 154403.5342 - mean_absolute_error: 301.4848 - acc: 0.0000e+00\n",
      "Epoch 49/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 153669.1100 - mean_absolute_error: 300.4769 - acc: 0.0000e+00\n",
      "Epoch 50/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 152930.4702 - mean_absolute_error: 299.4812 - acc: 0.0000e+00\n",
      "Epoch 51/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 152204.5099 - mean_absolute_error: 298.4765 - acc: 0.0000e+00\n",
      "Epoch 52/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 151476.2298 - mean_absolute_error: 297.4547 - acc: 0.0000e+00\n",
      "Epoch 53/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 150739.7727 - mean_absolute_error: 296.4876 - acc: 0.0000e+00\n",
      "Epoch 54/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 150042.6923 - mean_absolute_error: 295.4933 - acc: 0.0000e+00\n",
      "Epoch 55/100\n",
      "475/475 [==============================] - 6s 12ms/step - loss: 149319.0248 - mean_absolute_error: 294.5338 - acc: 0.0000e+00\n",
      "Epoch 56/100\n",
      "475/475 [==============================] - 6s 13ms/step - loss: 148606.1578 - mean_absolute_error: 293.5529 - acc: 0.0000e+00\n",
      "Epoch 57/100\n",
      "475/475 [==============================] - 6s 12ms/step - loss: 147915.3223 - mean_absolute_error: 292.5640 - acc: 0.0000e+00\n",
      "Epoch 58/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 147221.8562 - mean_absolute_error: 291.6249 - acc: 0.0000e+00\n",
      "Epoch 59/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 146510.0205 - mean_absolute_error: 290.6619 - acc: 0.0000e+00\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "475/475 [==============================] - 4s 9ms/step - loss: 145837.8639 - mean_absolute_error: 289.7079 - acc: 0.0000e+00\n",
      "Epoch 61/100\n",
      "475/475 [==============================] - 4s 8ms/step - loss: 145141.7851 - mean_absolute_error: 288.7561 - acc: 0.0000e+00\n",
      "Epoch 62/100\n",
      "475/475 [==============================] - 4s 9ms/step - loss: 144472.5931 - mean_absolute_error: 287.8102 - acc: 0.0000e+00\n",
      "Epoch 63/100\n",
      "475/475 [==============================] - 4s 9ms/step - loss: 143795.0927 - mean_absolute_error: 286.8742 - acc: 0.0000e+00\n",
      "Epoch 64/100\n",
      "475/475 [==============================] - 4s 9ms/step - loss: 143118.9632 - mean_absolute_error: 285.9249 - acc: 0.0000e+00\n",
      "Epoch 65/100\n",
      "475/475 [==============================] - 4s 9ms/step - loss: 142465.5583 - mean_absolute_error: 285.0191 - acc: 0.0000e+00\n",
      "Epoch 66/100\n",
      "475/475 [==============================] - 4s 9ms/step - loss: 141800.0370 - mean_absolute_error: 284.0722 - acc: 0.0000e+00\n",
      "Epoch 67/100\n",
      "475/475 [==============================] - 5s 10ms/step - loss: 141137.5125 - mean_absolute_error: 283.1514 - acc: 0.0000e+00\n",
      "Epoch 68/100\n",
      "475/475 [==============================] - 4s 9ms/step - loss: 140492.6428 - mean_absolute_error: 282.2351 - acc: 0.0000e+00\n",
      "Epoch 69/100\n",
      "475/475 [==============================] - 4s 9ms/step - loss: 139845.8302 - mean_absolute_error: 281.3178 - acc: 0.0000e+00\n",
      "Epoch 70/100\n",
      "475/475 [==============================] - 4s 9ms/step - loss: 139209.0713 - mean_absolute_error: 280.3826 - acc: 0.0000e+00\n",
      "Epoch 71/100\n",
      "475/475 [==============================] - 5s 10ms/step - loss: 138561.9391 - mean_absolute_error: 279.4621 - acc: 0.0000e+00\n",
      "Epoch 72/100\n",
      "475/475 [==============================] - 5s 10ms/step - loss: 137938.7003 - mean_absolute_error: 278.5530 - acc: 0.0000e+00\n",
      "Epoch 73/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 137305.0189 - mean_absolute_error: 277.6297 - acc: 0.0000e+00\n",
      "Epoch 74/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 136672.9206 - mean_absolute_error: 276.7350 - acc: 0.0000e+00\n",
      "Epoch 75/100\n",
      "475/475 [==============================] - 5s 10ms/step - loss: 136066.6048 - mean_absolute_error: 275.8364 - acc: 0.0000e+00\n",
      "Epoch 76/100\n",
      "475/475 [==============================] - 5s 10ms/step - loss: 135449.7227 - mean_absolute_error: 274.9144 - acc: 0.0000e+00\n",
      "Epoch 77/100\n",
      "475/475 [==============================] - 4s 9ms/step - loss: 134846.5142 - mean_absolute_error: 274.0332 - acc: 0.0000e+00\n",
      "Epoch 78/100\n",
      "475/475 [==============================] - 4s 9ms/step - loss: 134218.5298 - mean_absolute_error: 273.1294 - acc: 0.0000e+00\n",
      "Epoch 79/100\n",
      "475/475 [==============================] - 5s 11ms/step - loss: 133630.9570 - mean_absolute_error: 272.2690 - acc: 0.0000e+00\n",
      "Epoch 80/100\n",
      "475/475 [==============================] - 4s 9ms/step - loss: 133025.8988 - mean_absolute_error: 271.4166 - acc: 0.0000e+00\n",
      "Epoch 81/100\n",
      "475/475 [==============================] - 4s 9ms/step - loss: 132429.6924 - mean_absolute_error: 270.5583 - acc: 0.0000e+00\n",
      "Epoch 82/100\n",
      "475/475 [==============================] - 4s 9ms/step - loss: 131844.4665 - mean_absolute_error: 269.7246 - acc: 0.0000e+00\n",
      "Epoch 83/100\n",
      "475/475 [==============================] - 4s 9ms/step - loss: 131271.3757 - mean_absolute_error: 268.8840 - acc: 0.0000e+00\n",
      "Epoch 84/100\n",
      "475/475 [==============================] - 5s 10ms/step - loss: 130658.3854 - mean_absolute_error: 268.0425 - acc: 0.0000e+00\n",
      "Epoch 85/100\n",
      "475/475 [==============================] - 4s 9ms/step - loss: 130102.5063 - mean_absolute_error: 267.2258 - acc: 0.0000e+00\n",
      "Epoch 86/100\n",
      "475/475 [==============================] - 5s 10ms/step - loss: 129525.3101 - mean_absolute_error: 266.3719 - acc: 0.0000e+00\n",
      "Epoch 87/100\n",
      "475/475 [==============================] - 4s 9ms/step - loss: 128960.6533 - mean_absolute_error: 265.5774 - acc: 0.0000e+00\n",
      "Epoch 88/100\n",
      "475/475 [==============================] - 5s 9ms/step - loss: 128381.2617 - mean_absolute_error: 264.7384 - acc: 0.0000e+00\n",
      "Epoch 89/100\n",
      "475/475 [==============================] - 5s 9ms/step - loss: 127832.4589 - mean_absolute_error: 263.9396 - acc: 0.0000e+00\n",
      "Epoch 90/100\n",
      "475/475 [==============================] - 4s 9ms/step - loss: 127273.8583 - mean_absolute_error: 263.1504 - acc: 0.0000e+00\n",
      "Epoch 91/100\n",
      "475/475 [==============================] - 4s 9ms/step - loss: 126726.0896 - mean_absolute_error: 262.3290 - acc: 0.0000e+00\n",
      "Epoch 92/100\n",
      "475/475 [==============================] - 4s 9ms/step - loss: 126168.1014 - mean_absolute_error: 261.5258 - acc: 0.0000e+00\n",
      "Epoch 93/100\n",
      "475/475 [==============================] - 4s 8ms/step - loss: 125626.6365 - mean_absolute_error: 260.7223 - acc: 0.0000e+00\n",
      "Epoch 94/100\n",
      "475/475 [==============================] - 4s 9ms/step - loss: 125090.4165 - mean_absolute_error: 259.9385 - acc: 0.0000e+00\n",
      "Epoch 95/100\n",
      "475/475 [==============================] - 5s 10ms/step - loss: 124553.9194 - mean_absolute_error: 259.1546 - acc: 0.0000e+00\n",
      "Epoch 96/100\n",
      "475/475 [==============================] - 4s 9ms/step - loss: 124020.5415 - mean_absolute_error: 258.3880 - acc: 0.0000e+00\n",
      "Epoch 97/100\n",
      "475/475 [==============================] - 4s 9ms/step - loss: 123487.1697 - mean_absolute_error: 257.5917 - acc: 0.0000e+00\n",
      "Epoch 98/100\n",
      "475/475 [==============================] - 4s 9ms/step - loss: 122964.9750 - mean_absolute_error: 256.8475 - acc: 0.0000e+00\n",
      "Epoch 99/100\n",
      "475/475 [==============================] - 5s 10ms/step - loss: 122450.5227 - mean_absolute_error: 256.0958 - acc: 0.0000e+00\n",
      "Epoch 100/100\n",
      "475/475 [==============================] - 4s 9ms/step - loss: 121929.7867 - mean_absolute_error: 255.3338 - acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b59dca2cf8>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "def smape_kun2(y_true, y_pred):\n",
    "    return K.mean((np.abs(y_pred - y_true) * 200/ (np.abs(y_pred) + np.abs(y_true))))\n",
    "\n",
    "from keras.layers import LSTM\n",
    "import keras.backend as K\n",
    "\n",
    "X_Train2 = np.reshape(X_Train, (X_Train.shape[0], X_Train.shape[1], 1))\n",
    "\n",
    "regressor = Sequential()\n",
    "\n",
    "regressor.add(LSTM(units = 100, return_sequences = True, input_shape = (X_Train2.shape[1],1)))\n",
    "# regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(LSTM(units = 100, return_sequences = True))\n",
    "# regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(LSTM(units = 100, return_sequences = True))\n",
    "# regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(LSTM(units = 100))\n",
    "# regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(Dense(units = 1))\n",
    "\n",
    "regressor.compile(optimizer = 'adam', loss='mean_squared_error',  metrics=['mae','accuracy'])\n",
    "\n",
    "regressor.fit(X_Train2, Y_Train, epochs = 100, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[142.48123, 142.48123, 142.48123, 142.48123, 142.48123, 142.48123, 142.48123, 142.48123, 142.48123, 142.48123, 142.48123, 142.48123, 142.48123, 142.48123, 142.48123, 142.48123, 142.48123, 142.48123, 142.48123, 142.48123, 142.48123, 142.48123, 142.48123, 142.48123, 142.48123]\n",
      "91.85384851003653\n"
     ]
    }
   ],
   "source": [
    "X_Test2 = np.reshape(X_Test, (X_Test.shape[0], X_Test.shape[1], 1))\n",
    "\n",
    "preds = regressor.predict(X_Test2)\n",
    "\n",
    "preds = twoone(preds)\n",
    "print(preds)\n",
    "print(smape_kun(Y_Test,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "475/475 [==============================] - 10s 20ms/step - loss: 204036.5968 - smape_kun2: 198.5021\n",
      "Epoch 2/500\n",
      "475/475 [==============================] - 0s 141us/step - loss: 202708.7019 - smape_kun2: 196.4696\n",
      "Epoch 3/500\n",
      "475/475 [==============================] - 0s 99us/step - loss: 201133.3982 - smape_kun2: 194.6221\n",
      "Epoch 4/500\n",
      "475/475 [==============================] - 0s 130us/step - loss: 198962.5322 - smape_kun2: 192.8430\n",
      "Epoch 5/500\n",
      "475/475 [==============================] - 0s 130us/step - loss: 195706.1873 - smape_kun2: 190.5870\n",
      "Epoch 6/500\n",
      "475/475 [==============================] - 0s 143us/step - loss: 191418.3061 - smape_kun2: 187.9163\n",
      "Epoch 7/500\n",
      "475/475 [==============================] - 0s 178us/step - loss: 185839.9747 - smape_kun2: 184.8510\n",
      "Epoch 8/500\n",
      "475/475 [==============================] - 0s 124us/step - loss: 178785.6684 - smape_kun2: 180.8041\n",
      "Epoch 9/500\n",
      "475/475 [==============================] - 0s 218us/step - loss: 170023.8937 - smape_kun2: 176.1281\n",
      "Epoch 10/500\n",
      "475/475 [==============================] - 0s 216us/step - loss: 161170.6503 - smape_kun2: 170.5348\n",
      "Epoch 11/500\n",
      "475/475 [==============================] - 0s 208us/step - loss: 151192.5098 - smape_kun2: 164.3745\n",
      "Epoch 12/500\n",
      "475/475 [==============================] - 0s 181us/step - loss: 143542.4468 - smape_kun2: 158.4061\n",
      "Epoch 13/500\n",
      "475/475 [==============================] - 0s 218us/step - loss: 137808.6543 - smape_kun2: 153.1916\n",
      "Epoch 14/500\n",
      "475/475 [==============================] - 0s 170us/step - loss: 133835.4497 - smape_kun2: 148.9678\n",
      "Epoch 15/500\n",
      "475/475 [==============================] - 0s 168us/step - loss: 130567.4682 - smape_kun2: 146.4039\n",
      "Epoch 16/500\n",
      "475/475 [==============================] - 0s 176us/step - loss: 127784.6199 - smape_kun2: 144.9697\n",
      "Epoch 17/500\n",
      "475/475 [==============================] - 0s 193us/step - loss: 124915.2141 - smape_kun2: 144.6079\n",
      "Epoch 18/500\n",
      "475/475 [==============================] - 0s 130us/step - loss: 122003.1788 - smape_kun2: 144.5424\n",
      "Epoch 19/500\n",
      "475/475 [==============================] - 0s 126us/step - loss: 118798.7246 - smape_kun2: 143.9884\n",
      "Epoch 20/500\n",
      "475/475 [==============================] - 0s 147us/step - loss: 115580.9678 - smape_kun2: 143.1008\n",
      "Epoch 21/500\n",
      "475/475 [==============================] - 0s 130us/step - loss: 112304.6534 - smape_kun2: 142.8189\n",
      "Epoch 22/500\n",
      "475/475 [==============================] - 0s 130us/step - loss: 108627.9089 - smape_kun2: 142.4969\n",
      "Epoch 23/500\n",
      "475/475 [==============================] - 0s 130us/step - loss: 104941.4925 - smape_kun2: 141.9222\n",
      "Epoch 24/500\n",
      "475/475 [==============================] - 0s 124us/step - loss: 101054.6867 - smape_kun2: 141.3998\n",
      "Epoch 25/500\n",
      "475/475 [==============================] - 0s 118us/step - loss: 96791.6244 - smape_kun2: 141.0719\n",
      "Epoch 26/500\n",
      "475/475 [==============================] - 0s 134us/step - loss: 92299.6879 - smape_kun2: 140.2917\n",
      "Epoch 27/500\n",
      "475/475 [==============================] - 0s 122us/step - loss: 87656.7114 - smape_kun2: 139.0290\n",
      "Epoch 28/500\n",
      "475/475 [==============================] - 0s 178us/step - loss: 82532.8526 - smape_kun2: 138.2623\n",
      "Epoch 29/500\n",
      "475/475 [==============================] - 0s 187us/step - loss: 77297.4564 - smape_kun2: 137.4894\n",
      "Epoch 30/500\n",
      "475/475 [==============================] - 0s 118us/step - loss: 71837.4653 - smape_kun2: 135.5474\n",
      "Epoch 31/500\n",
      "475/475 [==============================] - 0s 124us/step - loss: 66329.5076 - smape_kun2: 132.8144\n",
      "Epoch 32/500\n",
      "475/475 [==============================] - 0s 128us/step - loss: 60565.8304 - smape_kun2: 129.9466\n",
      "Epoch 33/500\n",
      "475/475 [==============================] - 0s 155us/step - loss: 54691.9507 - smape_kun2: 126.2962\n",
      "Epoch 34/500\n",
      "475/475 [==============================] - 0s 202us/step - loss: 49065.1886 - smape_kun2: 120.3007\n",
      "Epoch 35/500\n",
      "475/475 [==============================] - 0s 235us/step - loss: 43193.3256 - smape_kun2: 112.3049\n",
      "Epoch 36/500\n",
      "475/475 [==============================] - 0s 216us/step - loss: 37604.8490 - smape_kun2: 104.2107\n",
      "Epoch 37/500\n",
      "475/475 [==============================] - 0s 181us/step - loss: 32469.1539 - smape_kun2: 95.4743\n",
      "Epoch 38/500\n",
      "475/475 [==============================] - 0s 189us/step - loss: 27398.1257 - smape_kun2: 85.6950\n",
      "Epoch 39/500\n",
      "475/475 [==============================] - 0s 164us/step - loss: 23061.3950 - smape_kun2: 77.3420\n",
      "Epoch 40/500\n",
      "475/475 [==============================] - 0s 197us/step - loss: 19156.8218 - smape_kun2: 69.4382\n",
      "Epoch 41/500\n",
      "475/475 [==============================] - 0s 170us/step - loss: 15950.8269 - smape_kun2: 63.2039\n",
      "Epoch 42/500\n",
      "475/475 [==============================] - 0s 166us/step - loss: 13215.3480 - smape_kun2: 56.5138\n",
      "Epoch 43/500\n",
      "475/475 [==============================] - 0s 139us/step - loss: 11055.7746 - smape_kun2: 50.2729\n",
      "Epoch 44/500\n",
      "475/475 [==============================] - 0s 124us/step - loss: 9401.7835 - smape_kun2: 45.5309\n",
      "Epoch 45/500\n",
      "475/475 [==============================] - 0s 132us/step - loss: 8200.1962 - smape_kun2: 42.7479\n",
      "Epoch 46/500\n",
      "475/475 [==============================] - 0s 128us/step - loss: 7341.0385 - smape_kun2: 40.7274\n",
      "Epoch 47/500\n",
      "475/475 [==============================] - 0s 143us/step - loss: 6754.4061 - smape_kun2: 38.8838\n",
      "Epoch 48/500\n",
      "475/475 [==============================] - 0s 109us/step - loss: 6407.5610 - smape_kun2: 38.5067\n",
      "Epoch 49/500\n",
      "475/475 [==============================] - 0s 122us/step - loss: 6150.1786 - smape_kun2: 37.3894\n",
      "Epoch 50/500\n",
      "475/475 [==============================] - 0s 136us/step - loss: 5998.8090 - smape_kun2: 36.8786\n",
      "Epoch 51/500\n",
      "475/475 [==============================] - 0s 113us/step - loss: 5928.3962 - smape_kun2: 36.6664\n",
      "Epoch 52/500\n",
      "475/475 [==============================] - 0s 113us/step - loss: 5881.4781 - smape_kun2: 36.7584\n",
      "Epoch 53/500\n",
      "475/475 [==============================] - 0s 118us/step - loss: 5831.0183 - smape_kun2: 36.7802\n",
      "Epoch 54/500\n",
      "475/475 [==============================] - 0s 120us/step - loss: 5763.5724 - smape_kun2: 36.5979\n",
      "Epoch 55/500\n",
      "475/475 [==============================] - 0s 126us/step - loss: 5746.2117 - smape_kun2: 36.5936\n",
      "Epoch 56/500\n",
      "475/475 [==============================] - 0s 153us/step - loss: 5730.7687 - smape_kun2: 36.5587\n",
      "Epoch 57/500\n",
      "475/475 [==============================] - 0s 178us/step - loss: 5693.7329 - smape_kun2: 36.4627\n",
      "Epoch 58/500\n",
      "475/475 [==============================] - 0s 136us/step - loss: 5683.3443 - smape_kun2: 36.4360\n",
      "Epoch 59/500\n",
      "475/475 [==============================] - 0s 120us/step - loss: 5672.4209 - smape_kun2: 36.4369\n",
      "Epoch 60/500\n",
      "475/475 [==============================] - 0s 134us/step - loss: 5648.7028 - smape_kun2: 36.3392\n",
      "Epoch 61/500\n",
      "475/475 [==============================] - 0s 227us/step - loss: 5658.0760 - smape_kun2: 36.3996\n",
      "Epoch 62/500\n",
      "475/475 [==============================] - 0s 162us/step - loss: 5632.1975 - smape_kun2: 36.3181\n",
      "Epoch 63/500\n",
      "475/475 [==============================] - 0s 208us/step - loss: 5622.8333 - smape_kun2: 36.3208\n",
      "Epoch 64/500\n",
      "475/475 [==============================] - 0s 157us/step - loss: 5596.0034 - smape_kun2: 36.2729\n",
      "Epoch 65/500\n",
      "475/475 [==============================] - 0s 113us/step - loss: 5585.4278 - smape_kun2: 36.3125\n",
      "Epoch 66/500\n",
      "475/475 [==============================] - 0s 120us/step - loss: 5609.3201 - smape_kun2: 36.4847\n",
      "Epoch 67/500\n",
      "475/475 [==============================] - 0s 139us/step - loss: 5564.1719 - smape_kun2: 36.2805\n",
      "Epoch 68/500\n",
      "475/475 [==============================] - 0s 111us/step - loss: 5586.7558 - smape_kun2: 36.3304\n",
      "Epoch 69/500\n",
      "475/475 [==============================] - 0s 126us/step - loss: 5544.1418 - smape_kun2: 36.3607\n",
      "Epoch 70/500\n",
      "475/475 [==============================] - 0s 126us/step - loss: 5547.9500 - smape_kun2: 36.2431\n",
      "Epoch 71/500\n",
      "475/475 [==============================] - 0s 118us/step - loss: 5534.4092 - smape_kun2: 36.2659\n",
      "Epoch 72/500\n",
      "475/475 [==============================] - 0s 139us/step - loss: 5537.5880 - smape_kun2: 36.2853\n",
      "Epoch 73/500\n",
      "475/475 [==============================] - 0s 120us/step - loss: 5551.5642 - smape_kun2: 36.3955\n",
      "Epoch 74/500\n",
      "475/475 [==============================] - 0s 139us/step - loss: 5526.5130 - smape_kun2: 36.1734\n",
      "Epoch 75/500\n",
      "475/475 [==============================] - 0s 115us/step - loss: 5529.7883 - smape_kun2: 36.1987\n",
      "Epoch 76/500\n",
      "475/475 [==============================] - 0s 155us/step - loss: 5522.6981 - smape_kun2: 36.3925\n",
      "Epoch 77/500\n",
      "475/475 [==============================] - 0s 309us/step - loss: 5515.1383 - smape_kun2: 36.1853\n",
      "Epoch 78/500\n",
      "475/475 [==============================] - 0s 254us/step - loss: 5498.4396 - smape_kun2: 36.0854\n",
      "Epoch 79/500\n",
      "475/475 [==============================] - 0s 286us/step - loss: 5491.2148 - smape_kun2: 36.2415\n",
      "Epoch 80/500\n",
      "475/475 [==============================] - 0s 283us/step - loss: 5481.9041 - smape_kun2: 36.2192\n",
      "Epoch 81/500\n",
      "475/475 [==============================] - 0s 340us/step - loss: 5482.8853 - smape_kun2: 36.1929\n",
      "Epoch 82/500\n",
      "475/475 [==============================] - 0s 260us/step - loss: 5486.4230 - smape_kun2: 36.1666\n",
      "Epoch 83/500\n",
      "475/475 [==============================] - 0s 269us/step - loss: 5526.9952 - smape_kun2: 36.3833\n",
      "Epoch 84/500\n",
      "475/475 [==============================] - ETA: 0s - loss: 5338.3874 - smape_kun2: 34.00 - 0s 239us/step - loss: 5485.0657 - smape_kun2: 36.2241\n",
      "Epoch 85/500\n",
      "475/475 [==============================] - 0s 220us/step - loss: 5470.5504 - smape_kun2: 36.1097\n",
      "Epoch 86/500\n",
      "475/475 [==============================] - 0s 248us/step - loss: 5461.6687 - smape_kun2: 36.1526\n",
      "Epoch 87/500\n",
      "475/475 [==============================] - 0s 338us/step - loss: 5473.3285 - smape_kun2: 36.2625\n",
      "Epoch 88/500\n",
      "475/475 [==============================] - 0s 231us/step - loss: 5484.1142 - smape_kun2: 36.1772\n",
      "Epoch 89/500\n",
      "475/475 [==============================] - 0s 231us/step - loss: 5471.3855 - smape_kun2: 36.1509\n",
      "Epoch 90/500\n",
      "475/475 [==============================] - 0s 265us/step - loss: 5452.3095 - smape_kun2: 36.1656\n",
      "Epoch 91/500\n",
      "475/475 [==============================] - 0s 250us/step - loss: 5446.5490 - smape_kun2: 36.2671\n",
      "Epoch 92/500\n",
      "475/475 [==============================] - 0s 233us/step - loss: 5466.6643 - smape_kun2: 36.2521\n",
      "Epoch 93/500\n",
      "475/475 [==============================] - 0s 286us/step - loss: 5473.5558 - smape_kun2: 36.1470\n",
      "Epoch 94/500\n",
      "475/475 [==============================] - 0s 315us/step - loss: 5443.3883 - smape_kun2: 36.1173\n",
      "Epoch 95/500\n",
      "475/475 [==============================] - 0s 281us/step - loss: 5459.8895 - smape_kun2: 36.2607\n",
      "Epoch 96/500\n",
      "475/475 [==============================] - 0s 256us/step - loss: 5450.8816 - smape_kun2: 36.1086\n",
      "Epoch 97/500\n",
      "475/475 [==============================] - 0s 252us/step - loss: 5471.9749 - smape_kun2: 36.2553\n",
      "Epoch 98/500\n",
      "475/475 [==============================] - 0s 256us/step - loss: 5446.8809 - smape_kun2: 36.0843\n",
      "Epoch 99/500\n",
      "475/475 [==============================] - 0s 241us/step - loss: 5464.8769 - smape_kun2: 36.0322\n",
      "Epoch 100/500\n",
      "475/475 [==============================] - 0s 212us/step - loss: 5430.2481 - smape_kun2: 36.1849\n",
      "Epoch 101/500\n",
      "475/475 [==============================] - 0s 227us/step - loss: 5440.7195 - smape_kun2: 36.0617\n",
      "Epoch 102/500\n",
      "475/475 [==============================] - 0s 136us/step - loss: 5422.8058 - smape_kun2: 36.0641\n",
      "Epoch 103/500\n",
      "475/475 [==============================] - 0s 130us/step - loss: 5439.6357 - smape_kun2: 36.0684\n",
      "Epoch 104/500\n",
      "475/475 [==============================] - 0s 147us/step - loss: 5422.3590 - smape_kun2: 36.1180\n",
      "Epoch 105/500\n",
      "475/475 [==============================] - 0s 160us/step - loss: 5452.6437 - smape_kun2: 36.2492\n",
      "Epoch 106/500\n",
      "475/475 [==============================] - 0s 109us/step - loss: 5457.0443 - smape_kun2: 36.0063\n",
      "Epoch 107/500\n",
      "475/475 [==============================] - 0s 99us/step - loss: 5450.2792 - smape_kun2: 36.1287\n",
      "Epoch 108/500\n",
      "475/475 [==============================] - 0s 157us/step - loss: 5485.9680 - smape_kun2: 36.1678\n",
      "Epoch 109/500\n",
      "475/475 [==============================] - 0s 172us/step - loss: 5402.9354 - smape_kun2: 36.0846\n",
      "Epoch 110/500\n",
      "475/475 [==============================] - 0s 143us/step - loss: 5442.0564 - smape_kun2: 36.2827\n",
      "Epoch 111/500\n",
      "475/475 [==============================] - 0s 164us/step - loss: 5414.5434 - smape_kun2: 36.1261\n",
      "Epoch 112/500\n",
      "475/475 [==============================] - 0s 136us/step - loss: 5421.7209 - smape_kun2: 36.1136\n",
      "Epoch 113/500\n",
      "475/475 [==============================] - 0s 111us/step - loss: 5442.3422 - smape_kun2: 35.9923\n",
      "Epoch 114/500\n",
      "475/475 [==============================] - 0s 162us/step - loss: 5420.2380 - smape_kun2: 36.0947\n",
      "Epoch 115/500\n",
      "475/475 [==============================] - 0s 181us/step - loss: 5439.7292 - smape_kun2: 36.2820\n",
      "Epoch 116/500\n",
      "475/475 [==============================] - 0s 252us/step - loss: 5475.8672 - smape_kun2: 35.9628\n",
      "Epoch 117/500\n",
      "475/475 [==============================] - 0s 275us/step - loss: 5416.1807 - smape_kun2: 35.9699\n",
      "Epoch 118/500\n",
      "475/475 [==============================] - 0s 273us/step - loss: 5456.6295 - smape_kun2: 36.2615\n",
      "Epoch 119/500\n",
      "475/475 [==============================] - 0s 229us/step - loss: 5472.5008 - smape_kun2: 35.8969\n",
      "Epoch 120/500\n",
      "475/475 [==============================] - 0s 277us/step - loss: 5422.1907 - smape_kun2: 36.1439\n",
      "Epoch 121/500\n",
      "475/475 [==============================] - 0s 265us/step - loss: 5416.2046 - smape_kun2: 36.1855\n",
      "Epoch 122/500\n",
      "475/475 [==============================] - 0s 237us/step - loss: 5485.7576 - smape_kun2: 36.1202\n",
      "Epoch 123/500\n",
      "475/475 [==============================] - 0s 218us/step - loss: 5409.6990 - smape_kun2: 36.0537\n",
      "Epoch 124/500\n",
      "475/475 [==============================] - ETA: 0s - loss: 5369.9431 - smape_kun2: 38.71 - 0s 260us/step - loss: 5406.3752 - smape_kun2: 36.0182\n",
      "Epoch 125/500\n",
      "475/475 [==============================] - 0s 246us/step - loss: 5414.3045 - smape_kun2: 36.0729\n",
      "Epoch 126/500\n",
      "475/475 [==============================] - 0s 248us/step - loss: 5411.1890 - smape_kun2: 36.1363\n",
      "Epoch 127/500\n",
      "475/475 [==============================] - 0s 239us/step - loss: 5399.2947 - smape_kun2: 36.1113\n",
      "Epoch 128/500\n",
      "475/475 [==============================] - 0s 304us/step - loss: 5403.8293 - smape_kun2: 36.05880s - loss: 5649.3151 - smape_kun2: 38.46\n",
      "Epoch 129/500\n",
      "475/475 [==============================] - 0s 208us/step - loss: 5420.1280 - smape_kun2: 36.1823\n",
      "Epoch 130/500\n",
      "475/475 [==============================] - 0s 250us/step - loss: 5406.2741 - smape_kun2: 36.0275\n",
      "Epoch 131/500\n",
      "475/475 [==============================] - 0s 206us/step - loss: 5401.6938 - smape_kun2: 36.0951\n",
      "Epoch 132/500\n",
      "475/475 [==============================] - 0s 174us/step - loss: 5399.8906 - smape_kun2: 36.2059\n",
      "Epoch 133/500\n",
      "475/475 [==============================] - 0s 168us/step - loss: 5398.2358 - smape_kun2: 36.0738\n",
      "Epoch 134/500\n",
      "475/475 [==============================] - 0s 136us/step - loss: 5393.8916 - smape_kun2: 35.9970\n",
      "Epoch 135/500\n",
      "475/475 [==============================] - 0s 128us/step - loss: 5425.2609 - smape_kun2: 36.1030\n",
      "Epoch 136/500\n",
      "475/475 [==============================] - 0s 92us/step - loss: 5419.4033 - smape_kun2: 36.0234\n",
      "Epoch 137/500\n",
      "475/475 [==============================] - 0s 101us/step - loss: 5426.9476 - smape_kun2: 36.0873\n",
      "Epoch 138/500\n",
      "475/475 [==============================] - 0s 111us/step - loss: 5476.6097 - smape_kun2: 36.0340\n",
      "Epoch 139/500\n",
      "475/475 [==============================] - 0s 124us/step - loss: 5463.7984 - smape_kun2: 36.2512\n",
      "Epoch 140/500\n",
      "475/475 [==============================] - 0s 115us/step - loss: 5399.9092 - smape_kun2: 36.1610\n",
      "Epoch 141/500\n",
      "475/475 [==============================] - 0s 111us/step - loss: 5444.1600 - smape_kun2: 36.1732\n",
      "Epoch 142/500\n",
      "475/475 [==============================] - 0s 122us/step - loss: 5393.5988 - smape_kun2: 36.1687\n",
      "Epoch 143/500\n",
      "475/475 [==============================] - 0s 199us/step - loss: 5395.4602 - smape_kun2: 36.0158\n",
      "Epoch 144/500\n",
      "475/475 [==============================] - 0s 246us/step - loss: 5452.3493 - smape_kun2: 36.2622\n",
      "Epoch 145/500\n",
      "475/475 [==============================] - 0s 241us/step - loss: 5380.7507 - smape_kun2: 36.0502\n",
      "Epoch 146/500\n",
      "475/475 [==============================] - 0s 241us/step - loss: 5412.5325 - smape_kun2: 36.1387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147/500\n",
      "475/475 [==============================] - 0s 248us/step - loss: 5423.2824 - smape_kun2: 36.2355\n",
      "Epoch 148/500\n",
      "475/475 [==============================] - 0s 237us/step - loss: 5378.7614 - smape_kun2: 36.1072\n",
      "Epoch 149/500\n",
      "475/475 [==============================] - 0s 275us/step - loss: 5389.2777 - smape_kun2: 36.1935\n",
      "Epoch 150/500\n",
      "475/475 [==============================] - 0s 271us/step - loss: 5407.2879 - smape_kun2: 36.1693\n",
      "Epoch 151/500\n",
      "475/475 [==============================] - 0s 298us/step - loss: 5396.1624 - smape_kun2: 36.1248\n",
      "Epoch 152/500\n",
      "475/475 [==============================] - 0s 262us/step - loss: 5403.3298 - smape_kun2: 36.1781\n",
      "Epoch 153/500\n",
      "475/475 [==============================] - 0s 262us/step - loss: 5401.6929 - smape_kun2: 36.2457\n",
      "Epoch 154/500\n",
      "475/475 [==============================] - 0s 229us/step - loss: 5398.9051 - smape_kun2: 36.2449\n",
      "Epoch 155/500\n",
      "475/475 [==============================] - 0s 332us/step - loss: 5424.0853 - smape_kun2: 36.2432\n",
      "Epoch 156/500\n",
      "475/475 [==============================] - 0s 271us/step - loss: 5384.4259 - smape_kun2: 36.2668\n",
      "Epoch 157/500\n",
      "475/475 [==============================] - 0s 237us/step - loss: 5409.3160 - smape_kun2: 36.2507\n",
      "Epoch 158/500\n",
      "475/475 [==============================] - 0s 218us/step - loss: 5402.8133 - smape_kun2: 36.1501\n",
      "Epoch 159/500\n",
      "475/475 [==============================] - 0s 214us/step - loss: 5374.6339 - smape_kun2: 36.0761\n",
      "Epoch 160/500\n",
      "475/475 [==============================] - 0s 183us/step - loss: 5377.0792 - smape_kun2: 35.9837\n",
      "Epoch 161/500\n",
      "475/475 [==============================] - 0s 174us/step - loss: 5405.4631 - smape_kun2: 36.2508\n",
      "Epoch 162/500\n",
      "475/475 [==============================] - 0s 139us/step - loss: 5411.0355 - smape_kun2: 36.0629\n",
      "Epoch 163/500\n",
      "475/475 [==============================] - 0s 113us/step - loss: 5382.0215 - smape_kun2: 36.0722\n",
      "Epoch 164/500\n",
      "475/475 [==============================] - 0s 120us/step - loss: 5378.0375 - smape_kun2: 36.1465\n",
      "Epoch 165/500\n",
      "475/475 [==============================] - 0s 122us/step - loss: 5388.1961 - smape_kun2: 36.1025\n",
      "Epoch 166/500\n",
      "475/475 [==============================] - 0s 145us/step - loss: 5380.0956 - smape_kun2: 36.1832\n",
      "Epoch 167/500\n",
      "475/475 [==============================] - 0s 151us/step - loss: 5388.8312 - smape_kun2: 36.3140\n",
      "Epoch 168/500\n",
      "475/475 [==============================] - 0s 225us/step - loss: 5381.1827 - smape_kun2: 36.1223\n",
      "Epoch 169/500\n",
      "475/475 [==============================] - 0s 141us/step - loss: 5370.7098 - smape_kun2: 36.0347\n",
      "Epoch 170/500\n",
      "475/475 [==============================] - 0s 147us/step - loss: 5372.2840 - smape_kun2: 36.1293\n",
      "Epoch 171/500\n",
      "475/475 [==============================] - 0s 160us/step - loss: 5375.2160 - smape_kun2: 36.0287\n",
      "Epoch 172/500\n",
      "475/475 [==============================] - 0s 183us/step - loss: 5558.0194 - smape_kun2: 36.6088\n",
      "Epoch 173/500\n",
      "475/475 [==============================] - 0s 139us/step - loss: 5395.0998 - smape_kun2: 36.0343\n",
      "Epoch 174/500\n",
      "475/475 [==============================] - 0s 187us/step - loss: 5385.1181 - smape_kun2: 35.9460\n",
      "Epoch 175/500\n",
      "475/475 [==============================] - 0s 170us/step - loss: 5402.8528 - smape_kun2: 36.2416\n",
      "Epoch 176/500\n",
      "475/475 [==============================] - 0s 170us/step - loss: 5375.9964 - smape_kun2: 36.2471\n",
      "Epoch 177/500\n",
      "475/475 [==============================] - 0s 185us/step - loss: 5379.2237 - smape_kun2: 36.0872\n",
      "Epoch 178/500\n",
      "475/475 [==============================] - 0s 197us/step - loss: 5390.2147 - smape_kun2: 36.1509\n",
      "Epoch 179/500\n",
      "475/475 [==============================] - 0s 208us/step - loss: 5391.7347 - smape_kun2: 36.0460\n",
      "Epoch 180/500\n",
      "475/475 [==============================] - 0s 172us/step - loss: 5382.2325 - smape_kun2: 35.9749\n",
      "Epoch 181/500\n",
      "475/475 [==============================] - 0s 153us/step - loss: 5380.0306 - smape_kun2: 36.0702\n",
      "Epoch 182/500\n",
      "475/475 [==============================] - 0s 181us/step - loss: 5443.6367 - smape_kun2: 36.0536\n",
      "Epoch 183/500\n",
      "475/475 [==============================] - 0s 174us/step - loss: 5390.7559 - smape_kun2: 36.1351\n",
      "Epoch 184/500\n",
      "475/475 [==============================] - 0s 164us/step - loss: 5355.5309 - smape_kun2: 36.0506\n",
      "Epoch 185/500\n",
      "475/475 [==============================] - 0s 160us/step - loss: 5385.2509 - smape_kun2: 35.9765\n",
      "Epoch 186/500\n",
      "475/475 [==============================] - 0s 162us/step - loss: 5379.4861 - smape_kun2: 36.1696\n",
      "Epoch 187/500\n",
      "475/475 [==============================] - 0s 174us/step - loss: 5420.6469 - smape_kun2: 36.2994\n",
      "Epoch 188/500\n",
      "475/475 [==============================] - 0s 191us/step - loss: 5433.9993 - smape_kun2: 36.0342\n",
      "Epoch 189/500\n",
      "475/475 [==============================] - 0s 208us/step - loss: 5355.5376 - smape_kun2: 36.0921\n",
      "Epoch 190/500\n",
      "475/475 [==============================] - 0s 313us/step - loss: 5406.2041 - smape_kun2: 36.3185\n",
      "Epoch 191/500\n",
      "475/475 [==============================] - 0s 296us/step - loss: 5347.9061 - smape_kun2: 36.1212\n",
      "Epoch 192/500\n",
      "475/475 [==============================] - 0s 273us/step - loss: 5431.4661 - smape_kun2: 36.1648\n",
      "Epoch 193/500\n",
      "475/475 [==============================] - 0s 229us/step - loss: 5379.5379 - smape_kun2: 36.1003\n",
      "Epoch 194/500\n",
      "475/475 [==============================] - 0s 265us/step - loss: 5366.8590 - smape_kun2: 36.1104\n",
      "Epoch 195/500\n",
      "475/475 [==============================] - 0s 248us/step - loss: 5410.3661 - smape_kun2: 36.3280\n",
      "Epoch 196/500\n",
      "475/475 [==============================] - 0s 191us/step - loss: 5367.2357 - smape_kun2: 36.0987\n",
      "Epoch 197/500\n",
      "475/475 [==============================] - 0s 197us/step - loss: 5372.3752 - smape_kun2: 35.9870\n",
      "Epoch 198/500\n",
      "475/475 [==============================] - 0s 191us/step - loss: 5401.9170 - smape_kun2: 36.3574\n",
      "Epoch 199/500\n",
      "475/475 [==============================] - 0s 195us/step - loss: 5380.6964 - smape_kun2: 36.1662\n",
      "Epoch 200/500\n",
      "475/475 [==============================] - 0s 216us/step - loss: 5366.7570 - smape_kun2: 36.1520\n",
      "Epoch 201/500\n",
      "475/475 [==============================] - 0s 210us/step - loss: 5373.2199 - smape_kun2: 36.0952\n",
      "Epoch 202/500\n",
      "475/475 [==============================] - 0s 204us/step - loss: 5383.4865 - smape_kun2: 36.0658\n",
      "Epoch 203/500\n",
      "475/475 [==============================] - 0s 239us/step - loss: 5367.1494 - smape_kun2: 36.1750\n",
      "Epoch 204/500\n",
      "475/475 [==============================] - 0s 250us/step - loss: 5366.9419 - smape_kun2: 36.1050\n",
      "Epoch 205/500\n",
      "475/475 [==============================] - 0s 227us/step - loss: 5385.7074 - smape_kun2: 36.0430\n",
      "Epoch 206/500\n",
      "475/475 [==============================] - 0s 172us/step - loss: 5376.0937 - smape_kun2: 36.2361\n",
      "Epoch 207/500\n",
      "475/475 [==============================] - 0s 195us/step - loss: 5367.5856 - smape_kun2: 36.2159\n",
      "Epoch 208/500\n",
      "475/475 [==============================] - 0s 208us/step - loss: 5382.9388 - smape_kun2: 36.0553\n",
      "Epoch 209/500\n",
      "475/475 [==============================] - 0s 223us/step - loss: 5397.1758 - smape_kun2: 36.1938\n",
      "Epoch 210/500\n",
      "475/475 [==============================] - 0s 225us/step - loss: 5359.9141 - smape_kun2: 36.2781\n",
      "Epoch 211/500\n",
      "475/475 [==============================] - 0s 220us/step - loss: 5398.2516 - smape_kun2: 35.96180s - loss: 5341.0250 - smape_kun2: 37.17\n",
      "Epoch 212/500\n",
      "475/475 [==============================] - 0s 162us/step - loss: 5356.4790 - smape_kun2: 36.0947\n",
      "Epoch 213/500\n",
      "475/475 [==============================] - 0s 185us/step - loss: 5362.8668 - smape_kun2: 36.1269\n",
      "Epoch 214/500\n",
      "475/475 [==============================] - 0s 227us/step - loss: 5363.9583 - smape_kun2: 36.0453\n",
      "Epoch 215/500\n",
      "475/475 [==============================] - 0s 231us/step - loss: 5397.3508 - smape_kun2: 36.1543\n",
      "Epoch 216/500\n",
      "475/475 [==============================] - 0s 214us/step - loss: 5364.3753 - smape_kun2: 36.1167\n",
      "Epoch 217/500\n",
      "475/475 [==============================] - 0s 178us/step - loss: 5348.0206 - smape_kun2: 36.0897\n",
      "Epoch 218/500\n",
      "475/475 [==============================] - ETA: 0s - loss: 5269.1979 - smape_kun2: 34.78 - 0s 244us/step - loss: 5369.3265 - smape_kun2: 36.2122\n",
      "Epoch 219/500\n",
      "475/475 [==============================] - 0s 212us/step - loss: 5357.2212 - smape_kun2: 36.2320\n",
      "Epoch 220/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "475/475 [==============================] - 0s 189us/step - loss: 5390.3476 - smape_kun2: 36.1752\n",
      "Epoch 221/500\n",
      "475/475 [==============================] - 0s 178us/step - loss: 5351.7149 - smape_kun2: 36.1595\n",
      "Epoch 222/500\n",
      "475/475 [==============================] - 0s 189us/step - loss: 5371.4031 - smape_kun2: 36.1656\n",
      "Epoch 223/500\n",
      "475/475 [==============================] - 0s 151us/step - loss: 5361.4658 - smape_kun2: 36.0200\n",
      "Epoch 224/500\n",
      "475/475 [==============================] - 0s 153us/step - loss: 5378.4879 - smape_kun2: 36.1380\n",
      "Epoch 225/500\n",
      "475/475 [==============================] - 0s 155us/step - loss: 5360.8855 - smape_kun2: 36.0408\n",
      "Epoch 226/500\n",
      "475/475 [==============================] - 0s 191us/step - loss: 5364.2449 - smape_kun2: 36.0658\n",
      "Epoch 227/500\n",
      "475/475 [==============================] - 0s 197us/step - loss: 5356.5331 - smape_kun2: 36.1357\n",
      "Epoch 228/500\n",
      "475/475 [==============================] - 0s 160us/step - loss: 5391.5457 - smape_kun2: 36.1753\n",
      "Epoch 229/500\n",
      "475/475 [==============================] - 0s 170us/step - loss: 5374.0220 - smape_kun2: 36.2789\n",
      "Epoch 230/500\n",
      "475/475 [==============================] - 0s 185us/step - loss: 5366.7583 - smape_kun2: 36.1110\n",
      "Epoch 231/500\n",
      "475/475 [==============================] - 0s 216us/step - loss: 5366.4903 - smape_kun2: 36.0356\n",
      "Epoch 232/500\n",
      "475/475 [==============================] - 0s 214us/step - loss: 5396.1430 - smape_kun2: 36.3078\n",
      "Epoch 233/500\n",
      "475/475 [==============================] - 0s 208us/step - loss: 5388.2298 - smape_kun2: 36.0705\n",
      "Epoch 234/500\n",
      "475/475 [==============================] - 0s 250us/step - loss: 5414.3518 - smape_kun2: 36.4114\n",
      "Epoch 235/500\n",
      "475/475 [==============================] - 0s 279us/step - loss: 5407.3819 - smape_kun2: 36.1774\n",
      "Epoch 236/500\n",
      "475/475 [==============================] - 0s 294us/step - loss: 5439.7416 - smape_kun2: 36.3092\n",
      "Epoch 237/500\n",
      "475/475 [==============================] - 0s 258us/step - loss: 5355.3608 - smape_kun2: 36.1264\n",
      "Epoch 238/500\n",
      "475/475 [==============================] - 0s 279us/step - loss: 5370.9827 - smape_kun2: 36.0631\n",
      "Epoch 239/500\n",
      "475/475 [==============================] - 0s 229us/step - loss: 5355.2601 - smape_kun2: 36.1444\n",
      "Epoch 240/500\n",
      "475/475 [==============================] - 0s 300us/step - loss: 5345.0902 - smape_kun2: 36.2472\n",
      "Epoch 241/500\n",
      "475/475 [==============================] - 0s 252us/step - loss: 5361.2270 - smape_kun2: 36.2342\n",
      "Epoch 242/500\n",
      "475/475 [==============================] - 0s 286us/step - loss: 5375.3150 - smape_kun2: 36.1480\n",
      "Epoch 243/500\n",
      "475/475 [==============================] - 0s 210us/step - loss: 5353.2998 - smape_kun2: 36.1959\n",
      "Epoch 244/500\n",
      "475/475 [==============================] - 0s 244us/step - loss: 5370.8371 - smape_kun2: 36.1551\n",
      "Epoch 245/500\n",
      "475/475 [==============================] - 0s 252us/step - loss: 5371.8021 - smape_kun2: 36.0414\n",
      "Epoch 246/500\n",
      "475/475 [==============================] - 0s 225us/step - loss: 5356.5634 - smape_kun2: 36.2326\n",
      "Epoch 247/500\n",
      "475/475 [==============================] - 0s 235us/step - loss: 5368.4038 - smape_kun2: 36.2979\n",
      "Epoch 248/500\n",
      "475/475 [==============================] - 0s 225us/step - loss: 5369.2088 - smape_kun2: 36.0959\n",
      "Epoch 249/500\n",
      "475/475 [==============================] - 0s 214us/step - loss: 5358.7265 - smape_kun2: 36.1596\n",
      "Epoch 250/500\n",
      "475/475 [==============================] - 0s 363us/step - loss: 5351.0835 - smape_kun2: 36.0860\n",
      "Epoch 251/500\n",
      "475/475 [==============================] - 0s 250us/step - loss: 5362.3894 - smape_kun2: 36.0108\n",
      "Epoch 252/500\n",
      "475/475 [==============================] - 0s 246us/step - loss: 5373.5971 - smape_kun2: 36.1196\n",
      "Epoch 253/500\n",
      "475/475 [==============================] - 0s 246us/step - loss: 5352.3851 - smape_kun2: 36.1568\n",
      "Epoch 254/500\n",
      "475/475 [==============================] - 0s 239us/step - loss: 5339.9280 - smape_kun2: 36.1031\n",
      "Epoch 255/500\n",
      "475/475 [==============================] - 0s 214us/step - loss: 5365.5310 - smape_kun2: 36.3273\n",
      "Epoch 256/500\n",
      "475/475 [==============================] - 0s 210us/step - loss: 5381.3629 - smape_kun2: 36.0732\n",
      "Epoch 257/500\n",
      "475/475 [==============================] - 0s 197us/step - loss: 5352.1312 - smape_kun2: 36.0194\n",
      "Epoch 258/500\n",
      "475/475 [==============================] - 0s 208us/step - loss: 5375.3478 - smape_kun2: 36.1987\n",
      "Epoch 259/500\n",
      "475/475 [==============================] - 0s 197us/step - loss: 5369.8775 - smape_kun2: 36.2760\n",
      "Epoch 260/500\n",
      "475/475 [==============================] - 0s 185us/step - loss: 5352.7538 - smape_kun2: 36.1579\n",
      "Epoch 261/500\n",
      "475/475 [==============================] - 0s 250us/step - loss: 5355.7827 - smape_kun2: 36.0493\n",
      "Epoch 262/500\n",
      "475/475 [==============================] - 0s 218us/step - loss: 5347.2410 - smape_kun2: 36.1074\n",
      "Epoch 263/500\n",
      "475/475 [==============================] - 0s 187us/step - loss: 5422.7344 - smape_kun2: 36.3727\n",
      "Epoch 264/500\n",
      "475/475 [==============================] - 0s 155us/step - loss: 5329.8247 - smape_kun2: 35.9566\n",
      "Epoch 265/500\n",
      "475/475 [==============================] - 0s 170us/step - loss: 5356.6516 - smape_kun2: 36.0918\n",
      "Epoch 266/500\n",
      "475/475 [==============================] - 0s 176us/step - loss: 5369.6648 - smape_kun2: 36.3834\n",
      "Epoch 267/500\n",
      "475/475 [==============================] - 0s 191us/step - loss: 5369.5026 - smape_kun2: 36.1017\n",
      "Epoch 268/500\n",
      "475/475 [==============================] - 0s 204us/step - loss: 5367.6767 - smape_kun2: 36.1614\n",
      "Epoch 269/500\n",
      "475/475 [==============================] - 0s 185us/step - loss: 5335.7928 - smape_kun2: 36.1588\n",
      "Epoch 270/500\n",
      "475/475 [==============================] - 0s 185us/step - loss: 5395.3963 - smape_kun2: 35.9917\n",
      "Epoch 271/500\n",
      "475/475 [==============================] - 0s 170us/step - loss: 5442.9028 - smape_kun2: 36.4426\n",
      "Epoch 272/500\n",
      "475/475 [==============================] - 0s 193us/step - loss: 5352.8861 - smape_kun2: 36.0987\n",
      "Epoch 273/500\n",
      "475/475 [==============================] - 0s 170us/step - loss: 5379.6158 - smape_kun2: 36.1228\n",
      "Epoch 274/500\n",
      "475/475 [==============================] - 0s 191us/step - loss: 5352.7183 - smape_kun2: 36.4335\n",
      "Epoch 275/500\n",
      "475/475 [==============================] - 0s 181us/step - loss: 5349.5463 - smape_kun2: 36.1629\n",
      "Epoch 276/500\n",
      "475/475 [==============================] - 0s 176us/step - loss: 5355.3512 - smape_kun2: 36.0964\n",
      "Epoch 277/500\n",
      "475/475 [==============================] - 0s 202us/step - loss: 5350.4094 - smape_kun2: 36.0793\n",
      "Epoch 278/500\n",
      "475/475 [==============================] - 0s 246us/step - loss: 5337.2049 - smape_kun2: 36.02290s - loss: 4675.1841 - smape_kun2: 34.76\n",
      "Epoch 279/500\n",
      "475/475 [==============================] - 0s 212us/step - loss: 5353.4760 - smape_kun2: 36.1389\n",
      "Epoch 280/500\n",
      "475/475 [==============================] - 0s 174us/step - loss: 5355.0355 - smape_kun2: 36.2460\n",
      "Epoch 281/500\n",
      "475/475 [==============================] - 0s 193us/step - loss: 5386.2639 - smape_kun2: 36.0751\n",
      "Epoch 282/500\n",
      "475/475 [==============================] - 0s 178us/step - loss: 5356.1293 - smape_kun2: 36.2234\n",
      "Epoch 283/500\n",
      "475/475 [==============================] - 0s 214us/step - loss: 5413.2465 - smape_kun2: 36.2072\n",
      "Epoch 284/500\n",
      "475/475 [==============================] - 0s 212us/step - loss: 5339.2874 - smape_kun2: 36.16210s - loss: 5324.3213 - smape_kun2: 35.37\n",
      "Epoch 285/500\n",
      "475/475 [==============================] - 0s 223us/step - loss: 5334.2910 - smape_kun2: 36.1346\n",
      "Epoch 286/500\n",
      "475/475 [==============================] - 0s 143us/step - loss: 5358.0916 - smape_kun2: 36.0688\n",
      "Epoch 287/500\n",
      "475/475 [==============================] - 0s 168us/step - loss: 5345.1732 - smape_kun2: 36.1283\n",
      "Epoch 288/500\n",
      "475/475 [==============================] - 0s 145us/step - loss: 5367.0853 - smape_kun2: 36.1394\n",
      "Epoch 289/500\n",
      "475/475 [==============================] - 0s 164us/step - loss: 5361.3809 - smape_kun2: 36.2546\n",
      "Epoch 290/500\n",
      "475/475 [==============================] - 0s 204us/step - loss: 5362.6903 - smape_kun2: 36.0743\n",
      "Epoch 291/500\n",
      "475/475 [==============================] - 0s 328us/step - loss: 5395.4203 - smape_kun2: 36.3686\n",
      "Epoch 292/500\n",
      "475/475 [==============================] - 0s 317us/step - loss: 5376.9184 - smape_kun2: 36.0725\n",
      "Epoch 293/500\n",
      "475/475 [==============================] - 0s 325us/step - loss: 5368.6428 - smape_kun2: 36.2758\n",
      "Epoch 294/500\n",
      "475/475 [==============================] - 0s 357us/step - loss: 5383.8517 - smape_kun2: 36.1759\n",
      "Epoch 295/500\n",
      "475/475 [==============================] - 0s 130us/step - loss: 5419.1496 - smape_kun2: 36.3699\n",
      "Epoch 296/500\n",
      "475/475 [==============================] - 0s 136us/step - loss: 5365.9669 - smape_kun2: 36.0991\n",
      "Epoch 297/500\n",
      "475/475 [==============================] - 0s 237us/step - loss: 5365.1332 - smape_kun2: 36.1222\n",
      "Epoch 298/500\n",
      "475/475 [==============================] - 0s 128us/step - loss: 5347.8952 - smape_kun2: 36.3030\n",
      "Epoch 299/500\n",
      "475/475 [==============================] - 0s 141us/step - loss: 5358.0912 - smape_kun2: 36.1960\n",
      "Epoch 300/500\n",
      "475/475 [==============================] - 0s 193us/step - loss: 5359.8058 - smape_kun2: 36.2489\n",
      "Epoch 301/500\n",
      "475/475 [==============================] - 0s 216us/step - loss: 5347.1688 - smape_kun2: 36.2127\n",
      "Epoch 302/500\n",
      "475/475 [==============================] - 0s 206us/step - loss: 5357.8475 - smape_kun2: 36.2607\n",
      "Epoch 303/500\n",
      "475/475 [==============================] - 0s 162us/step - loss: 5346.6252 - smape_kun2: 36.2805\n",
      "Epoch 304/500\n",
      "475/475 [==============================] - 0s 166us/step - loss: 5345.5650 - smape_kun2: 36.1653\n",
      "Epoch 305/500\n",
      "475/475 [==============================] - 0s 208us/step - loss: 5334.2133 - smape_kun2: 36.1073\n",
      "Epoch 306/500\n",
      "475/475 [==============================] - 0s 118us/step - loss: 5349.8029 - smape_kun2: 36.1857\n",
      "Epoch 307/500\n",
      "475/475 [==============================] - 0s 132us/step - loss: 5340.2890 - smape_kun2: 36.2300\n",
      "Epoch 308/500\n",
      "475/475 [==============================] - 0s 225us/step - loss: 5356.7618 - smape_kun2: 36.0352\n",
      "Epoch 309/500\n",
      "475/475 [==============================] - 0s 124us/step - loss: 5374.0328 - smape_kun2: 36.1207\n",
      "Epoch 310/500\n",
      "475/475 [==============================] - 0s 208us/step - loss: 5339.8354 - smape_kun2: 36.2331\n",
      "Epoch 311/500\n",
      "475/475 [==============================] - 0s 187us/step - loss: 5357.4831 - smape_kun2: 36.1141\n",
      "Epoch 312/500\n",
      "475/475 [==============================] - 0s 170us/step - loss: 5352.5473 - smape_kun2: 36.2068\n",
      "Epoch 313/500\n",
      "475/475 [==============================] - 0s 105us/step - loss: 5341.1178 - smape_kun2: 36.1869\n",
      "Epoch 314/500\n",
      "475/475 [==============================] - 0s 88us/step - loss: 5365.3791 - smape_kun2: 36.0314\n",
      "Epoch 315/500\n",
      "475/475 [==============================] - 0s 88us/step - loss: 5348.7656 - smape_kun2: 36.0958\n",
      "Epoch 316/500\n",
      "475/475 [==============================] - 0s 84us/step - loss: 5381.7994 - smape_kun2: 36.1705\n",
      "Epoch 317/500\n",
      "475/475 [==============================] - 0s 84us/step - loss: 5376.8707 - smape_kun2: 36.2403\n",
      "Epoch 318/500\n",
      "475/475 [==============================] - 0s 90us/step - loss: 5352.8930 - smape_kun2: 36.2776\n",
      "Epoch 319/500\n",
      "475/475 [==============================] - 0s 76us/step - loss: 5364.0293 - smape_kun2: 36.0514\n",
      "Epoch 320/500\n",
      "475/475 [==============================] - 0s 96us/step - loss: 5345.1032 - smape_kun2: 36.1531\n",
      "Epoch 321/500\n",
      "475/475 [==============================] - 0s 98us/step - loss: 5363.8144 - smape_kun2: 36.2332\n",
      "Epoch 322/500\n",
      "475/475 [==============================] - 0s 103us/step - loss: 5406.4462 - smape_kun2: 36.0770\n",
      "Epoch 323/500\n",
      "475/475 [==============================] - 0s 88us/step - loss: 5348.2695 - smape_kun2: 36.3014\n",
      "Epoch 324/500\n",
      "475/475 [==============================] - 0s 82us/step - loss: 5341.5613 - smape_kun2: 36.2628\n",
      "Epoch 325/500\n",
      "475/475 [==============================] - 0s 78us/step - loss: 5397.6367 - smape_kun2: 36.0919\n",
      "Epoch 326/500\n",
      "475/475 [==============================] - 0s 90us/step - loss: 5378.8696 - smape_kun2: 36.4475\n",
      "Epoch 327/500\n",
      "475/475 [==============================] - 0s 80us/step - loss: 5332.0965 - smape_kun2: 36.1088\n",
      "Epoch 328/500\n",
      "475/475 [==============================] - 0s 80us/step - loss: 5353.2300 - smape_kun2: 35.9505\n",
      "Epoch 329/500\n",
      "475/475 [==============================] - 0s 82us/step - loss: 5339.3941 - smape_kun2: 36.1102\n",
      "Epoch 330/500\n",
      "475/475 [==============================] - 0s 94us/step - loss: 5330.9808 - smape_kun2: 36.1571\n",
      "Epoch 331/500\n",
      "475/475 [==============================] - 0s 99us/step - loss: 5337.2475 - smape_kun2: 36.0993\n",
      "Epoch 332/500\n",
      "475/475 [==============================] - 0s 94us/step - loss: 5325.2554 - smape_kun2: 36.1846\n",
      "Epoch 333/500\n",
      "475/475 [==============================] - 0s 78us/step - loss: 5362.8544 - smape_kun2: 36.2099\n",
      "Epoch 334/500\n",
      "475/475 [==============================] - 0s 80us/step - loss: 5375.6010 - smape_kun2: 36.0229\n",
      "Epoch 335/500\n",
      "475/475 [==============================] - 0s 78us/step - loss: 5382.6527 - smape_kun2: 36.1319\n",
      "Epoch 336/500\n",
      "475/475 [==============================] - 0s 80us/step - loss: 5376.6305 - smape_kun2: 36.0554\n",
      "Epoch 337/500\n",
      "475/475 [==============================] - 0s 82us/step - loss: 5384.6842 - smape_kun2: 36.3109\n",
      "Epoch 338/500\n",
      "475/475 [==============================] - 0s 84us/step - loss: 5325.8993 - smape_kun2: 36.1699\n",
      "Epoch 339/500\n",
      "475/475 [==============================] - 0s 80us/step - loss: 5362.9522 - smape_kun2: 36.1358\n",
      "Epoch 340/500\n",
      "475/475 [==============================] - 0s 80us/step - loss: 5341.6021 - smape_kun2: 35.9859\n",
      "Epoch 341/500\n",
      "475/475 [==============================] - 0s 78us/step - loss: 5367.0020 - smape_kun2: 36.2355\n",
      "Epoch 342/500\n",
      "475/475 [==============================] - 0s 113us/step - loss: 5351.1309 - smape_kun2: 36.0952\n",
      "Epoch 343/500\n",
      "475/475 [==============================] - 0s 155us/step - loss: 5323.5107 - smape_kun2: 35.9948\n",
      "Epoch 344/500\n",
      "475/475 [==============================] - 0s 103us/step - loss: 5406.9151 - smape_kun2: 36.3360\n",
      "Epoch 345/500\n",
      "475/475 [==============================] - 0s 82us/step - loss: 5421.1523 - smape_kun2: 36.1533\n",
      "Epoch 346/500\n",
      "475/475 [==============================] - 0s 80us/step - loss: 5333.9286 - smape_kun2: 36.1155\n",
      "Epoch 347/500\n",
      "475/475 [==============================] - 0s 78us/step - loss: 5350.1315 - smape_kun2: 36.3552\n",
      "Epoch 348/500\n",
      "475/475 [==============================] - 0s 86us/step - loss: 5365.3720 - smape_kun2: 36.1656\n",
      "Epoch 349/500\n",
      "475/475 [==============================] - 0s 82us/step - loss: 5338.2266 - smape_kun2: 36.3388\n",
      "Epoch 350/500\n",
      "475/475 [==============================] - 0s 80us/step - loss: 5330.6427 - smape_kun2: 36.1437\n",
      "Epoch 351/500\n",
      "475/475 [==============================] - 0s 82us/step - loss: 5335.0182 - smape_kun2: 36.1238\n",
      "Epoch 352/500\n",
      "475/475 [==============================] - 0s 80us/step - loss: 5348.0612 - smape_kun2: 36.2228\n",
      "Epoch 353/500\n",
      "475/475 [==============================] - 0s 82us/step - loss: 5353.1957 - smape_kun2: 36.0798\n",
      "Epoch 354/500\n",
      "475/475 [==============================] - 0s 78us/step - loss: 5341.8939 - smape_kun2: 36.1094\n",
      "Epoch 355/500\n",
      "475/475 [==============================] - 0s 94us/step - loss: 5321.1019 - smape_kun2: 36.0200\n",
      "Epoch 356/500\n",
      "475/475 [==============================] - 0s 86us/step - loss: 5375.1903 - smape_kun2: 36.2300\n",
      "Epoch 357/500\n",
      "475/475 [==============================] - 0s 82us/step - loss: 5353.5750 - smape_kun2: 36.3048\n",
      "Epoch 358/500\n",
      "475/475 [==============================] - 0s 107us/step - loss: 5351.3673 - smape_kun2: 36.2622\n",
      "Epoch 359/500\n",
      "475/475 [==============================] - 0s 99us/step - loss: 5329.9407 - smape_kun2: 36.2126\n",
      "Epoch 360/500\n",
      "475/475 [==============================] - 0s 94us/step - loss: 5372.1790 - smape_kun2: 36.0920\n",
      "Epoch 361/500\n",
      "475/475 [==============================] - 0s 103us/step - loss: 5392.3521 - smape_kun2: 36.3878\n",
      "Epoch 362/500\n",
      "475/475 [==============================] - 0s 105us/step - loss: 5369.2085 - smape_kun2: 36.1861\n",
      "Epoch 363/500\n",
      "475/475 [==============================] - 0s 111us/step - loss: 5372.4942 - smape_kun2: 36.1130\n",
      "Epoch 364/500\n",
      "475/475 [==============================] - 0s 97us/step - loss: 5323.5406 - smape_kun2: 36.1366\n",
      "Epoch 365/500\n",
      "475/475 [==============================] - 0s 113us/step - loss: 5386.0285 - smape_kun2: 36.1174\n",
      "Epoch 366/500\n",
      "475/475 [==============================] - 0s 113us/step - loss: 5327.3163 - smape_kun2: 36.1589\n",
      "Epoch 367/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "475/475 [==============================] - 0s 84us/step - loss: 5340.0255 - smape_kun2: 36.0469\n",
      "Epoch 368/500\n",
      "475/475 [==============================] - 0s 82us/step - loss: 5363.2554 - smape_kun2: 36.1629\n",
      "Epoch 369/500\n",
      "475/475 [==============================] - 0s 78us/step - loss: 5342.0576 - smape_kun2: 36.0898\n",
      "Epoch 370/500\n",
      "475/475 [==============================] - 0s 88us/step - loss: 5328.7693 - smape_kun2: 36.1402\n",
      "Epoch 371/500\n",
      "475/475 [==============================] - 0s 84us/step - loss: 5359.2374 - smape_kun2: 36.4024\n",
      "Epoch 372/500\n",
      "475/475 [==============================] - 0s 92us/step - loss: 5343.9443 - smape_kun2: 36.2025\n",
      "Epoch 373/500\n",
      "475/475 [==============================] - 0s 94us/step - loss: 5358.7697 - smape_kun2: 36.0913\n",
      "Epoch 374/500\n",
      "475/475 [==============================] - 0s 105us/step - loss: 5355.0413 - smape_kun2: 36.0663\n",
      "Epoch 375/500\n",
      "475/475 [==============================] - 0s 101us/step - loss: 5359.1259 - smape_kun2: 36.0625\n",
      "Epoch 376/500\n",
      "475/475 [==============================] - 0s 338us/step - loss: 5387.4838 - smape_kun2: 36.4570\n",
      "Epoch 377/500\n",
      "475/475 [==============================] - 0s 208us/step - loss: 5314.9088 - smape_kun2: 36.0539\n",
      "Epoch 378/500\n",
      "475/475 [==============================] - 0s 262us/step - loss: 5358.6081 - smape_kun2: 36.0602\n",
      "Epoch 379/500\n",
      "475/475 [==============================] - 0s 187us/step - loss: 5335.1084 - smape_kun2: 36.3042\n",
      "Epoch 380/500\n",
      "475/475 [==============================] - 0s 168us/step - loss: 5351.5582 - smape_kun2: 36.1935\n",
      "Epoch 381/500\n",
      "475/475 [==============================] - 0s 139us/step - loss: 5329.9387 - smape_kun2: 36.2022\n",
      "Epoch 382/500\n",
      "475/475 [==============================] - 0s 149us/step - loss: 5351.4514 - smape_kun2: 36.1729\n",
      "Epoch 383/500\n",
      "475/475 [==============================] - 0s 176us/step - loss: 5365.9031 - smape_kun2: 36.1314\n",
      "Epoch 384/500\n",
      "475/475 [==============================] - 0s 235us/step - loss: 5395.8335 - smape_kun2: 36.3475\n",
      "Epoch 385/500\n",
      "475/475 [==============================] - 0s 300us/step - loss: 5352.1876 - smape_kun2: 36.2024\n",
      "Epoch 386/500\n",
      "475/475 [==============================] - 0s 168us/step - loss: 5365.7956 - smape_kun2: 36.2634\n",
      "Epoch 387/500\n",
      "475/475 [==============================] - 0s 181us/step - loss: 5338.7009 - smape_kun2: 36.4766\n",
      "Epoch 388/500\n",
      "475/475 [==============================] - 0s 185us/step - loss: 5351.9667 - smape_kun2: 36.2805\n",
      "Epoch 389/500\n",
      "475/475 [==============================] - 0s 210us/step - loss: 5379.7872 - smape_kun2: 36.1856\n",
      "Epoch 390/500\n",
      "475/475 [==============================] - 0s 185us/step - loss: 5344.9290 - smape_kun2: 36.3377\n",
      "Epoch 391/500\n",
      "475/475 [==============================] - 0s 157us/step - loss: 5381.4292 - smape_kun2: 36.1775\n",
      "Epoch 392/500\n",
      "475/475 [==============================] - 0s 174us/step - loss: 5399.3275 - smape_kun2: 36.4797\n",
      "Epoch 393/500\n",
      "475/475 [==============================] - 0s 183us/step - loss: 5305.3370 - smape_kun2: 36.1687\n",
      "Epoch 394/500\n",
      "475/475 [==============================] - 0s 155us/step - loss: 5363.7113 - smape_kun2: 36.0591\n",
      "Epoch 395/500\n",
      "475/475 [==============================] - 0s 155us/step - loss: 5354.4790 - smape_kun2: 36.1643\n",
      "Epoch 396/500\n",
      "475/475 [==============================] - 0s 195us/step - loss: 5326.0696 - smape_kun2: 36.1678\n",
      "Epoch 397/500\n",
      "475/475 [==============================] - 0s 189us/step - loss: 5334.1663 - smape_kun2: 36.1293\n",
      "Epoch 398/500\n",
      "475/475 [==============================] - 0s 170us/step - loss: 5349.5133 - smape_kun2: 36.3158\n",
      "Epoch 399/500\n",
      "475/475 [==============================] - 0s 170us/step - loss: 5357.2645 - smape_kun2: 36.0812\n",
      "Epoch 400/500\n",
      "475/475 [==============================] - 0s 139us/step - loss: 5379.8247 - smape_kun2: 36.1473\n",
      "Epoch 401/500\n",
      "475/475 [==============================] - 0s 147us/step - loss: 5306.8036 - smape_kun2: 36.0253\n",
      "Epoch 402/500\n",
      "475/475 [==============================] - 0s 290us/step - loss: 5359.8918 - smape_kun2: 36.0265\n",
      "Epoch 403/500\n",
      "475/475 [==============================] - 0s 244us/step - loss: 5337.1738 - smape_kun2: 36.1113\n",
      "Epoch 404/500\n",
      "475/475 [==============================] - 0s 237us/step - loss: 5328.9656 - smape_kun2: 36.1131\n",
      "Epoch 405/500\n",
      "475/475 [==============================] - 0s 292us/step - loss: 5321.4881 - smape_kun2: 36.1307\n",
      "Epoch 406/500\n",
      "475/475 [==============================] - 0s 172us/step - loss: 5336.3375 - smape_kun2: 36.1833\n",
      "Epoch 407/500\n",
      "475/475 [==============================] - 0s 189us/step - loss: 5346.4779 - smape_kun2: 36.2114\n",
      "Epoch 408/500\n",
      "475/475 [==============================] - 0s 248us/step - loss: 5362.9017 - smape_kun2: 35.9757\n",
      "Epoch 409/500\n",
      "475/475 [==============================] - 0s 178us/step - loss: 5358.1874 - smape_kun2: 36.1029\n",
      "Epoch 410/500\n",
      "475/475 [==============================] - 0s 185us/step - loss: 5343.6404 - smape_kun2: 36.1056\n",
      "Epoch 411/500\n",
      "475/475 [==============================] - 0s 193us/step - loss: 5345.5125 - smape_kun2: 36.1495\n",
      "Epoch 412/500\n",
      "475/475 [==============================] - 0s 130us/step - loss: 5359.4325 - smape_kun2: 36.1567\n",
      "Epoch 413/500\n",
      "475/475 [==============================] - 0s 181us/step - loss: 5319.6273 - smape_kun2: 36.1059\n",
      "Epoch 414/500\n",
      "475/475 [==============================] - 0s 332us/step - loss: 5345.0340 - smape_kun2: 36.1271\n",
      "Epoch 415/500\n",
      "475/475 [==============================] - 0s 210us/step - loss: 5334.1389 - smape_kun2: 36.2057\n",
      "Epoch 416/500\n",
      "475/475 [==============================] - 0s 206us/step - loss: 5330.1800 - smape_kun2: 36.2485\n",
      "Epoch 417/500\n",
      "475/475 [==============================] - 0s 130us/step - loss: 5330.2989 - smape_kun2: 36.1257\n",
      "Epoch 418/500\n",
      "475/475 [==============================] - 0s 134us/step - loss: 5362.5737 - smape_kun2: 36.0655\n",
      "Epoch 419/500\n",
      "475/475 [==============================] - 0s 183us/step - loss: 5364.7645 - smape_kun2: 36.4326\n",
      "Epoch 420/500\n",
      "475/475 [==============================] - 0s 212us/step - loss: 5348.0740 - smape_kun2: 36.3973\n",
      "Epoch 421/500\n",
      "475/475 [==============================] - 0s 199us/step - loss: 5329.5100 - smape_kun2: 36.1204\n",
      "Epoch 422/500\n",
      "475/475 [==============================] - 0s 233us/step - loss: 5345.2658 - smape_kun2: 36.0381\n",
      "Epoch 423/500\n",
      "475/475 [==============================] - 0s 187us/step - loss: 5353.1296 - smape_kun2: 36.2862\n",
      "Epoch 424/500\n",
      "475/475 [==============================] - 0s 183us/step - loss: 5346.3408 - smape_kun2: 36.0861\n",
      "Epoch 425/500\n",
      "475/475 [==============================] - 0s 178us/step - loss: 5349.3607 - smape_kun2: 36.2074\n",
      "Epoch 426/500\n",
      "475/475 [==============================] - 0s 206us/step - loss: 5340.3496 - smape_kun2: 36.2247\n",
      "Epoch 427/500\n",
      "475/475 [==============================] - 0s 162us/step - loss: 5336.1658 - smape_kun2: 36.0836\n",
      "Epoch 428/500\n",
      "475/475 [==============================] - 0s 214us/step - loss: 5317.6027 - smape_kun2: 36.0672\n",
      "Epoch 429/500\n",
      "475/475 [==============================] - 0s 187us/step - loss: 5321.3043 - smape_kun2: 36.1559\n",
      "Epoch 430/500\n",
      "475/475 [==============================] - 0s 120us/step - loss: 5324.5088 - smape_kun2: 36.0839\n",
      "Epoch 431/500\n",
      "475/475 [==============================] - 0s 109us/step - loss: 5312.7430 - smape_kun2: 36.0697\n",
      "Epoch 432/500\n",
      "475/475 [==============================] - 0s 99us/step - loss: 5323.8841 - smape_kun2: 36.0738\n",
      "Epoch 433/500\n",
      "475/475 [==============================] - 0s 101us/step - loss: 5342.4236 - smape_kun2: 36.0269\n",
      "Epoch 434/500\n",
      "475/475 [==============================] - 0s 103us/step - loss: 5328.0928 - smape_kun2: 36.0683\n",
      "Epoch 435/500\n",
      "475/475 [==============================] - 0s 107us/step - loss: 5338.6731 - smape_kun2: 36.2631\n",
      "Epoch 436/500\n",
      "475/475 [==============================] - 0s 105us/step - loss: 5466.6376 - smape_kun2: 36.8124\n",
      "Epoch 437/500\n",
      "475/475 [==============================] - 0s 107us/step - loss: 5384.8742 - smape_kun2: 36.1681\n",
      "Epoch 438/500\n",
      "475/475 [==============================] - 0s 103us/step - loss: 5349.3016 - smape_kun2: 36.2343\n",
      "Epoch 439/500\n",
      "475/475 [==============================] - 0s 122us/step - loss: 5484.2797 - smape_kun2: 36.1562\n",
      "Epoch 440/500\n",
      "475/475 [==============================] - 0s 126us/step - loss: 5375.0232 - smape_kun2: 36.2552\n",
      "Epoch 441/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "475/475 [==============================] - 0s 111us/step - loss: 5377.0448 - smape_kun2: 36.1810\n",
      "Epoch 442/500\n",
      "475/475 [==============================] - 0s 92us/step - loss: 5356.5267 - smape_kun2: 36.1158\n",
      "Epoch 443/500\n",
      "475/475 [==============================] - 0s 120us/step - loss: 5363.3819 - smape_kun2: 36.2975\n",
      "Epoch 444/500\n",
      "475/475 [==============================] - 0s 103us/step - loss: 5318.9728 - smape_kun2: 36.0456\n",
      "Epoch 445/500\n",
      "475/475 [==============================] - 0s 84us/step - loss: 5345.1021 - smape_kun2: 36.0801\n",
      "Epoch 446/500\n",
      "475/475 [==============================] - 0s 94us/step - loss: 5349.0850 - smape_kun2: 36.3693\n",
      "Epoch 447/500\n",
      "475/475 [==============================] - 0s 105us/step - loss: 5343.7777 - smape_kun2: 36.0449\n",
      "Epoch 448/500\n",
      "475/475 [==============================] - 0s 176us/step - loss: 5336.7034 - smape_kun2: 36.0533\n",
      "Epoch 449/500\n",
      "475/475 [==============================] - 0s 189us/step - loss: 5318.1898 - smape_kun2: 36.1343\n",
      "Epoch 450/500\n",
      "475/475 [==============================] - 0s 115us/step - loss: 5323.5095 - smape_kun2: 36.0929\n",
      "Epoch 451/500\n",
      "475/475 [==============================] - 0s 124us/step - loss: 5347.2822 - smape_kun2: 36.2960\n",
      "Epoch 452/500\n",
      "475/475 [==============================] - 0s 113us/step - loss: 5358.0013 - smape_kun2: 36.1869\n",
      "Epoch 453/500\n",
      "475/475 [==============================] - 0s 103us/step - loss: 5408.3108 - smape_kun2: 36.4798\n",
      "Epoch 454/500\n",
      "475/475 [==============================] - 0s 124us/step - loss: 5324.1272 - smape_kun2: 36.2735\n",
      "Epoch 455/500\n",
      "475/475 [==============================] - 0s 99us/step - loss: 5332.2806 - smape_kun2: 36.0180\n",
      "Epoch 456/500\n",
      "475/475 [==============================] - 0s 90us/step - loss: 5336.2863 - smape_kun2: 35.9756\n",
      "Epoch 457/500\n",
      "475/475 [==============================] - 0s 86us/step - loss: 5338.9308 - smape_kun2: 36.1774\n",
      "Epoch 458/500\n",
      "475/475 [==============================] - 0s 69us/step - loss: 5378.2120 - smape_kun2: 35.9871\n",
      "Epoch 459/500\n",
      "475/475 [==============================] - 0s 88us/step - loss: 5431.9256 - smape_kun2: 36.2942\n",
      "Epoch 460/500\n",
      "475/475 [==============================] - 0s 66us/step - loss: 5338.9349 - smape_kun2: 36.1686\n",
      "Epoch 461/500\n",
      "475/475 [==============================] - 0s 124us/step - loss: 5352.7071 - smape_kun2: 36.1291\n",
      "Epoch 462/500\n",
      "475/475 [==============================] - 0s 88us/step - loss: 5341.9195 - smape_kun2: 36.2484\n",
      "Epoch 463/500\n",
      "475/475 [==============================] - 0s 86us/step - loss: 5328.1453 - smape_kun2: 36.1036\n",
      "Epoch 464/500\n",
      "475/475 [==============================] - 0s 94us/step - loss: 5334.9626 - smape_kun2: 36.1879\n",
      "Epoch 465/500\n",
      "475/475 [==============================] - 0s 122us/step - loss: 5330.9116 - smape_kun2: 36.2422\n",
      "Epoch 466/500\n",
      "475/475 [==============================] - 0s 206us/step - loss: 5343.0218 - smape_kun2: 36.2149\n",
      "Epoch 467/500\n",
      "475/475 [==============================] - 0s 128us/step - loss: 5386.8922 - smape_kun2: 36.0545\n",
      "Epoch 468/500\n",
      "475/475 [==============================] - 0s 115us/step - loss: 5359.4217 - smape_kun2: 36.2772\n",
      "Epoch 469/500\n",
      "475/475 [==============================] - 0s 120us/step - loss: 5345.7411 - smape_kun2: 36.2542\n",
      "Epoch 470/500\n",
      "475/475 [==============================] - 0s 109us/step - loss: 5340.6559 - smape_kun2: 36.1890\n",
      "Epoch 471/500\n",
      "475/475 [==============================] - 0s 67us/step - loss: 5349.9471 - smape_kun2: 36.1955\n",
      "Epoch 472/500\n",
      "475/475 [==============================] - 0s 134us/step - loss: 5343.8413 - smape_kun2: 36.1865\n",
      "Epoch 473/500\n",
      "475/475 [==============================] - 0s 99us/step - loss: 5331.0970 - smape_kun2: 36.2430\n",
      "Epoch 474/500\n",
      "475/475 [==============================] - 0s 90us/step - loss: 5333.1835 - smape_kun2: 36.1705\n",
      "Epoch 475/500\n",
      "475/475 [==============================] - 0s 97us/step - loss: 5343.6369 - smape_kun2: 36.1573\n",
      "Epoch 476/500\n",
      "475/475 [==============================] - 0s 101us/step - loss: 5335.1976 - smape_kun2: 36.0965\n",
      "Epoch 477/500\n",
      "475/475 [==============================] - 0s 94us/step - loss: 5331.1176 - smape_kun2: 36.1299\n",
      "Epoch 478/500\n",
      "475/475 [==============================] - 0s 90us/step - loss: 5337.2761 - smape_kun2: 36.0551\n",
      "Epoch 479/500\n",
      "475/475 [==============================] - 0s 92us/step - loss: 5363.2186 - smape_kun2: 36.1788\n",
      "Epoch 480/500\n",
      "475/475 [==============================] - 0s 103us/step - loss: 5325.2847 - smape_kun2: 36.0789\n",
      "Epoch 481/500\n",
      "475/475 [==============================] - 0s 88us/step - loss: 5346.2634 - smape_kun2: 36.0341\n",
      "Epoch 482/500\n",
      "475/475 [==============================] - 0s 90us/step - loss: 5323.0998 - smape_kun2: 36.1112\n",
      "Epoch 483/500\n",
      "475/475 [==============================] - 0s 86us/step - loss: 5349.9697 - smape_kun2: 36.2335\n",
      "Epoch 484/500\n",
      "475/475 [==============================] - 0s 90us/step - loss: 5356.2153 - smape_kun2: 36.0442\n",
      "Epoch 485/500\n",
      "475/475 [==============================] - 0s 86us/step - loss: 5340.1761 - smape_kun2: 36.2121\n",
      "Epoch 486/500\n",
      "475/475 [==============================] - 0s 94us/step - loss: 5327.2610 - smape_kun2: 36.2968\n",
      "Epoch 487/500\n",
      "475/475 [==============================] - 0s 84us/step - loss: 5319.4816 - smape_kun2: 36.1377\n",
      "Epoch 488/500\n",
      "475/475 [==============================] - 0s 105us/step - loss: 5347.2216 - smape_kun2: 36.1244\n",
      "Epoch 489/500\n",
      "475/475 [==============================] - 0s 86us/step - loss: 5336.1449 - smape_kun2: 36.1490\n",
      "Epoch 490/500\n",
      "475/475 [==============================] - 0s 83us/step - loss: 5341.2647 - smape_kun2: 36.1527\n",
      "Epoch 491/500\n",
      "475/475 [==============================] - 0s 66us/step - loss: 5330.8299 - smape_kun2: 36.2821\n",
      "Epoch 492/500\n",
      "475/475 [==============================] - 0s 79us/step - loss: 5356.5780 - smape_kun2: 36.2123\n",
      "Epoch 493/500\n",
      "475/475 [==============================] - 0s 66us/step - loss: 5347.5576 - smape_kun2: 36.0939\n",
      "Epoch 494/500\n",
      "475/475 [==============================] - 0s 66us/step - loss: 5338.2573 - smape_kun2: 36.2533\n",
      "Epoch 495/500\n",
      "475/475 [==============================] - 0s 80us/step - loss: 5329.8580 - smape_kun2: 36.1253\n",
      "Epoch 496/500\n",
      "475/475 [==============================] - 0s 101us/step - loss: 5315.3754 - smape_kun2: 36.1528\n",
      "Epoch 497/500\n",
      "475/475 [==============================] - 0s 78us/step - loss: 5333.2289 - smape_kun2: 36.1969\n",
      "Epoch 498/500\n",
      "475/475 [==============================] - 0s 78us/step - loss: 5351.5919 - smape_kun2: 36.1773\n",
      "Epoch 499/500\n",
      "475/475 [==============================] - 0s 191us/step - loss: 5388.0360 - smape_kun2: 36.1219\n",
      "Epoch 500/500\n",
      "475/475 [==============================] - 0s 122us/step - loss: 5357.7534 - smape_kun2: 36.2610\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "def build_regressor():\n",
    "    regressor = Sequential()\n",
    "    regressor.add(Dense(units=60, input_dim=30))\n",
    "    regressor.add(Dense(units=80))\n",
    "    regressor.add(Dense(units=1))\n",
    "    regressor.compile(optimizer='adam', loss='mean_squared_error',  metrics=[smape_kun2])\n",
    "    return regressor\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "regressor = KerasRegressor(build_fn=build_regressor, batch_size=64,epochs=500)\n",
    "\n",
    "results=regressor.fit(X_Train,Y_Train)\n",
    "\n",
    "preds = regressor.predict(X_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[890.8737   276.46103  596.31995  967.76605  444.57666  368.0778\n",
      " 908.9635   254.65173  760.87427  210.63638  -42.37513   48.488903\n",
      " 678.8518   359.79123  754.20416  835.45465  257.5565   132.3937\n",
      " 260.53668  133.7095   947.7647   232.34094  186.837    155.31703\n",
      " 138.75868 ]\n",
      "26.052232887845776\n"
     ]
    }
   ],
   "source": [
    "#preds = twoone(preds)\n",
    "print(preds)\n",
    "print(smape_kun(Y_Test,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2,svd_solver='full')\n",
    "X_pca = pca.fit_transform(X)\n",
    "# print(pca.explained_variance_)\n",
    "\n",
    "X_reduced, X_test_reduced, Y_Train, Y_Test = train_test_split(X_pca, Y, test_size = 0.05, random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.09596894778119\n",
      "[808.00692392 168.07008994 655.99543112 868.81104253 534.1630428\n",
      " 249.16776434 825.62297657 329.74390178 722.22092148 186.66001476\n",
      "  60.02655008 123.26864696 672.00425871 204.35558482 719.59177543\n",
      " 751.34258603 135.16065898 263.91702929 127.47422573 171.28422122\n",
      " 838.67664428 287.51104222 125.58149886 215.24300264 238.27416895]\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(X_reduced,Y_Train)\n",
    "\n",
    "y_pred = reg.predict(X_test_reduced)\n",
    "\n",
    "print(smape_kun(Y_Test,y_pred))\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.89290866285403\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/nickycan/much-faster-smape-metric-function\n",
    "# https://stackoverflow.com/questions/51444630/how-to-use-smape-evaluation-metric-on-train-dataset\n",
    "# https://stackoverflow.com/questions/41925157/logisticregression-unknown-label-type-continuous-using-sklearn-in-python\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_jobs=-100000000000000000000000000000000000000000000000000000000000000000000000)\n",
    "\n",
    "model.fit(X_Train,Y_Train)\n",
    "\n",
    "y_pred = model.predict(X_Test)\n",
    "\n",
    "print(smape_kun(Y_Test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 858.10393263  338.35985324  593.78704778  980.26435107  395.15560108\n",
      "  322.48834466  852.3925124   282.36088116  707.46396514  236.61318795\n",
      "    0.          -29.68706276  607.4751719   303.83908373  781.92738793\n",
      "  800.64062639  307.42732284  215.41203314  242.88912554   90.35830554\n",
      " 1045.57883515  170.22135781  184.55509373  177.8287852   196.20445605]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Reduced Dataset Using Main Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2095,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df[['x23','x27','x1']].values\n",
    "Y = df['y'].fillna(0)\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size = 0.05, random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.947119659857407\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_jobs=-100000000000000000000000000000000000000000000000000000000000000000000000)\n",
    "\n",
    "model.fit(X_Train,Y_Train)\n",
    "\n",
    "y_pred = model.predict(X_Test)\n",
    "\n",
    "print(smape_kun(Y_Test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[973.35741739 290.7526603  550.16044726 965.88266561 389.49491833\n",
      " 326.10931005 796.04043019 226.79088581 757.97755271 248.23066973\n",
      "   0.         -35.35359376 625.74974575 332.37854465 819.4811063\n",
      " 856.29751248 292.83895775 226.67837731 261.63100507  59.87685686\n",
      " 980.00510116 203.11722888 135.09892497 173.76461476 195.18597236]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Reduced Dataset Using Main Features (amplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df[['x23','x27','x1','x30','x25']].values\n",
    "Y = df['y'].fillna(0)\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size = 0.05, random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2788,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.01631533302832\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_jobs=-100000000000000000000000000000000000000000000000000000000000000000000000)\n",
    "\n",
    "model.fit(X_Train,Y_Train)\n",
    "\n",
    "y_pred = model.predict(X_Test)\n",
    "\n",
    "print(smape_kun(Y_Test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2789,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1004.49299284  296.67215509  606.92203677  930.80696417  389.45011453\n",
      "  322.79420456  797.41376627  224.13645043  776.42725742  224.60236181\n",
      "    0.          -28.86249122  607.64636444  312.73809626  768.1643219\n",
      "  832.00251452  342.38202229  185.27106599  217.03195252   81.4940105\n",
      " 1010.36273526  189.87525196  167.49650988  170.41302696  237.07337841]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2599,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.56804321077557\n",
      "[819.75728623 156.31577423 585.4534609  773.7382153  469.04389869\n",
      " 216.02018993 778.59398493 374.52788425 642.70970511  69.50485025\n",
      "  71.26889868  16.69786567 620.97426679 181.84609393 664.33560279\n",
      " 746.48143099 162.70711136 376.33849894 237.81512018  64.85932864\n",
      " 809.79270624 315.69572433 307.93111503 370.78816388 370.88070697]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(X_Train,Y_Train)\n",
    "\n",
    "y_pred = reg.predict(X_Test)\n",
    "\n",
    "print(smape_kun(Y_Test,y_pred))\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Deep Learning (new approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2806,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "475/475 [==============================] - 11s 24ms/step - loss: 203049.2304 - smape_kun2: 195.7171\n",
      "Epoch 2/8\n",
      "475/475 [==============================] - 0s 210us/step - loss: 186720.7950 - smape_kun2: 181.7566\n",
      "Epoch 3/8\n",
      "475/475 [==============================] - 0s 229us/step - loss: 149973.8881 - smape_kun2: 146.1624\n",
      "Epoch 4/8\n",
      "475/475 [==============================] - 0s 216us/step - loss: 122084.5621 - smape_kun2: 125.3523\n",
      "Epoch 5/8\n",
      "475/475 [==============================] - 0s 233us/step - loss: 102739.6822 - smape_kun2: 124.3061\n",
      "Epoch 6/8\n",
      "475/475 [==============================] - 0s 231us/step - loss: 72982.2772 - smape_kun2: 107.5489\n",
      "Epoch 7/8\n",
      "475/475 [==============================] - 0s 267us/step - loss: 37188.7445 - smape_kun2: 76.9594\n",
      "Epoch 8/8\n",
      "475/475 [==============================] - 0s 301us/step - loss: 23492.4762 - smape_kun2: 54.0372\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "def build_regressor():\n",
    "    regressor = Sequential()\n",
    "    regressor.add(Dense(units=20, input_dim=5))\n",
    "    regressor.add(Dense(units=80))\n",
    "    regressor.add(Dense(units=40))\n",
    "    regressor.add(Dense(units=10))\n",
    "    regressor.add(Dense(units=80))\n",
    "    regressor.add(Dense(units=40))\n",
    "    regressor.add(Dense(units=1))\n",
    "    regressor.compile(optimizer='adam', loss='mean_squared_error',  metrics=[smape_kun2])\n",
    "    return regressor\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "regressor = KerasRegressor(build_fn=build_regressor, batch_size=32,epochs=8)\n",
    "\n",
    "results=regressor.fit(X_Train,Y_Train)\n",
    "\n",
    "preds = regressor.predict(X_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2807,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[710.4563   215.78812  593.3473   727.5604   501.24008  223.35695\n",
      " 665.9081   438.1447   586.57715  140.50111   44.925117  50.298145\n",
      " 625.37634  234.62103  616.1633   640.2709   213.27249  397.76004\n",
      " 299.56528  108.69307  726.1779   444.77487  304.3918   357.29996\n",
      " 379.29443 ]\n",
      "47.17531720243735\n"
     ]
    }
   ],
   "source": [
    "#preds = twoone(preds)\n",
    "print(preds)\n",
    "print(smape_kun(Y_Test,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input data:\n",
      "(497, 124)\n",
      "Training set shape for X (inputs):\n",
      "(472, 123)\n",
      "Training set shape for Y (output):\n",
      "(472,)\n",
      "(472, 123, 1)\n",
      "(25, 123, 1)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_91 (LSTM)               (None, 123, 3)            60        \n",
      "_________________________________________________________________\n",
      "dropout_77 (Dropout)         (None, 123, 3)            0         \n",
      "_________________________________________________________________\n",
      "lstm_92 (LSTM)               (None, 256)               266240    \n",
      "_________________________________________________________________\n",
      "dropout_78 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 266,557\n",
      "Trainable params: 266,557\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 424 samples, validate on 48 samples\n",
      "Epoch 1/100\n",
      "424/424 [==============================] - 22s 52ms/step - loss: 196.7672 - val_loss: 181.2472\n",
      "Epoch 2/100\n",
      "424/424 [==============================] - 6s 14ms/step - loss: 182.9561 - val_loss: 171.3772\n",
      "Epoch 3/100\n",
      "424/424 [==============================] - 6s 14ms/step - loss: 171.7416 - val_loss: 157.6683\n",
      "Epoch 4/100\n",
      "424/424 [==============================] - 7s 16ms/step - loss: 154.9986 - val_loss: 136.6821\n",
      "Epoch 5/100\n",
      "424/424 [==============================] - 6s 15ms/step - loss: 132.8697 - val_loss: 102.3504\n",
      "Epoch 6/100\n",
      "424/424 [==============================] - 7s 15ms/step - loss: 100.8569 - val_loss: 154.3274\n",
      "Epoch 7/100\n",
      "424/424 [==============================] - 6s 15ms/step - loss: 154.8047 - val_loss: 91.0977\n",
      "Epoch 8/100\n",
      "424/424 [==============================] - 6s 15ms/step - loss: 100.4826 - val_loss: 85.5094\n",
      "Epoch 9/100\n",
      "424/424 [==============================] - 7s 16ms/step - loss: 88.2387 - val_loss: 95.2731\n",
      "Epoch 10/100\n",
      "424/424 [==============================] - 7s 17ms/step - loss: 94.2712 - val_loss: 99.3287\n",
      "Epoch 11/100\n",
      "424/424 [==============================] - 7s 16ms/step - loss: 98.6190 - val_loss: 100.3767\n",
      "Epoch 12/100\n",
      "424/424 [==============================] - 7s 16ms/step - loss: 99.0197 - val_loss: 99.6721\n",
      "Epoch 13/100\n",
      "424/424 [==============================] - 7s 17ms/step - loss: 98.2685 - val_loss: 97.8437\n",
      "Epoch 14/100\n",
      "424/424 [==============================] - 7s 16ms/step - loss: 96.5077 - val_loss: 95.1525\n",
      "Epoch 15/100\n",
      "424/424 [==============================] - 8s 19ms/step - loss: 94.0423 - val_loss: 91.7735\n",
      "Epoch 16/100\n",
      "424/424 [==============================] - 7s 16ms/step - loss: 91.3501 - val_loss: 87.7617\n",
      "Epoch 17/100\n",
      "424/424 [==============================] - 6s 15ms/step - loss: 88.4748 - val_loss: 83.9422\n",
      "Epoch 18/100\n",
      "424/424 [==============================] - 7s 15ms/step - loss: 86.3650 - val_loss: 80.2093\n",
      "Epoch 19/100\n",
      "424/424 [==============================] - 7s 16ms/step - loss: 84.0166 - val_loss: 77.2433\n",
      "Epoch 20/100\n",
      "424/424 [==============================] - 7s 16ms/step - loss: 82.9946 - val_loss: 75.3853\n",
      "Epoch 21/100\n",
      "424/424 [==============================] - 7s 16ms/step - loss: 84.0637 - val_loss: 75.2517\n",
      "Epoch 22/100\n",
      "424/424 [==============================] - 6s 15ms/step - loss: 84.7922 - val_loss: 75.7612\n",
      "Epoch 23/100\n",
      "424/424 [==============================] - 6s 14ms/step - loss: 86.3470 - val_loss: 75.7691\n",
      "Epoch 24/100\n",
      "424/424 [==============================] - 8s 19ms/step - loss: 86.1426 - val_loss: 75.2548\n",
      "Epoch 25/100\n",
      "424/424 [==============================] - 7s 16ms/step - loss: 85.0906 - val_loss: 74.6867\n",
      "Epoch 26/100\n",
      "424/424 [==============================] - 7s 18ms/step - loss: 84.5048 - val_loss: 74.8864\n",
      "Epoch 27/100\n",
      "424/424 [==============================] - 7s 17ms/step - loss: 82.9599 - val_loss: 75.6900\n",
      "Epoch 28/100\n",
      "424/424 [==============================] - 7s 16ms/step - loss: 82.8440 - val_loss: 76.6169\n",
      "Epoch 29/100\n",
      "424/424 [==============================] - 7s 16ms/step - loss: 82.2312 - val_loss: 77.4726\n",
      "Epoch 30/100\n",
      "424/424 [==============================] - 7s 15ms/step - loss: 82.0806 - val_loss: 78.4233\n",
      "Epoch 31/100\n",
      "424/424 [==============================] - 6s 15ms/step - loss: 83.1273 - val_loss: 79.1351\n",
      "Epoch 32/100\n",
      "424/424 [==============================] - 6s 15ms/step - loss: 82.9800 - val_loss: 79.6366\n",
      "Epoch 33/100\n",
      "424/424 [==============================] - 6s 15ms/step - loss: 83.6304 - val_loss: 79.8158\n",
      "Epoch 34/100\n",
      "424/424 [==============================] - 7s 17ms/step - loss: 83.7712 - val_loss: 79.7507\n",
      "Epoch 35/100\n",
      "424/424 [==============================] - 7s 16ms/step - loss: 83.2507 - val_loss: 79.4105\n",
      "Epoch 36/100\n",
      "424/424 [==============================] - 7s 16ms/step - loss: 83.1496 - val_loss: 78.8301\n",
      "Epoch 37/100\n",
      "424/424 [==============================] - 7s 16ms/step - loss: 82.6612 - val_loss: 78.0922\n",
      "Epoch 38/100\n",
      "424/424 [==============================] - 7s 16ms/step - loss: 82.5410 - val_loss: 77.2827\n",
      "Epoch 39/100\n",
      "424/424 [==============================] - 7s 16ms/step - loss: 81.8106 - val_loss: 76.5132\n",
      "Epoch 40/100\n",
      "424/424 [==============================] - 6s 15ms/step - loss: 81.6108 - val_loss: 75.8454\n",
      "Epoch 41/100\n",
      "424/424 [==============================] - 6s 15ms/step - loss: 81.1823 - val_loss: 75.2927\n",
      "Epoch 42/100\n",
      "424/424 [==============================] - 6s 15ms/step - loss: 81.5965 - val_loss: 74.7386\n",
      "Epoch 43/100\n",
      "424/424 [==============================] - 7s 16ms/step - loss: 82.3389 - val_loss: 74.3896\n",
      "Epoch 44/100\n",
      "424/424 [==============================] - 7s 16ms/step - loss: 81.9079 - val_loss: 74.1817\n",
      "Epoch 45/100\n",
      "424/424 [==============================] - 7s 16ms/step - loss: 81.7974 - val_loss: 74.0597\n",
      "Epoch 46/100\n",
      "424/424 [==============================] - 7s 16ms/step - loss: 81.9157 - val_loss: 73.9990\n",
      "Epoch 47/100\n",
      "424/424 [==============================] - 7s 16ms/step - loss: 82.0212 - val_loss: 74.0260\n",
      "Epoch 48/100\n",
      "424/424 [==============================] - 6s 15ms/step - loss: 81.7516 - val_loss: 74.1188\n",
      "Epoch 49/100\n",
      "424/424 [==============================] - 6s 15ms/step - loss: 81.6870 - val_loss: 74.2931\n",
      "Epoch 50/100\n",
      "424/424 [==============================] - 6s 14ms/step - loss: 81.6389 - val_loss: 74.4749\n",
      "Epoch 51/100\n",
      "424/424 [==============================] - 6s 15ms/step - loss: 81.5708 - val_loss: 74.7213\n",
      "Epoch 52/100\n",
      "424/424 [==============================] - 7s 16ms/step - loss: 81.2570 - val_loss: 74.9715\n",
      "Epoch 53/100\n",
      "424/424 [==============================] - 7s 16ms/step - loss: 81.0896 - val_loss: 75.2046\n",
      "Epoch 54/100\n",
      "424/424 [==============================] - 7s 16ms/step - loss: 81.0615 - val_loss: 75.4595\n",
      "Epoch 55/100\n",
      "424/424 [==============================] - 7s 17ms/step - loss: 81.5871 - val_loss: 75.5774\n",
      "Epoch 56/100\n",
      "424/424 [==============================] - 6s 15ms/step - loss: 81.1563 - val_loss: 75.5469\n",
      "Epoch 57/100\n",
      "424/424 [==============================] - 6s 14ms/step - loss: 81.0020 - val_loss: 75.4159\n",
      "Epoch 58/100\n",
      "424/424 [==============================] - 6s 14ms/step - loss: 81.0649 - val_loss: 75.0983\n",
      "Epoch 59/100\n",
      "424/424 [==============================] - 8s 20ms/step - loss: 81.0781 - val_loss: 74.6346\n",
      "Epoch 60/100\n",
      "424/424 [==============================] - 10s 24ms/step - loss: 80.6418 - val_loss: 74.2112\n",
      "Epoch 61/100\n",
      "424/424 [==============================] - 11s 26ms/step - loss: 81.0066 - val_loss: 73.8833\n",
      "Epoch 62/100\n",
      "424/424 [==============================] - 9s 22ms/step - loss: 80.3579 - val_loss: 73.5490\n",
      "Epoch 63/100\n",
      "424/424 [==============================] - 9s 22ms/step - loss: 80.7598 - val_loss: 73.1913\n",
      "Epoch 64/100\n",
      "424/424 [==============================] - 9s 20ms/step - loss: 80.9440 - val_loss: 72.9367\n",
      "Epoch 65/100\n",
      "424/424 [==============================] - 9s 21ms/step - loss: 80.3054 - val_loss: 72.8065\n",
      "Epoch 66/100\n",
      "424/424 [==============================] - 8s 18ms/step - loss: 80.7451 - val_loss: 72.7416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/100\n",
      "424/424 [==============================] - 8s 20ms/step - loss: 80.0644 - val_loss: 72.6984\n",
      "Epoch 68/100\n",
      "424/424 [==============================] - 8s 20ms/step - loss: 80.4553 - val_loss: 72.7068\n",
      "Epoch 69/100\n",
      "424/424 [==============================] - 7s 16ms/step - loss: 80.2698 - val_loss: 72.7497\n",
      "Epoch 70/100\n",
      "424/424 [==============================] - 8s 18ms/step - loss: 80.2121 - val_loss: 72.7964\n",
      "Epoch 71/100\n",
      "424/424 [==============================] - 8s 18ms/step - loss: 80.0919 - val_loss: 72.7845\n",
      "Epoch 72/100\n",
      "424/424 [==============================] - 8s 20ms/step - loss: 79.8311 - val_loss: 72.7789\n",
      "Epoch 73/100\n",
      "424/424 [==============================] - 7s 17ms/step - loss: 80.0159 - val_loss: 72.7410\n",
      "Epoch 74/100\n",
      "424/424 [==============================] - 8s 20ms/step - loss: 80.0130 - val_loss: 72.6863\n",
      "Epoch 75/100\n",
      "424/424 [==============================] - 10s 23ms/step - loss: 79.9028 - val_loss: 72.5806\n",
      "Epoch 76/100\n",
      "424/424 [==============================] - 8s 19ms/step - loss: 79.5459 - val_loss: 72.4304\n",
      "Epoch 77/100\n",
      "424/424 [==============================] - 8s 18ms/step - loss: 79.5396 - val_loss: 72.1787\n",
      "Epoch 78/100\n",
      "424/424 [==============================] - 8s 18ms/step - loss: 79.7059 - val_loss: 71.8184\n",
      "Epoch 79/100\n",
      "424/424 [==============================] - 7s 17ms/step - loss: 79.8166 - val_loss: 71.4692\n",
      "Epoch 80/100\n",
      "424/424 [==============================] - 7s 17ms/step - loss: 79.1595 - val_loss: 71.2237\n",
      "Epoch 81/100\n",
      "424/424 [==============================] - 7s 16ms/step - loss: 79.4883 - val_loss: 71.0191\n",
      "Epoch 82/100\n",
      "424/424 [==============================] - 7s 17ms/step - loss: 78.9734 - val_loss: 70.8792\n",
      "Epoch 83/100\n",
      "424/424 [==============================] - 7s 16ms/step - loss: 79.5937 - val_loss: 70.7286\n",
      "Epoch 84/100\n",
      "424/424 [==============================] - 7s 17ms/step - loss: 79.0744 - val_loss: 70.5363\n",
      "Epoch 85/100\n",
      "424/424 [==============================] - 7s 17ms/step - loss: 79.4009 - val_loss: 70.4570\n",
      "Epoch 86/100\n",
      "424/424 [==============================] - 7s 17ms/step - loss: 79.7576 - val_loss: 70.4112\n",
      "Epoch 87/100\n",
      "424/424 [==============================] - 7s 17ms/step - loss: 79.3233 - val_loss: 70.4597\n",
      "Epoch 88/100\n",
      "424/424 [==============================] - 7s 16ms/step - loss: 78.4216 - val_loss: 70.5156\n",
      "Epoch 89/100\n",
      "424/424 [==============================] - 7s 15ms/step - loss: 79.0878 - val_loss: 70.5014\n",
      "Epoch 90/100\n",
      "424/424 [==============================] - 7s 18ms/step - loss: 79.1670 - val_loss: 70.3695\n",
      "Epoch 91/100\n",
      "424/424 [==============================] - 7s 16ms/step - loss: 78.3487 - val_loss: 70.3423\n",
      "Epoch 92/100\n",
      "424/424 [==============================] - 8s 18ms/step - loss: 78.0766 - val_loss: 70.3293\n",
      "Epoch 93/100\n",
      "424/424 [==============================] - 8s 20ms/step - loss: 78.5588 - val_loss: 70.3605\n",
      "Epoch 94/100\n",
      "424/424 [==============================] - 8s 19ms/step - loss: 78.3493 - val_loss: 70.3709\n",
      "Epoch 95/100\n",
      "424/424 [==============================] - 10s 23ms/step - loss: 78.4026 - val_loss: 70.3458\n",
      "Epoch 96/100\n",
      "424/424 [==============================] - 7s 16ms/step - loss: 78.4194 - val_loss: 70.3177\n",
      "Epoch 97/100\n",
      "424/424 [==============================] - 7s 15ms/step - loss: 78.8779 - val_loss: 70.3499\n",
      "Epoch 98/100\n",
      "424/424 [==============================] - 6s 15ms/step - loss: 78.8018 - val_loss: 70.2802\n",
      "Epoch 99/100\n",
      "424/424 [==============================] - 7s 17ms/step - loss: 78.9087 - val_loss: 70.1481\n",
      "Epoch 100/100\n",
      "424/424 [==============================] - 7s 16ms/step - loss: 78.2791 - val_loss: 70.0581\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "import numpy\n",
    "from sklearn.utils import shuffle\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    " \n",
    "df = df.fillna(0)\n",
    "# Normalize data on the (-1, 1) interval.\n",
    "scaler = MinMaxScaler(feature_range = (-1, 1))\n",
    "scaled = scaler.fit_transform(df.drop(['Unnamed: 0'], axis = 1).values)\n",
    "y = df['y'].values.astype(\"float32\").reshape(-1, 1)\n",
    "y = scaler.fit_transform(y)\n",
    "\n",
    "# Convert to data frame.\n",
    "series = pd.DataFrame(scaled)\n",
    "\n",
    "# Helper function to create a windowed data set.\n",
    "# FIXME: Copying & overwriting is flawed!\n",
    "def create_window(data, window_size = 1):    \n",
    "    data_s = data.copy()\n",
    "    for i in range(window_size):\n",
    "        data = pd.concat([data, data_s.shift(-(i + 1))], \n",
    "                            axis = 1)\n",
    "        \n",
    "    data.dropna(axis=0, inplace=True)\n",
    "    return(data)\n",
    "\n",
    "# FIXME: We'll use this only for demonstration purposes.\n",
    "series_backup = series.copy()\n",
    "t = create_window(series_backup, 1)\n",
    "t.head()\n",
    "\n",
    "window_size = 3\n",
    "series = create_window(series, window_size)\n",
    "print(\"Shape of input data:\")\n",
    "print(series.shape)\n",
    "\n",
    "# Using 80% of data for training, 20% for validation.\n",
    "# FIXME: Need to align with example 1.\n",
    "TRAIN_SIZE = 0.95\n",
    "\n",
    "nrow = round(TRAIN_SIZE * series.shape[0])\n",
    "\n",
    "train = series.iloc[:nrow, :]\n",
    "test = series.iloc[nrow:, :]\n",
    "\n",
    "# Shuffle training data.\n",
    "train = shuffle(train)\n",
    "\n",
    "train_X = train.iloc[:, :-1]\n",
    "test_X = test.iloc[:, :-1]\n",
    "\n",
    "train_Y = train.iloc[:, -1]\n",
    "test_Y = test.iloc[:, -1]\n",
    "#print(test_Y)\n",
    "\n",
    "print(\"Training set shape for X (inputs):\")\n",
    "print(train_X.shape)\n",
    "print(\"Training set shape for Y (output):\")\n",
    "print(train_Y.shape)\n",
    "\n",
    "train_X = np.reshape(train_X.values, (train_X.shape[0], train_X.shape[1], 1))\n",
    "test_X = np.reshape(test_X.values, (test_X.shape[0], test_X.shape[1], 1))\n",
    "\n",
    "print(train_X.shape)\n",
    "print(test_X.shape)\n",
    "\n",
    "# Define the model.\n",
    "model2 = Sequential()\n",
    "model2.add(LSTM(input_shape = (123, 1), \n",
    "               units = window_size, \n",
    "               return_sequences = True))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(LSTM(256))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(1))\n",
    "model2.add(Activation(\"linear\"))\n",
    "model2.compile(loss = smape_kun2, \n",
    "              optimizer = \"adam\")\n",
    "model2.summary()\n",
    "\n",
    "# Fit the model.\n",
    "model2.fit(train_X, \n",
    "          train_Y, \n",
    "          batch_size = 512,\n",
    "          epochs = 100,\n",
    "          validation_split = 0.1)\n",
    "\n",
    "# Predict on test data.\n",
    "pred_test = model2.predict(test_X)\n",
    "\n",
    "\n",
    "#print('2',test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 1)\n",
      "MSE for predicted test set: 0.480295\n"
     ]
    }
   ],
   "source": [
    "# Apply inverse transformation to get back true values.\n",
    "o = np.reshape(test_Y.values,(-1, 1))\n",
    "print(o.shape)\n",
    "test_y_actual = scaler.inverse_transform(o)\n",
    "\n",
    "print(\"MSE for predicted test set: %2f\" % mean_squared_error(test_y_actual, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.61111957]\n",
      " [-0.72331846]\n",
      " [-0.72034   ]\n",
      " [-0.6269158 ]\n",
      " [-0.6582618 ]\n",
      " [-0.64835536]\n",
      " [-0.7090759 ]\n",
      " [-0.673497  ]\n",
      " [-0.70071363]\n",
      " [-0.7666764 ]\n",
      " [-0.6609383 ]\n",
      " [-0.7463302 ]\n",
      " [-0.6972548 ]\n",
      " [-0.6859517 ]\n",
      " [-0.61553586]\n",
      " [-0.6359142 ]\n",
      " [-0.7767145 ]\n",
      " [-0.55652344]\n",
      " [-0.72518015]\n",
      " [-0.6811406 ]\n",
      " [-0.7377026 ]\n",
      " [-0.68439806]\n",
      " [-0.71651304]\n",
      " [-0.72356   ]\n",
      " [-0.7897708 ]]\n",
      "200.0\n"
     ]
    }
   ],
   "source": [
    "#preds = twoone(preds)\n",
    "print(pred_test)\n",
    "\n",
    "# def twoone(list1):\n",
    "#     return [val for lst in list1 for val in lst]\n",
    "\n",
    "# pred_test = twoone(pred_test)\n",
    "# print(smape_kun2(test_y_actual, pred_test))\n",
    "Y_Test = K.variable(Y_Test)\n",
    "pred_test = K.variable(pred_test)\n",
    "\n",
    "print(K.eval(smape_kun2(Y_Test, pred_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries (training set, test set): (475, 25)\n"
     ]
    }
   ],
   "source": [
    "# Get the raw data values from the pandas data frame.\n",
    "df = df.fillna(0)\n",
    "data_raw = (df.drop(['Unnamed: 0'], axis = 1)).values.astype(\"float32\")\n",
    "y = df['y'].values.astype(\"float32\").reshape(-1, 1)\n",
    "y = scaler.fit_transform(y)\n",
    "\n",
    "# We apply the MinMax scaler from sklearn\n",
    "# to normalize data in the (0, 1) interval.\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "dataset = scaler.fit_transform(data_raw)\n",
    "\n",
    "# Print a few values.\n",
    "dataset[0:5]\n",
    "\n",
    "TRAIN_SIZE = 0.95\n",
    "\n",
    "train_size = int(len(dataset) * TRAIN_SIZE)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size, :], dataset[train_size:len(dataset), :]\n",
    "print(\"Number of entries (training set, test set): \" + str((len(train), len(test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: This helper function should be rewritten using numpy's shift function. See below.\n",
    "def create_dataset(dataset, window_size = 1):\n",
    "    data_X, data_Y = [], []\n",
    "    for i in range(len(dataset) - window_size - 1):\n",
    "        a = dataset[i:(i + window_size), 0]\n",
    "        data_X.append(a)\n",
    "        data_Y.append(dataset[i + window_size, 0])\n",
    "    return(np.array(data_X), np.array(data_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training data shape:\n",
      "(473, 1)\n",
      "New training data shape:\n",
      "(473, 1, 1)\n",
      "Epoch 1/100\n",
      " - 16s - loss: 55.0756\n",
      "Epoch 2/100\n",
      " - 2s - loss: 26.1142\n",
      "Epoch 3/100\n",
      " - 2s - loss: 25.9600\n",
      "Epoch 4/100\n",
      " - 2s - loss: 25.7724\n",
      "Epoch 5/100\n",
      " - 2s - loss: 25.6087\n",
      "Epoch 6/100\n",
      " - 2s - loss: 25.4123\n",
      "Epoch 7/100\n",
      " - 2s - loss: 25.4521\n",
      "Epoch 8/100\n",
      " - 2s - loss: 25.2616\n",
      "Epoch 9/100\n",
      " - 2s - loss: 25.2138\n",
      "Epoch 10/100\n",
      " - 1s - loss: 25.1534\n",
      "Epoch 11/100\n",
      " - 1s - loss: 25.2288\n",
      "Epoch 12/100\n",
      " - 1s - loss: 25.2183\n",
      "Epoch 13/100\n",
      " - 1s - loss: 25.1427\n",
      "Epoch 14/100\n",
      " - 1s - loss: 25.0938\n",
      "Epoch 15/100\n",
      " - 1s - loss: 25.0517\n",
      "Epoch 16/100\n",
      " - 1s - loss: 25.1864\n",
      "Epoch 17/100\n",
      " - 1s - loss: 25.0677\n",
      "Epoch 18/100\n",
      " - 1s - loss: 25.0811\n",
      "Epoch 19/100\n",
      " - 1s - loss: 25.1333\n",
      "Epoch 20/100\n",
      " - 1s - loss: 25.1060\n",
      "Epoch 21/100\n",
      " - 2s - loss: 25.0837\n",
      "Epoch 22/100\n",
      " - 1s - loss: 25.0206\n",
      "Epoch 23/100\n",
      " - 1s - loss: 25.1240\n",
      "Epoch 24/100\n",
      " - 1s - loss: 25.0657\n",
      "Epoch 25/100\n",
      " - 1s - loss: 25.0347\n",
      "Epoch 26/100\n",
      " - 1s - loss: 25.1397\n",
      "Epoch 27/100\n",
      " - 1s - loss: 25.0357\n",
      "Epoch 28/100\n",
      " - 1s - loss: 25.1069\n",
      "Epoch 29/100\n",
      " - 1s - loss: 25.0039\n",
      "Epoch 30/100\n",
      " - 1s - loss: 25.0392\n",
      "Epoch 31/100\n",
      " - 1s - loss: 25.1024\n",
      "Epoch 32/100\n",
      " - 2s - loss: 25.0854\n",
      "Epoch 33/100\n",
      " - 1s - loss: 25.1123\n",
      "Epoch 34/100\n",
      " - 1s - loss: 25.0905\n",
      "Epoch 35/100\n",
      " - 1s - loss: 25.0203\n",
      "Epoch 36/100\n",
      " - 1s - loss: 25.1244\n",
      "Epoch 37/100\n",
      " - 1s - loss: 25.0053\n",
      "Epoch 38/100\n",
      " - 1s - loss: 24.9940\n",
      "Epoch 39/100\n",
      " - 1s - loss: 25.1458\n",
      "Epoch 40/100\n",
      " - 1s - loss: 25.0618\n",
      "Epoch 41/100\n",
      " - 1s - loss: 25.0504\n",
      "Epoch 42/100\n",
      " - 1s - loss: 25.1427\n",
      "Epoch 43/100\n",
      " - 1s - loss: 25.0401\n",
      "Epoch 44/100\n",
      " - 1s - loss: 25.0968\n",
      "Epoch 45/100\n",
      " - 1s - loss: 25.0223\n",
      "Epoch 46/100\n",
      " - 1s - loss: 25.0473\n",
      "Epoch 47/100\n",
      " - 1s - loss: 24.9087\n",
      "Epoch 48/100\n",
      " - 1s - loss: 25.1301\n",
      "Epoch 49/100\n",
      " - 2s - loss: 25.0812\n",
      "Epoch 50/100\n",
      " - 1s - loss: 24.9856\n",
      "Epoch 51/100\n",
      " - 1s - loss: 25.1150\n",
      "Epoch 52/100\n",
      " - 1s - loss: 25.1346\n",
      "Epoch 53/100\n",
      " - 1s - loss: 25.1228\n",
      "Epoch 54/100\n",
      " - 1s - loss: 25.0214\n",
      "Epoch 55/100\n",
      " - 1s - loss: 25.1631\n",
      "Epoch 56/100\n",
      " - 1s - loss: 25.1265\n",
      "Epoch 57/100\n",
      " - 1s - loss: 24.9689\n",
      "Epoch 58/100\n",
      " - 1s - loss: 25.1180\n",
      "Epoch 59/100\n",
      " - 1s - loss: 24.9824\n",
      "Epoch 60/100\n",
      " - 2s - loss: 25.1574\n",
      "Epoch 61/100\n",
      " - 1s - loss: 25.0411\n",
      "Epoch 62/100\n",
      " - 1s - loss: 25.1916\n",
      "Epoch 63/100\n",
      " - 2s - loss: 25.0352\n",
      "Epoch 64/100\n",
      " - 2s - loss: 25.0082\n",
      "Epoch 65/100\n",
      " - 2s - loss: 25.1062\n",
      "Epoch 66/100\n",
      " - 2s - loss: 25.0115\n",
      "Epoch 67/100\n",
      " - 2s - loss: 25.0103\n",
      "Epoch 68/100\n",
      " - 2s - loss: 24.9456\n",
      "Epoch 69/100\n",
      " - 3s - loss: 25.0815\n",
      "Epoch 70/100\n",
      " - 2s - loss: 25.0669\n",
      "Epoch 71/100\n",
      " - 1s - loss: 25.0828\n",
      "Epoch 72/100\n",
      " - 1s - loss: 25.0545\n",
      "Epoch 73/100\n",
      " - 2s - loss: 24.9388\n",
      "Epoch 74/100\n",
      " - 2s - loss: 25.1438\n",
      "Epoch 75/100\n",
      " - 2s - loss: 25.0153\n",
      "Epoch 76/100\n",
      " - 2s - loss: 24.9720\n",
      "Epoch 77/100\n",
      " - 1s - loss: 25.0226\n",
      "Epoch 78/100\n",
      " - 2s - loss: 25.0698\n",
      "Epoch 79/100\n",
      " - 1s - loss: 25.1097\n",
      "Epoch 80/100\n",
      " - 1s - loss: 25.0237\n",
      "Epoch 81/100\n",
      " - 1s - loss: 25.0611\n",
      "Epoch 82/100\n",
      " - 1s - loss: 25.0615\n",
      "Epoch 83/100\n",
      " - 2s - loss: 25.0156\n",
      "Epoch 84/100\n",
      " - 1s - loss: 25.1001\n",
      "Epoch 85/100\n",
      " - 1s - loss: 25.0180\n",
      "Epoch 86/100\n",
      " - 1s - loss: 25.0842\n",
      "Epoch 87/100\n",
      " - 2s - loss: 25.0301\n",
      "Epoch 88/100\n",
      " - 2s - loss: 25.0493\n",
      "Epoch 89/100\n",
      " - 1s - loss: 24.8339\n",
      "Epoch 90/100\n",
      " - 1s - loss: 25.1190\n",
      "Epoch 91/100\n",
      " - 1s - loss: 25.0054\n",
      "Epoch 92/100\n",
      " - 1s - loss: 25.0787\n",
      "Epoch 93/100\n",
      " - 2s - loss: 25.0257\n",
      "Epoch 94/100\n",
      " - 2s - loss: 24.9703\n",
      "Epoch 95/100\n",
      " - 1s - loss: 25.0284\n",
      "Epoch 96/100\n",
      " - 1s - loss: 25.0707\n",
      "Epoch 97/100\n",
      " - 2s - loss: 25.0053\n",
      "Epoch 98/100\n",
      " - 2s - loss: 25.0364\n",
      "Epoch 99/100\n",
      " - 3s - loss: 25.0342\n",
      "Epoch 100/100\n",
      " - 2s - loss: 25.0686\n"
     ]
    }
   ],
   "source": [
    "# Create test and training sets for one-step-ahead regression.\n",
    "window_size = 1\n",
    "train_X, train_Y = create_dataset(train, window_size)\n",
    "test_X, test_Y = create_dataset(test, window_size)\n",
    "print(\"Original training data shape:\")\n",
    "print(train_X.shape)\n",
    "\n",
    "# Reshape the input data into appropriate form for Keras.\n",
    "train_X = np.reshape(train_X, (train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = np.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(\"New training data shape:\")\n",
    "print(train_X.shape)\n",
    "\n",
    "def fit_model(train_X, train_Y, window_size = 1):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(4, \n",
    "                   input_shape = (1, window_size)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss = smape_kun2, \n",
    "                  optimizer = \"adam\")\n",
    "    model.fit(train_X, \n",
    "              train_Y, \n",
    "              epochs = 100, \n",
    "              batch_size = 1, \n",
    "              verbose = 2)\n",
    "    \n",
    "    return(model)\n",
    "\n",
    "# Fit the first model.\n",
    "model1 = fit_model(train_X, train_Y, window_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data score: 173.11 RMSE\n",
      "Test data score: 172.13 RMSE\n"
     ]
    }
   ],
   "source": [
    "y = df['y'].values.astype(\"float32\").reshape(-1, 1)\n",
    "y = scaler.fit_transform(y)\n",
    "\n",
    "def predict_and_score(model, X, Y):\n",
    "    # Make predictions on the original scale of the data.\n",
    "    pred = scaler.inverse_transform(model.predict(X))\n",
    "    # Prepare Y data to also be on the original scale for interpretability.\n",
    "    orig_data = scaler.inverse_transform([Y])\n",
    "    # Calculate RMSE.\n",
    "    score = math.sqrt(mean_squared_error(orig_data[0], pred[:, 0]))\n",
    "    return(score, pred)\n",
    "\n",
    "rmse_train, train_predict = predict_and_score(model1, train_X, train_Y)\n",
    "rmse_test, test_predict = predict_and_score(model1, test_X, test_Y)\n",
    "\n",
    "print(\"Training data score: %.2f RMSE\" % rmse_train)\n",
    "print(\"Test data score: %.2f RMSE\" % rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.45538\n"
     ]
    }
   ],
   "source": [
    "y = df['y'].values.astype(\"float32\").reshape(-1, 1)\n",
    "y = scaler.fit_transform(y)\n",
    "\n",
    "pred = scaler.inverse_transform(model1.predict(test_X))\n",
    "\n",
    "Y_Test = K.variable(Y_Test.values)\n",
    "pred = K.variable(pred)\n",
    "\n",
    "print(K.eval(smape_kun2(Y_Test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def smape_kun2(y_true, y_pred):\n",
    "    res = K.mean((np.abs(y_pred - y_true) * 200/ (np.abs(y_pred) + np.abs(y_true))))\n",
    "    if res == np.nan:\n",
    "        return K.variable(0)\n",
    "    else: \n",
    "        return res\n",
    "#     return K.mean((np.abs(y_pred - y_true) * 200/ (np.abs(y_pred) + np.abs(y_true))).fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(475, 30, 1)\n",
      "Epoch 1/200\n",
      "475/475 [==============================] - 15s 32ms/step - loss: 203996.6898 - smape_kun2: 198.5727\n",
      "Epoch 2/200\n",
      "475/475 [==============================] - 0s 446us/step - loss: 200040.4642 - smape_kun2: 190.8903\n",
      "Epoch 3/200\n",
      "475/475 [==============================] - 0s 488us/step - loss: 186928.3396 - smape_kun2: 170.2987\n",
      "Epoch 4/200\n",
      "475/475 [==============================] - 0s 496us/step - loss: 156921.6122 - smape_kun2: 131.9623\n",
      "Epoch 5/200\n",
      "475/475 [==============================] - 0s 379us/step - loss: 108262.6940 - smape_kun2: 85.8509\n",
      "Epoch 6/200\n",
      "475/475 [==============================] - 0s 353us/step - loss: 66258.6245 - smape_kun2: 60.3617\n",
      "Epoch 7/200\n",
      "475/475 [==============================] - 0s 353us/step - loss: 48855.7920 - smape_kun2: 57.7586\n",
      "Epoch 8/200\n",
      "475/475 [==============================] - 0s 505us/step - loss: 38909.8090 - smape_kun2: 55.1537\n",
      "Epoch 9/200\n",
      "475/475 [==============================] - 0s 420us/step - loss: 32349.3881 - smape_kun2: 52.3547\n",
      "Epoch 10/200\n",
      "475/475 [==============================] - 0s 521us/step - loss: 28051.4199 - smape_kun2: 50.8338\n",
      "Epoch 11/200\n",
      "475/475 [==============================] - 0s 420us/step - loss: 25073.4297 - smape_kun2: 49.6296\n",
      "Epoch 12/200\n",
      "475/475 [==============================] - 0s 387us/step - loss: 22949.7272 - smape_kun2: 48.8517\n",
      "Epoch 13/200\n",
      "475/475 [==============================] - 0s 374us/step - loss: 21490.3959 - smape_kun2: 48.3054\n",
      "Epoch 14/200\n",
      "475/475 [==============================] - 0s 453us/step - loss: 20338.4917 - smape_kun2: 48.1617\n",
      "Epoch 15/200\n",
      "475/475 [==============================] - 0s 412us/step - loss: 19475.3478 - smape_kun2: 48.2808\n",
      "Epoch 16/200\n",
      "475/475 [==============================] - 0s 387us/step - loss: 18788.3105 - smape_kun2: 48.2426\n",
      "Epoch 17/200\n",
      "475/475 [==============================] - 0s 362us/step - loss: 18177.1956 - smape_kun2: 48.3766\n",
      "Epoch 18/200\n",
      "475/475 [==============================] - 0s 353us/step - loss: 17646.8242 - smape_kun2: 48.4548\n",
      "Epoch 19/200\n",
      "475/475 [==============================] - 0s 370us/step - loss: 17478.6899 - smape_kun2: 48.3093\n",
      "Epoch 20/200\n",
      "475/475 [==============================] - 0s 370us/step - loss: 16804.0122 - smape_kun2: 48.3329\n",
      "Epoch 21/200\n",
      "475/475 [==============================] - 0s 370us/step - loss: 16508.5011 - smape_kun2: 48.4031\n",
      "Epoch 22/200\n",
      "475/475 [==============================] - 0s 353us/step - loss: 16245.0166 - smape_kun2: 48.4001\n",
      "Epoch 23/200\n",
      "475/475 [==============================] - 0s 353us/step - loss: 15952.9947 - smape_kun2: 48.0516\n",
      "Epoch 24/200\n",
      "475/475 [==============================] - 0s 362us/step - loss: 15812.2492 - smape_kun2: 48.2793 0s - loss: 15123.4648 - smape_kun2: 47.796\n",
      "Epoch 25/200\n",
      "475/475 [==============================] - 0s 429us/step - loss: 15431.0797 - smape_kun2: 47.8965\n",
      "Epoch 26/200\n",
      "475/475 [==============================] - 0s 437us/step - loss: 15168.8224 - smape_kun2: 47.9523\n",
      "Epoch 27/200\n",
      "475/475 [==============================] - 0s 387us/step - loss: 14808.9616 - smape_kun2: 47.6607\n",
      "Epoch 28/200\n",
      "475/475 [==============================] - 0s 437us/step - loss: 14642.3865 - smape_kun2: 47.3468 0s - loss: 15922.5029 - smape_kun2: 50.5\n",
      "Epoch 29/200\n",
      "475/475 [==============================] - 0s 362us/step - loss: 14409.7047 - smape_kun2: 47.4305\n",
      "Epoch 30/200\n",
      "475/475 [==============================] - 0s 378us/step - loss: 14155.9127 - smape_kun2: 47.0255\n",
      "Epoch 31/200\n",
      "475/475 [==============================] - 0s 446us/step - loss: 13896.7721 - smape_kun2: 46.8087 0s - loss: 13616.5950 - smape_kun2: 46.992\n",
      "Epoch 32/200\n",
      "475/475 [==============================] - 0s 387us/step - loss: 13627.4618 - smape_kun2: 46.8216\n",
      "Epoch 33/200\n",
      "475/475 [==============================] - 0s 378us/step - loss: 13335.9213 - smape_kun2: 46.5628\n",
      "Epoch 34/200\n",
      "475/475 [==============================] - 0s 420us/step - loss: 13058.5865 - smape_kun2: 46.1958\n",
      "Epoch 35/200\n",
      "475/475 [==============================] - 0s 353us/step - loss: 12780.7152 - smape_kun2: 46.2957\n",
      "Epoch 36/200\n",
      "475/475 [==============================] - 0s 454us/step - loss: 12410.0104 - smape_kun2: 45.6813\n",
      "Epoch 37/200\n",
      "475/475 [==============================] - 0s 479us/step - loss: 12072.1760 - smape_kun2: 45.3904\n",
      "Epoch 38/200\n",
      "475/475 [==============================] - 0s 412us/step - loss: 11785.1840 - smape_kun2: 45.3260\n",
      "Epoch 39/200\n",
      "475/475 [==============================] - 0s 381us/step - loss: 11451.0720 - smape_kun2: 44.8929\n",
      "Epoch 40/200\n",
      "475/475 [==============================] - 0s 401us/step - loss: 11169.4045 - smape_kun2: 44.4187\n",
      "Epoch 41/200\n",
      "475/475 [==============================] - 0s 427us/step - loss: 10923.2185 - smape_kun2: 44.0741\n",
      "Epoch 42/200\n",
      "475/475 [==============================] - 0s 547us/step - loss: 10646.3225 - smape_kun2: 44.2737\n",
      "Epoch 43/200\n",
      "475/475 [==============================] - 0s 437us/step - loss: 10318.9938 - smape_kun2: 43.5413\n",
      "Epoch 44/200\n",
      "475/475 [==============================] - 0s 420us/step - loss: 10137.0231 - smape_kun2: 43.2312\n",
      "Epoch 45/200\n",
      "475/475 [==============================] - 0s 354us/step - loss: 9892.7271 - smape_kun2: 42.9794\n",
      "Epoch 46/200\n",
      "475/475 [==============================] - 0s 496us/step - loss: 9658.0345 - smape_kun2: 42.7105\n",
      "Epoch 47/200\n",
      "475/475 [==============================] - 0s 437us/step - loss: 9419.2583 - smape_kun2: 42.3278\n",
      "Epoch 48/200\n",
      "475/475 [==============================] - 0s 362us/step - loss: 9253.8904 - smape_kun2: 42.0808\n",
      "Epoch 49/200\n",
      "475/475 [==============================] - 0s 362us/step - loss: 9101.3962 - smape_kun2: 42.0401\n",
      "Epoch 50/200\n",
      "475/475 [==============================] - 0s 580us/step - loss: 8895.4011 - smape_kun2: 41.7332\n",
      "Epoch 51/200\n",
      "475/475 [==============================] - 0s 359us/step - loss: 8771.2626 - smape_kun2: 41.4825\n",
      "Epoch 52/200\n",
      "475/475 [==============================] - 0s 331us/step - loss: 8633.3593 - smape_kun2: 41.3731\n",
      "Epoch 53/200\n",
      "475/475 [==============================] - 0s 389us/step - loss: 8491.4524 - smape_kun2: 41.1268\n",
      "Epoch 54/200\n",
      "475/475 [==============================] - 0s 519us/step - loss: 8413.9884 - smape_kun2: 41.0986\n",
      "Epoch 55/200\n",
      "475/475 [==============================] - 0s 513us/step - loss: 8275.3797 - smape_kun2: 40.9188\n",
      "Epoch 56/200\n",
      "475/475 [==============================] - 0s 389us/step - loss: 8211.9699 - smape_kun2: 40.6271\n",
      "Epoch 57/200\n",
      "475/475 [==============================] - 0s 412us/step - loss: 8143.9151 - smape_kun2: 40.8253\n",
      "Epoch 58/200\n",
      "475/475 [==============================] - 0s 362us/step - loss: 8013.7743 - smape_kun2: 40.4235\n",
      "Epoch 59/200\n",
      "475/475 [==============================] - 0s 370us/step - loss: 7960.7765 - smape_kun2: 40.29390s - loss: 8094.1164 - smape_kun2: 41.84\n",
      "Epoch 60/200\n",
      "475/475 [==============================] - 0s 353us/step - loss: 7860.7502 - smape_kun2: 40.1170\n",
      "Epoch 61/200\n",
      "475/475 [==============================] - 0s 479us/step - loss: 7802.2546 - smape_kun2: 40.1353\n",
      "Epoch 62/200\n",
      "475/475 [==============================] - 0s 555us/step - loss: 7742.3562 - smape_kun2: 40.0428\n",
      "Epoch 63/200\n",
      "475/475 [==============================] - 0s 454us/step - loss: 7671.4235 - smape_kun2: 39.8937\n",
      "Epoch 64/200\n",
      "475/475 [==============================] - 0s 364us/step - loss: 7663.3353 - smape_kun2: 39.7306\n",
      "Epoch 65/200\n",
      "475/475 [==============================] - 0s 352us/step - loss: 7599.9625 - smape_kun2: 39.8574\n",
      "Epoch 66/200\n",
      "475/475 [==============================] - 0s 345us/step - loss: 7527.2252 - smape_kun2: 39.7161\n",
      "Epoch 67/200\n",
      "475/475 [==============================] - 0s 336us/step - loss: 7552.2367 - smape_kun2: 39.4147\n",
      "Epoch 68/200\n",
      "475/475 [==============================] - 0s 309us/step - loss: 7559.4067 - smape_kun2: 40.1933\n",
      "Epoch 69/200\n",
      "475/475 [==============================] - 0s 356us/step - loss: 7508.4141 - smape_kun2: 39.3642\n",
      "Epoch 70/200\n",
      "475/475 [==============================] - 0s 363us/step - loss: 7415.8927 - smape_kun2: 39.41010s - loss: 7168.9054 - smape_kun2: 33.\n",
      "Epoch 71/200\n",
      "475/475 [==============================] - 0s 442us/step - loss: 7325.7393 - smape_kun2: 39.3011\n",
      "Epoch 72/200\n",
      "475/475 [==============================] - 0s 362us/step - loss: 7309.6152 - smape_kun2: 39.2394\n",
      "Epoch 73/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "475/475 [==============================] - 0s 362us/step - loss: 7280.5651 - smape_kun2: 39.3943\n",
      "Epoch 74/200\n",
      "475/475 [==============================] - 0s 505us/step - loss: 7262.4858 - smape_kun2: 39.1597\n",
      "Epoch 75/200\n",
      "475/475 [==============================] - 0s 378us/step - loss: 7246.0146 - smape_kun2: 39.4484\n",
      "Epoch 76/200\n",
      "475/475 [==============================] - 0s 387us/step - loss: 7196.1622 - smape_kun2: 39.1309\n",
      "Epoch 77/200\n",
      "475/475 [==============================] - 0s 505us/step - loss: 7185.5806 - smape_kun2: 39.1040\n",
      "Epoch 78/200\n",
      "475/475 [==============================] - 0s 446us/step - loss: 7168.1956 - smape_kun2: 38.9994\n",
      "Epoch 79/200\n",
      "475/475 [==============================] - 0s 471us/step - loss: 7162.6535 - smape_kun2: 39.1626\n",
      "Epoch 80/200\n",
      "475/475 [==============================] - 0s 419us/step - loss: 7093.2609 - smape_kun2: 38.8484\n",
      "Epoch 81/200\n",
      "475/475 [==============================] - 0s 734us/step - loss: 7070.1488 - smape_kun2: 38.9343\n",
      "Epoch 82/200\n",
      "475/475 [==============================] - 0s 711us/step - loss: 7047.0499 - smape_kun2: 39.0492\n",
      "Epoch 83/200\n",
      "475/475 [==============================] - 0s 631us/step - loss: 7050.8157 - smape_kun2: 38.7607\n",
      "Epoch 84/200\n",
      "475/475 [==============================] - 0s 782us/step - loss: 7036.8342 - smape_kun2: 39.0621\n",
      "Epoch 85/200\n",
      "475/475 [==============================] - 0s 556us/step - loss: 7011.9863 - smape_kun2: 38.6765\n",
      "Epoch 86/200\n",
      "475/475 [==============================] - 0s 590us/step - loss: 7013.5216 - smape_kun2: 38.9587\n",
      "Epoch 87/200\n",
      "475/475 [==============================] - 0s 795us/step - loss: 6979.3416 - smape_kun2: 38.6870\n",
      "Epoch 88/200\n",
      "475/475 [==============================] - 1s 1ms/step - loss: 6958.5749 - smape_kun2: 39.1430\n",
      "Epoch 89/200\n",
      "475/475 [==============================] - 1s 1ms/step - loss: 6885.9740 - smape_kun2: 38.5071\n",
      "Epoch 90/200\n",
      "475/475 [==============================] - 0s 1ms/step - loss: 6873.1709 - smape_kun2: 38.6678: 0s - loss: 6284.0883 - smape_kun2: 35.80 - ETA: 0s - loss: 6473.6507 - smape_kun2: 37.\n",
      "Epoch 91/200\n",
      "475/475 [==============================] - 0s 715us/step - loss: 6869.4747 - smape_kun2: 38.5389\n",
      "Epoch 92/200\n",
      "475/475 [==============================] - 0s 807us/step - loss: 6814.9164 - smape_kun2: 38.4974\n",
      "Epoch 93/200\n",
      "475/475 [==============================] - 0s 858us/step - loss: 6806.4065 - smape_kun2: 38.4003\n",
      "Epoch 94/200\n",
      "475/475 [==============================] - 0s 723us/step - loss: 6787.1854 - smape_kun2: 38.5605\n",
      "Epoch 95/200\n",
      "475/475 [==============================] - 0s 799us/step - loss: 6785.6208 - smape_kun2: 38.47040s - loss: 7333.6640 - smape_kun2: 3\n",
      "Epoch 96/200\n",
      "475/475 [==============================] - 0s 841us/step - loss: 6758.3314 - smape_kun2: 38.5379\n",
      "Epoch 97/200\n",
      "475/475 [==============================] - 0s 715us/step - loss: 6751.2012 - smape_kun2: 38.3757\n",
      "Epoch 98/200\n",
      "475/475 [==============================] - 0s 882us/step - loss: 6711.6881 - smape_kun2: 38.5247\n",
      "Epoch 99/200\n",
      "475/475 [==============================] - 0s 675us/step - loss: 6707.9697 - smape_kun2: 38.3295\n",
      "Epoch 100/200\n",
      "475/475 [==============================] - 0s 580us/step - loss: 6725.0804 - smape_kun2: 38.3587\n",
      "Epoch 101/200\n",
      "475/475 [==============================] - 0s 833us/step - loss: 6673.6135 - smape_kun2: 38.2683\n",
      "Epoch 102/200\n",
      "475/475 [==============================] - 0s 614us/step - loss: 6659.3384 - smape_kun2: 38.3472\n",
      "Epoch 103/200\n",
      "475/475 [==============================] - 0s 631us/step - loss: 6612.7896 - smape_kun2: 38.1039\n",
      "Epoch 104/200\n",
      "475/475 [==============================] - 0s 1ms/step - loss: 6591.2123 - smape_kun2: 38.2381\n",
      "Epoch 105/200\n",
      "475/475 [==============================] - 0s 913us/step - loss: 6588.1666 - smape_kun2: 38.2353\n",
      "Epoch 106/200\n",
      "475/475 [==============================] - 0s 942us/step - loss: 6561.2636 - smape_kun2: 38.1080\n",
      "Epoch 107/200\n",
      "475/475 [==============================] - 0s 932us/step - loss: 6570.6691 - smape_kun2: 38.1795\n",
      "Epoch 108/200\n",
      "475/475 [==============================] - 0s 934us/step - loss: 6561.7295 - smape_kun2: 38.0908\n",
      "Epoch 109/200\n",
      "475/475 [==============================] - 0s 614us/step - loss: 6543.7095 - smape_kun2: 38.1046\n",
      "Epoch 110/200\n",
      "475/475 [==============================] - 0s 606us/step - loss: 6500.3213 - smape_kun2: 37.9279\n",
      "Epoch 111/200\n",
      "475/475 [==============================] - 0s 614us/step - loss: 6553.9107 - smape_kun2: 38.0843\n",
      "Epoch 112/200\n",
      "475/475 [==============================] - 0s 585us/step - loss: 6478.1102 - smape_kun2: 37.9276\n",
      "Epoch 113/200\n",
      "475/475 [==============================] - 0s 633us/step - loss: 6540.9383 - smape_kun2: 38.0486\n",
      "Epoch 114/200\n",
      "475/475 [==============================] - 0s 706us/step - loss: 6471.7820 - smape_kun2: 37.8902\n",
      "Epoch 115/200\n",
      "475/475 [==============================] - 0s 614us/step - loss: 6493.3080 - smape_kun2: 37.8646\n",
      "Epoch 116/200\n",
      "475/475 [==============================] - 0s 824us/step - loss: 6419.2273 - smape_kun2: 37.8884\n",
      "Epoch 117/200\n",
      "475/475 [==============================] - 0s 1ms/step - loss: 6436.8773 - smape_kun2: 37.8970\n",
      "Epoch 118/200\n",
      "475/475 [==============================] - 0s 989us/step - loss: 6398.3211 - smape_kun2: 37.9870\n",
      "Epoch 119/200\n",
      "475/475 [==============================] - 0s 965us/step - loss: 6378.3449 - smape_kun2: 37.7767\n",
      "Epoch 120/200\n",
      "475/475 [==============================] - 0s 706us/step - loss: 6396.0981 - smape_kun2: 37.8617\n",
      "Epoch 121/200\n",
      "475/475 [==============================] - 0s 597us/step - loss: 6358.5542 - smape_kun2: 37.7972\n",
      "Epoch 122/200\n",
      "475/475 [==============================] - 0s 862us/step - loss: 6365.7075 - smape_kun2: 37.6536\n",
      "Epoch 123/200\n",
      "475/475 [==============================] - 0s 882us/step - loss: 6372.4188 - smape_kun2: 37.81550s - loss: 6693.2113 - smape_kun2: 39.\n",
      "Epoch 124/200\n",
      "475/475 [==============================] - 0s 841us/step - loss: 6282.1054 - smape_kun2: 37.5841\n",
      "Epoch 125/200\n",
      "475/475 [==============================] - 0s 919us/step - loss: 6318.5794 - smape_kun2: 37.7120\n",
      "Epoch 126/200\n",
      "475/475 [==============================] - 0s 747us/step - loss: 6264.4750 - smape_kun2: 37.5634\n",
      "Epoch 127/200\n",
      "475/475 [==============================] - 0s 538us/step - loss: 6268.2003 - smape_kun2: nan\n",
      "Epoch 128/200\n",
      "475/475 [==============================] - 0s 614us/step - loss: 6335.8758 - smape_kun2: 37.5875\n",
      "Epoch 129/200\n",
      "475/475 [==============================] - 0s 614us/step - loss: 6243.0217 - smape_kun2: nan\n",
      "Epoch 130/200\n",
      "475/475 [==============================] - 0s 614us/step - loss: 6215.4427 - smape_kun2: nan\n",
      "Epoch 131/200\n",
      "475/475 [==============================] - 0s 908us/step - loss: 6206.8996 - smape_kun2: nan\n",
      "Epoch 132/200\n",
      "475/475 [==============================] - 0s 950us/step - loss: 6193.2446 - smape_kun2: nan\n",
      "Epoch 133/200\n",
      "475/475 [==============================] - 0s 840us/step - loss: 6268.1226 - smape_kun2: nan\n",
      "Epoch 134/200\n",
      "475/475 [==============================] - 0s 723us/step - loss: 6167.3585 - smape_kun2: nan\n",
      "Epoch 135/200\n",
      "475/475 [==============================] - 0s 589us/step - loss: 6141.8595 - smape_kun2: nan\n",
      "Epoch 136/200\n",
      "475/475 [==============================] - 0s 606us/step - loss: 6154.8497 - smape_kun2: nan\n",
      "Epoch 137/200\n",
      "475/475 [==============================] - 0s 631us/step - loss: 6137.5082 - smape_kun2: nan\n",
      "Epoch 138/200\n",
      "475/475 [==============================] - 0s 572us/step - loss: 6145.5504 - smape_kun2: nan\n",
      "Epoch 139/200\n",
      "475/475 [==============================] - 0s 597us/step - loss: 6084.4273 - smape_kun2: nan\n",
      "Epoch 140/200\n",
      "475/475 [==============================] - 0s 622us/step - loss: 6081.0355 - smape_kun2: nan\n",
      "Epoch 141/200\n",
      "475/475 [==============================] - 0s 664us/step - loss: 6107.7905 - smape_kun2: nan\n",
      "Epoch 142/200\n",
      "475/475 [==============================] - 0s 1ms/step - loss: 6083.4794 - smape_kun2: nan\n",
      "Epoch 143/200\n",
      "475/475 [==============================] - 0s 757us/step - loss: 6048.6303 - smape_kun2: nan\n",
      "Epoch 144/200\n",
      "475/475 [==============================] - 0s 605us/step - loss: 6019.5960 - smape_kun2: nan\n",
      "Epoch 145/200\n",
      "475/475 [==============================] - 0s 664us/step - loss: 6013.6070 - smape_kun2: nan\n",
      "Epoch 146/200\n",
      "475/475 [==============================] - 0s 656us/step - loss: 6005.4980 - smape_kun2: nan\n",
      "Epoch 147/200\n",
      "475/475 [==============================] - 0s 631us/step - loss: 6012.9272 - smape_kun2: nan\n",
      "Epoch 148/200\n",
      "475/475 [==============================] - 0s 572us/step - loss: 6006.8328 - smape_kun2: nan\n",
      "Epoch 149/200\n",
      "475/475 [==============================] - 0s 824us/step - loss: 6065.0695 - smape_kun2: nan\n",
      "Epoch 150/200\n",
      "475/475 [==============================] - 0s 639us/step - loss: 5947.9008 - smape_kun2: nan\n",
      "Epoch 151/200\n",
      "475/475 [==============================] - 0s 563us/step - loss: 5944.0066 - smape_kun2: nan\n",
      "Epoch 152/200\n",
      "475/475 [==============================] - 0s 547us/step - loss: 5929.8151 - smape_kun2: nan\n",
      "Epoch 153/200\n",
      "475/475 [==============================] - 0s 555us/step - loss: 5912.0391 - smape_kun2: nan\n",
      "Epoch 154/200\n",
      "475/475 [==============================] - 0s 563us/step - loss: 5920.2413 - smape_kun2: nan\n",
      "Epoch 155/200\n",
      "475/475 [==============================] - 0s 547us/step - loss: 5910.5486 - smape_kun2: nan\n",
      "Epoch 156/200\n",
      "475/475 [==============================] - 0s 540us/step - loss: 5924.4376 - smape_kun2: nan\n",
      "Epoch 157/200\n",
      "475/475 [==============================] - 0s 540us/step - loss: 5970.4551 - smape_kun2: nan\n",
      "Epoch 158/200\n",
      "475/475 [==============================] - 0s 538us/step - loss: 6041.2291 - smape_kun2: nan\n",
      "Epoch 159/200\n",
      "475/475 [==============================] - 0s 538us/step - loss: 5857.4950 - smape_kun2: nan\n",
      "Epoch 160/200\n",
      "475/475 [==============================] - 0s 521us/step - loss: 5976.5600 - smape_kun2: nan\n",
      "Epoch 161/200\n",
      "475/475 [==============================] - 0s 572us/step - loss: 5965.8074 - smape_kun2: nan\n",
      "Epoch 162/200\n",
      "475/475 [==============================] - 0s 538us/step - loss: 5842.3028 - smape_kun2: nan\n",
      "Epoch 163/200\n",
      "475/475 [==============================] - 0s 521us/step - loss: 5839.3946 - smape_kun2: nan\n",
      "Epoch 164/200\n",
      "475/475 [==============================] - 0s 530us/step - loss: 5924.4664 - smape_kun2: nan\n",
      "Epoch 165/200\n",
      "475/475 [==============================] - 0s 555us/step - loss: 5810.5255 - smape_kun2: nan\n",
      "Epoch 166/200\n",
      "475/475 [==============================] - 0s 597us/step - loss: 5758.0851 - smape_kun2: nan\n",
      "Epoch 167/200\n",
      "475/475 [==============================] - 0s 547us/step - loss: 5739.6715 - smape_kun2: nan\n",
      "Epoch 168/200\n",
      "475/475 [==============================] - 0s 563us/step - loss: 5759.0237 - smape_kun2: nan\n",
      "Epoch 169/200\n",
      "475/475 [==============================] - 0s 580us/step - loss: 5830.3633 - smape_kun2: nan\n",
      "Epoch 170/200\n",
      "475/475 [==============================] - 0s 580us/step - loss: 5775.7000 - smape_kun2: nan\n",
      "Epoch 171/200\n",
      "475/475 [==============================] - 0s 538us/step - loss: 5744.9568 - smape_kun2: nan\n",
      "Epoch 172/200\n",
      "475/475 [==============================] - 0s 555us/step - loss: 5735.2820 - smape_kun2: nan\n",
      "Epoch 173/200\n",
      "475/475 [==============================] - 0s 547us/step - loss: 5713.7479 - smape_kun2: nan\n",
      "Epoch 174/200\n",
      "475/475 [==============================] - 0s 505us/step - loss: 5718.9460 - smape_kun2: nan\n",
      "Epoch 175/200\n",
      "475/475 [==============================] - 0s 530us/step - loss: 5686.7225 - smape_kun2: nan\n",
      "Epoch 176/200\n",
      "475/475 [==============================] - 0s 562us/step - loss: 5678.5745 - smape_kun2: nan\n",
      "Epoch 177/200\n",
      "475/475 [==============================] - 0s 866us/step - loss: 5705.6264 - smape_kun2: nan\n",
      "Epoch 178/200\n",
      "475/475 [==============================] - 0s 757us/step - loss: 5670.4182 - smape_kun2: nan\n",
      "Epoch 179/200\n",
      "475/475 [==============================] - 0s 631us/step - loss: 5615.7399 - smape_kun2: nan\n",
      "Epoch 180/200\n",
      "475/475 [==============================] - 0s 606us/step - loss: 5648.7598 - smape_kun2: nan\n",
      "Epoch 181/200\n",
      "475/475 [==============================] - 0s 572us/step - loss: 5626.0289 - smape_kun2: nan\n",
      "Epoch 182/200\n",
      "475/475 [==============================] - 0s 765us/step - loss: 5611.7054 - smape_kun2: nan: 0s - loss: 5515.2627 - smape_kun\n",
      "Epoch 183/200\n",
      "475/475 [==============================] - 1s 1ms/step - loss: 5679.6195 - smape_kun2: nan\n",
      "Epoch 184/200\n",
      "475/475 [==============================] - 0s 984us/step - loss: 5701.6383 - smape_kun2: nan\n",
      "Epoch 185/200\n",
      "475/475 [==============================] - 0s 858us/step - loss: 5544.5325 - smape_kun2: nan\n",
      "Epoch 186/200\n",
      "475/475 [==============================] - 0s 937us/step - loss: 5573.7028 - smape_kun2: nan\n",
      "Epoch 187/200\n",
      "475/475 [==============================] - 0s 834us/step - loss: 5656.5117 - smape_kun2: nan\n",
      "Epoch 188/200\n",
      "475/475 [==============================] - 0s 572us/step - loss: 5557.5393 - smape_kun2: nan\n",
      "Epoch 189/200\n",
      "475/475 [==============================] - 0s 603us/step - loss: 5489.6652 - smape_kun2: nan\n",
      "Epoch 190/200\n",
      "475/475 [==============================] - 0s 588us/step - loss: 5519.4809 - smape_kun2: nan\n",
      "Epoch 191/200\n",
      "475/475 [==============================] - 0s 563us/step - loss: 5510.8585 - smape_kun2: nan\n",
      "Epoch 192/200\n",
      "475/475 [==============================] - 0s 574us/step - loss: 5461.1642 - smape_kun2: nan\n",
      "Epoch 193/200\n",
      "475/475 [==============================] - 0s 698us/step - loss: 5488.3873 - smape_kun2: nan\n",
      "Epoch 194/200\n",
      "475/475 [==============================] - 0s 976us/step - loss: 5441.0892 - smape_kun2: nan\n",
      "Epoch 195/200\n",
      "475/475 [==============================] - 0s 869us/step - loss: 5478.5196 - smape_kun2: nan\n",
      "Epoch 196/200\n",
      "475/475 [==============================] - 0s 938us/step - loss: 5426.9608 - smape_kun2: nan\n",
      "Epoch 197/200\n",
      "475/475 [==============================] - 0s 969us/step - loss: 5486.6397 - smape_kun2: nan\n",
      "Epoch 198/200\n",
      "475/475 [==============================] - 0s 758us/step - loss: 5474.3631 - smape_kun2: nan\n",
      "Epoch 199/200\n",
      "475/475 [==============================] - 0s 597us/step - loss: 5440.3422 - smape_kun2: nan\n",
      "Epoch 200/200\n",
      "475/475 [==============================] - 0s 622us/step - loss: 5428.0825 - smape_kun2: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1522cc53128>"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten\n",
    "#create model\n",
    "# train_X = np.reshape(X_Train, (X_Train.shape[0], X_Train.shape[1], 1))\n",
    "# test_X = np.reshape(X_Test, (X_Test.shape[0], X_Test.shape[1], 1))\n",
    "# print(train_X.shape)\n",
    "\n",
    "train_X = X_Train.reshape(X_Train.shape[0], X_Train.shape[1], 1)   #Reshape for CNN -  should work!!\n",
    "print(train_X.shape)\n",
    "test_X = X_Test.reshape(X_Test.shape[0], X_Test.shape[1], 1)\n",
    "\n",
    "model = Sequential()\n",
    "#add model layers\n",
    "model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(30, 1)))\n",
    "model.add(Conv1D(32, kernel_size=3, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='relu'))\n",
    "\n",
    "#compile model using accuracy to measure model performance\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=[smape_kun2])\n",
    "\n",
    "#train the model\n",
    "model.fit(train_X, Y_Train, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[867.7157 ]\n",
      " [262.68402]\n",
      " [607.5696 ]\n",
      " [946.83057]\n",
      " [432.23187]\n",
      " [337.71848]\n",
      " [859.1907 ]\n",
      " [277.25235]\n",
      " [732.86316]\n",
      " [247.81816]\n",
      " [  0.     ]\n",
      " [ 35.62082]\n",
      " [697.0402 ]\n",
      " [354.8297 ]\n",
      " [735.61194]\n",
      " [869.08417]\n",
      " [248.44254]\n",
      " [141.53867]\n",
      " [299.8991 ]\n",
      " [162.2626 ]\n",
      " [927.0357 ]\n",
      " [268.90594]\n",
      " [226.74643]\n",
      " [152.10052]\n",
      " [123.06133]]\n",
      "20.52364306574942\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow as t\n",
    "#predict first 4 images in the test set\n",
    "# pred = model.predict(test_X)\n",
    "# print(pred)\n",
    "# Y_Test = K.variable(Y_Test)\n",
    "# pred = K.variable(pred)\n",
    "\n",
    "# print(K.eval(smape_fast(Y_Test, pred)))\n",
    "\n",
    "pred = model.predict(test_X)\n",
    "print(pred)\n",
    "#Y_Test = K.variable(Y_Test)\n",
    "#pred = K.variable(pred)\n",
    "pred = twoone(pred)\n",
    "\n",
    "print((smape_kun(Y_Test, pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
